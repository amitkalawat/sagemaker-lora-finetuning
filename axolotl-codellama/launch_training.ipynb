{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This example notebook uses Axolotl to fine-tune large foundation models\n",
    "\n",
    "[Axolotl](https://github.com/OpenAccess-AI-Collective/axolotl) is a tool designed to streamline the fine-tuning of various AI models, offering support for multiple configurations and architectures.\n",
    "\n",
    "Features:\n",
    "\n",
    "- Train various Huggingface models such as llama, pythia, falcon, mpt\n",
    "- Supports fullfinetune, lora, qlora, relora, and gptq\n",
    "- Customize configurations using a simple yaml file or CLI overwrite\n",
    "- Load different dataset formats, use custom formats, or bring your own tokenized datasets\n",
    "- Integrated with xformer, flash attention, rope scaling, and multipacking\n",
    "- Works with single GPU or multiple GPUs via FSDP or Deepspeed\n",
    "- Easily run with Docker locally or on the cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (1.10.1)\n",
      "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /opt/conda/lib/python3.10/site-packages (from scipy) (1.23.5)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install -Uq sagemaker\n",
    "%pip install -Uq datasets\n",
    "!pip install -Uq transformers==4.33.1 \n",
    "!pip install -Uq bitsandbytes peft accelerate\n",
    "!pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import json\n",
    "from sagemaker import Model, image_uris, serializers, deserializers\n",
    "import time\n",
    "from pathlib import Path\n",
    "from utils import download_model\n",
    "\n",
    "boto3_session=boto3.session.Session()\n",
    "# boto3_session=boto3.session.Session()\n",
    "\n",
    "smr = boto3_session.client(\"sagemaker-runtime\") # sagemaker runtime client for invoking the endpoint\n",
    "sm = boto3_session.client(\"sagemaker\") \n",
    "s3_rsr = boto3_session.resource(\"s3\")\n",
    "role = sagemaker.get_execution_role()  \n",
    "\n",
    "sess = sagemaker.session.Session(boto3_session, sagemaker_client=sm, sagemaker_runtime_client=smr)  # sagemaker session for interacting with different AWS APIs\n",
    "bucket = sess.default_bucket()  # sagemaker session for interacting with different AWS APIs\n",
    "region = sess._region_name  # region name of the current SageMaker Studio environment\n",
    "s3_prefix = \"code-llama7b\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model already exists at CodeLlama-7b-hf\n",
      "Skipping download\n"
     ]
    }
   ],
   "source": [
    "# uncomment to download model\n",
    "local_model_path = download_model(\"codellama/CodeLlama-7b-hf\", \"CodeLlama-7b-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if list(s3_rsr.Bucket(bucket).objects.filter(Prefix=s3_prefix)) :\n",
    "    print(\"Model already exists on the S3 bucket\")\n",
    "    print(f\"If you want to upload a new model, please delete the existing model from the S3 bucket with the following command: \\n !aws s3 rm --recursive s3://{bucket}/{s3_prefix}\")\n",
    "    s3_model_location = f\"s3://{bucket}/{s3_prefix}\"\n",
    "else:\n",
    "    s3_model_location = sess.upload_data(path=local_model_path.as_posix(), bucket=bucket, key_prefix=s3_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Data and upload to S3\n",
    "[Spider dataset with schema](https://huggingface.co/datasets/b-mc2/sql-create-context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'context', 'answer'],\n",
      "        num_rows: 78577\n",
      "    })\n",
      "})\n",
      "Uploaded training data file to s3://sagemaker-us-west-2-376678947624/code-llama7b/data/spider_create_context_train.json\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "# download the training data mhenrichsen/alpaca_2k_test using the HuggingFace datasets library and save output as json\n",
    "dataset = datasets.load_dataset(\"b-mc2/sql-create-context\")\n",
    "print(dataset)\n",
    "\n",
    "data_path = Path(\"data\")\n",
    "data_path.mkdir(exist_ok=True)\n",
    "\n",
    "dataset[\"train\"].to_pandas().to_json(\"data/spider_create_context_train.json\", orient=\"records\", lines=True)\n",
    "s3_data = sess.upload_data(path=\"data/spider_create_context_train.json\", bucket=bucket, key_prefix=f\"{s3_prefix}/data\")\n",
    "\n",
    "print(f\"Uploaded training data file to {s3_data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-27 21:24:00   19871585 spider_create_context_train.json\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls $s3_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.debugger import TensorBoardOutputConfig\n",
    "import time\n",
    "\n",
    "str_time = time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())\n",
    "\n",
    "tb_output_config = TensorBoardOutputConfig(s3_output_path=f\"s3://{bucket}/{s3_prefix}/tensorboard/{str_time}\",\n",
    "    container_local_output_path=\"/opt/ml/output/tensorboard\")\n",
    "\n",
    "hyperparameters = {\n",
    "    \"config\": \"code-llama-7b-qlora.yml\",\n",
    "    \"deepspeed\": \"axolotl/deepspeed/zero2.json\"\n",
    "}\n",
    "\n",
    "\n",
    "estimator = PyTorch(\n",
    "    source_dir = \"src\",\n",
    "    entry_point=\"axolotl/src/axolotl/cli/train.py\",\n",
    "    sagemaker_session=sess,\n",
    "    role=role,\n",
    "    instance_count=2, \n",
    "    hyperparameters=hyperparameters,\n",
    "    instance_type=\"ml.g5.2xlarge\", \n",
    "    framework_version=\"2.0.1\",\n",
    "    py_version=\"py310\",\n",
    "    disable_profiler=True,\n",
    "    max_run=60*60*24*2,\n",
    "    keep_alive_period_in_seconds=3600,\n",
    "    tensorboard_output_config=tb_output_config,\n",
    "    environment = {\"HUGGINGFACE_HUB_CACHE\": \"/tmp\", \n",
    "                    \"LIBRARY_PATH\": \"/opt/conda/lib/\",\n",
    "                    \"TRANSFORMERS_CACHE\": \"/tmp\",\n",
    "                    \"NCCL_P2P_LEVEL\": \"NVL\"},\n",
    "    distribution={\"torch_distributed\": {\"enabled\": True}} \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: pytorch-training-2023-10-27-21-24-37-446\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-27 21:24:39 Starting - Starting the training job...\n",
      "2023-10-27 21:24:55 Starting - Preparing the instances for training......\n",
      "2023-10-27 21:26:08 Downloading - Downloading input data......................................................\n",
      "2023-10-27 21:35:06 Training - Training image download completed. Training in progress..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-10-27 21:35:08,717 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-10-27 21:35:08,730 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-10-27 21:35:08,739 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-10-27 21:35:08,746 sagemaker_pytorch_container.training INFO     Invoking TorchDistributed...\u001b[0m\n",
      "\u001b[34m2023-10-27 21:35:08,746 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-10-27 21:35:10,454 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing ./deepspeed-0.11.1-py3-none-any.whl (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mObtaining file:///opt/ml/code/axolotl (from -r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting tensorboard (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for tensorboard from https://files.pythonhosted.org/packages/69/38/fb2ac9c4c8efbe020ae88f6772be87d51ef18526ac541fc3393786b7c45a/tensorboard-2.15.0-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading tensorboard-2.15.0-py3-none-any.whl.metadata (1.7 kB)\u001b[0m\n",
      "\u001b[34mCollecting flash-attn==2.2.1 (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading flash_attn-2.2.1.tar.gz (2.3 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.3/2.3 MB 13.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[35mbash: no job control in this shell\u001b[0m\n",
      "\u001b[35m2023-10-27 21:35:08,677 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[35m2023-10-27 21:35:08,690 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2023-10-27 21:35:08,699 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[35m2023-10-27 21:35:08,706 sagemaker_pytorch_container.training INFO     Invoking TorchDistributed...\u001b[0m\n",
      "\u001b[35m2023-10-27 21:35:08,706 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[35m2023-10-27 21:35:10,398 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[35m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[35mProcessing ./deepspeed-0.11.1-py3-none-any.whl (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[35mObtaining file:///opt/ml/code/axolotl (from -r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mCollecting tensorboard (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for tensorboard from https://files.pythonhosted.org/packages/69/38/fb2ac9c4c8efbe020ae88f6772be87d51ef18526ac541fc3393786b7c45a/tensorboard-2.15.0-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading tensorboard-2.15.0-py3-none-any.whl.metadata (1.7 kB)\u001b[0m\n",
      "\u001b[35mCollecting flash-attn==2.2.1 (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[35mDownloading flash_attn-2.2.1.tar.gz (2.3 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.3/2.3 MB 13.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (0.22.0)\u001b[0m\n",
      "\u001b[35mCollecting pydantic<2.0 (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for pydantic<2.0 from https://files.pythonhosted.org/packages/e0/2f/d6f17f8385d718233bcae893d27525443d41201c938b68a4af3d591a33e4/pydantic-1.10.13-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading pydantic-1.10.13-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (149 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 149.6/149.6 kB 41.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from flash-attn==2.2.1->-r requirements.txt (line 2)) (2.0.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: einops in /opt/conda/lib/python3.10/site-packages (from flash-attn==2.2.1->-r requirements.txt (line 2)) (0.6.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from flash-attn==2.2.1->-r requirements.txt (line 2)) (23.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from flash-attn==2.2.1->-r requirements.txt (line 2)) (1.11.1)\u001b[0m\n",
      "\u001b[35mCollecting absl-py>=0.4 (from tensorboard->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for absl-py>=0.4 from https://files.pythonhosted.org/packages/01/e4/dc0a1dcc4e74e08d7abedab278c795eef54a224363bb18f5692f416d834f/absl_py-2.0.0-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading absl_py-2.0.0-py3-none-any.whl.metadata (2.3 kB)\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (0.22.0)\u001b[0m\n",
      "\u001b[34mCollecting pydantic<2.0 (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for pydantic<2.0 from https://files.pythonhosted.org/packages/e0/2f/d6f17f8385d718233bcae893d27525443d41201c938b68a4af3d591a33e4/pydantic-1.10.13-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading pydantic-1.10.13-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (149 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 149.6/149.6 kB 21.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from flash-attn==2.2.1->-r requirements.txt (line 2)) (2.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: einops in /opt/conda/lib/python3.10/site-packages (from flash-attn==2.2.1->-r requirements.txt (line 2)) (0.6.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from flash-attn==2.2.1->-r requirements.txt (line 2)) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from flash-attn==2.2.1->-r requirements.txt (line 2)) (1.11.1)\u001b[0m\n",
      "\u001b[34mCollecting absl-py>=0.4 (from tensorboard->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for absl-py>=0.4 from https://files.pythonhosted.org/packages/01/e4/dc0a1dcc4e74e08d7abedab278c795eef54a224363bb18f5692f416d834f/absl_py-2.0.0-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading absl_py-2.0.0-py3-none-any.whl.metadata (2.3 kB)\u001b[0m\n",
      "\u001b[35mCollecting grpcio>=1.48.2 (from tensorboard->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for grpcio>=1.48.2 from https://files.pythonhosted.org/packages/20/7f/e76618521aa9d33c6c1c9c3473f866da521678aa6ea2f4df3a896757748c/grpcio-1.59.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading grpcio-1.59.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\u001b[0m\n",
      "\u001b[35mCollecting google-auth<3,>=1.6.3 (from tensorboard->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for google-auth<3,>=1.6.3 from https://files.pythonhosted.org/packages/39/7c/2e4fa55a99f83ef9ef229ac5d59c44ceb90e2d0145711590c0fa39669f32/google_auth-2.23.3-py2.py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading google_auth-2.23.3-py2.py3-none-any.whl.metadata (4.2 kB)\u001b[0m\n",
      "\u001b[35mCollecting google-auth-oauthlib<2,>=0.5 (from tensorboard->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for google-auth-oauthlib<2,>=0.5 from https://files.pythonhosted.org/packages/ce/33/a907b4b67245647746dde8d61e1643ef5d210c88e090d491efd89eff9f95/google_auth_oauthlib-1.1.0-py2.py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading google_auth_oauthlib-1.1.0-py2.py3-none-any.whl.metadata (2.7 kB)\u001b[0m\n",
      "\u001b[35mCollecting markdown>=2.6.8 (from tensorboard->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for markdown>=2.6.8 from https://files.pythonhosted.org/packages/bb/c1/50caaec6cadc1c6adc8fe351e03bd646d6e4dd17f55fca0f4c8d7ea8d3e9/Markdown-3.5-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading Markdown-3.5-py3-none-any.whl.metadata (7.1 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: numpy>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 1)) (1.24.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: protobuf<4.24,>=3.19.6 in /opt/conda/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 1)) (3.20.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 1)) (2.31.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 1)) (65.6.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: six>1.9 in /opt/conda/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 1)) (1.16.0)\u001b[0m\n",
      "\u001b[35mCollecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for tensorboard-data-server<0.8.0,>=0.7.0 from https://files.pythonhosted.org/packages/73/c6/825dab04195756cf8ff2e12698f22513b3db2f64925bdd41671bfb33aaa5/tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mCollecting grpcio>=1.48.2 (from tensorboard->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for grpcio>=1.48.2 from https://files.pythonhosted.org/packages/20/7f/e76618521aa9d33c6c1c9c3473f866da521678aa6ea2f4df3a896757748c/grpcio-1.59.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading grpcio-1.59.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\u001b[0m\n",
      "\u001b[34mCollecting google-auth<3,>=1.6.3 (from tensorboard->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for google-auth<3,>=1.6.3 from https://files.pythonhosted.org/packages/39/7c/2e4fa55a99f83ef9ef229ac5d59c44ceb90e2d0145711590c0fa39669f32/google_auth-2.23.3-py2.py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading google_auth-2.23.3-py2.py3-none-any.whl.metadata (4.2 kB)\u001b[0m\n",
      "\u001b[34mCollecting google-auth-oauthlib<2,>=0.5 (from tensorboard->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for google-auth-oauthlib<2,>=0.5 from https://files.pythonhosted.org/packages/ce/33/a907b4b67245647746dde8d61e1643ef5d210c88e090d491efd89eff9f95/google_auth_oauthlib-1.1.0-py2.py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading google_auth_oauthlib-1.1.0-py2.py3-none-any.whl.metadata (2.7 kB)\u001b[0m\n",
      "\u001b[34mCollecting markdown>=2.6.8 (from tensorboard->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for markdown>=2.6.8 from https://files.pythonhosted.org/packages/bb/c1/50caaec6cadc1c6adc8fe351e03bd646d6e4dd17f55fca0f4c8d7ea8d3e9/Markdown-3.5-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading Markdown-3.5-py3-none-any.whl.metadata (7.1 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 1)) (1.24.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf<4.24,>=3.19.6 in /opt/conda/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 1)) (3.20.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 1)) (2.31.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 1)) (65.6.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>1.9 in /opt/conda/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 1)) (1.16.0)\u001b[0m\n",
      "\u001b[34mCollecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for tensorboard-data-server<0.8.0,>=0.7.0 from https://files.pythonhosted.org/packages/73/c6/825dab04195756cf8ff2e12698f22513b3db2f64925bdd41671bfb33aaa5/tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 1)) (2.3.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.21.0->-r requirements.txt (line 3)) (5.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.21.0->-r requirements.txt (line 3)) (6.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<2.0->-r requirements.txt (line 4)) (4.8.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: hjson in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.11.1->-r requirements.txt (line 5)) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.11.1->-r requirements.txt (line 5)) (9.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.11.1->-r requirements.txt (line 5)) (4.65.0)\u001b[0m\n",
      "\u001b[34mCollecting auto-gptq (from axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for auto-gptq from https://files.pythonhosted.org/packages/d9/e2/51a1e760837d22b01b9393d36f34a4273eb3a20287e32021376a45cce205/auto_gptq-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading auto_gptq-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\u001b[0m\n",
      "\u001b[34mCollecting peft (from axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for peft from https://files.pythonhosted.org/packages/37/1a/8d20e8704da9fa070eb909265584b960da57be1d833d550c59f50906dc5c/peft-0.5.0-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading peft-0.5.0-py3-none-any.whl.metadata (22 kB)\u001b[0m\n",
      "\u001b[34mCollecting transformers (from axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for transformers from https://files.pythonhosted.org/packages/c1/bd/f64d67df4d3b05a460f281defe830ffab6d7940b7ca98ec085e94e024781/transformers-4.34.1-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.34.1-py3-none-any.whl.metadata (121 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 121.5/121.5 kB 16.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting bitsandbytes>=0.41.1 (from axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for bitsandbytes>=0.41.1 from https://files.pythonhosted.org/packages/1e/2c/af22cd797fc368a9f098ed03015730e6568b884fe67f9940793d944a4b7b/bitsandbytes-0.41.1-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading bitsandbytes-0.41.1-py3-none-any.whl.metadata (9.8 kB)\u001b[0m\n",
      "\u001b[34mCollecting addict (from axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[35mDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard->-r requirements.txt (line 1)) (2.3.7)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.21.0->-r requirements.txt (line 3)) (5.9.5)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.21.0->-r requirements.txt (line 3)) (6.0.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<2.0->-r requirements.txt (line 4)) (4.8.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: hjson in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.11.1->-r requirements.txt (line 5)) (3.1.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.11.1->-r requirements.txt (line 5)) (9.0.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.11.1->-r requirements.txt (line 5)) (4.65.0)\u001b[0m\n",
      "\u001b[35mCollecting auto-gptq (from axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for auto-gptq from https://files.pythonhosted.org/packages/d9/e2/51a1e760837d22b01b9393d36f34a4273eb3a20287e32021376a45cce205/auto_gptq-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading auto_gptq-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\u001b[0m\n",
      "\u001b[35mCollecting peft (from axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for peft from https://files.pythonhosted.org/packages/37/1a/8d20e8704da9fa070eb909265584b960da57be1d833d550c59f50906dc5c/peft-0.5.0-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading peft-0.5.0-py3-none-any.whl.metadata (22 kB)\u001b[0m\n",
      "\u001b[35mCollecting transformers (from axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for transformers from https://files.pythonhosted.org/packages/c1/bd/f64d67df4d3b05a460f281defe830ffab6d7940b7ca98ec085e94e024781/transformers-4.34.1-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading transformers-4.34.1-py3-none-any.whl.metadata (121 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 121.5/121.5 kB 27.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting bitsandbytes>=0.41.1 (from axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for bitsandbytes>=0.41.1 from https://files.pythonhosted.org/packages/1e/2c/af22cd797fc368a9f098ed03015730e6568b884fe67f9940793d944a4b7b/bitsandbytes-0.41.1-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading bitsandbytes-0.41.1-py3-none-any.whl.metadata (9.8 kB)\u001b[0m\n",
      "\u001b[35mCollecting addict (from axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[35mDownloading addict-2.4.0-py3-none-any.whl (3.8 kB)\u001b[0m\n",
      "\u001b[35mCollecting fire (from axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[35mDownloading fire-0.5.0.tar.gz (88 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 88.3/88.3 kB 28.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mCollecting datasets (from axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for datasets from https://files.pythonhosted.org/packages/7c/55/b3432f43d6d7fee999bb23a547820d74c48ec540f5f7842e41aa5d8d5f3a/datasets-2.14.6-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading datasets-2.14.6-py3-none-any.whl.metadata (19 kB)\u001b[0m\n",
      "\u001b[35mCollecting sentencepiece (from axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[35mDownloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 103.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading addict-2.4.0-py3-none-any.whl (3.8 kB)\u001b[0m\n",
      "\u001b[34mCollecting fire (from axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading fire-0.5.0.tar.gz (88 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 88.3/88.3 kB 16.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting datasets (from axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for datasets from https://files.pythonhosted.org/packages/7c/55/b3432f43d6d7fee999bb23a547820d74c48ec540f5f7842e41aa5d8d5f3a/datasets-2.14.6-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.14.6-py3-none-any.whl.metadata (19 kB)\u001b[0m\n",
      "\u001b[34mCollecting sentencepiece (from axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 22.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting wandb (from axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for wandb from https://files.pythonhosted.org/packages/1c/5e/0362fa88679852c7fd3ac85ee5bd949426c4a51a61379010d4089be6d7ac/wandb-0.15.12-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading wandb-0.15.12-py3-none-any.whl.metadata (9.8 kB)\u001b[0m\n",
      "\u001b[34mCollecting xformers (from axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for xformers from https://files.pythonhosted.org/packages/bb/9c/bb1a40d5e8db2b34dd6e6c0f851e86b38a3d0840fff1bf14240eff7d3da6/xformers-0.0.22.post7-cp310-cp310-manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading xformers-0.0.22.post7-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.0 kB)\u001b[0m\n",
      "\u001b[34mCollecting optimum (from axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading optimum-1.13.2.tar.gz (300 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 301.0/301.0 kB 23.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mInstalling build dependencies: started\u001b[0m\n",
      "\u001b[35mCollecting wandb (from axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for wandb from https://files.pythonhosted.org/packages/1c/5e/0362fa88679852c7fd3ac85ee5bd949426c4a51a61379010d4089be6d7ac/wandb-0.15.12-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading wandb-0.15.12-py3-none-any.whl.metadata (9.8 kB)\u001b[0m\n",
      "\u001b[35mCollecting xformers (from axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for xformers from https://files.pythonhosted.org/packages/bb/9c/bb1a40d5e8db2b34dd6e6c0f851e86b38a3d0840fff1bf14240eff7d3da6/xformers-0.0.22.post7-cp310-cp310-manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading xformers-0.0.22.post7-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.0 kB)\u001b[0m\n",
      "\u001b[35mCollecting optimum (from axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[35mDownloading optimum-1.13.2.tar.gz (300 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 301.0/301.0 kB 48.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mInstalling build dependencies: started\u001b[0m\n",
      "\u001b[34mInstalling build dependencies: finished with status 'done'\u001b[0m\n",
      "\u001b[34mGetting requirements to build wheel: started\u001b[0m\n",
      "\u001b[34mGetting requirements to build wheel: finished with status 'done'\u001b[0m\n",
      "\u001b[34mPreparing metadata (pyproject.toml): started\u001b[0m\n",
      "\u001b[35mInstalling build dependencies: finished with status 'done'\u001b[0m\n",
      "\u001b[35mGetting requirements to build wheel: started\u001b[0m\n",
      "\u001b[35mGetting requirements to build wheel: finished with status 'done'\u001b[0m\n",
      "\u001b[35mPreparing metadata (pyproject.toml): started\u001b[0m\n",
      "\u001b[35mPreparing metadata (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[35mCollecting hf_transfer (from axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[35mDownloading hf_transfer-0.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.9 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.9/3.9 MB 112.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from axolotl==0.3.0->-r requirements.txt (line 6)) (0.4.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: numba in /opt/conda/lib/python3.10/site-packages (from axolotl==0.3.0->-r requirements.txt (line 6)) (0.57.1)\u001b[0m\n",
      "\u001b[35mCollecting bert-score==0.3.13 (from axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[35mDownloading bert_score-0.3.13-py3-none-any.whl (61 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.1/61.1 kB 17.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting evaluate==0.4.0 (from axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[35mDownloading evaluate-0.4.0-py3-none-any.whl (81 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 81.4/81.4 kB 26.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting rouge-score==0.1.2 (from axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[35mDownloading rouge_score-0.1.2.tar.gz (17 kB)\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from axolotl==0.3.0->-r requirements.txt (line 6)) (1.11.2)\u001b[0m\n",
      "\u001b[34mPreparing metadata (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting hf_transfer (from axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading hf_transfer-0.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.9 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.9/3.9 MB 32.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from axolotl==0.3.0->-r requirements.txt (line 6)) (0.4.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numba in /opt/conda/lib/python3.10/site-packages (from axolotl==0.3.0->-r requirements.txt (line 6)) (0.57.1)\u001b[0m\n",
      "\u001b[34mCollecting bert-score==0.3.13 (from axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading bert_score-0.3.13-py3-none-any.whl (61 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.1/61.1 kB 18.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting evaluate==0.4.0 (from axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading evaluate-0.4.0-py3-none-any.whl (81 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 81.4/81.4 kB 26.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting rouge-score==0.1.2 (from axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading rouge_score-0.1.2.tar.gz (17 kB)\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from axolotl==0.3.0->-r requirements.txt (line 6)) (1.11.2)\u001b[0m\n",
      "\u001b[34mCollecting scikit-learn==1.2.2 (from axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading scikit_learn-1.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.6 MB)\u001b[0m\n",
      "\u001b[35mCollecting scikit-learn==1.2.2 (from axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[35mDownloading scikit_learn-1.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.6 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.6/9.6 MB 97.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting pynvml (from axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[35mDownloading pynvml-11.5.0-py3-none-any.whl (53 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.1/53.1 kB 16.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting art (from axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for art from https://files.pythonhosted.org/packages/fc/53/d8792ac2ebb494db0e0ba3ad3f0a9ee71144a5ced266441166f7d038a37e/art-6.1-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading art-6.1-py3-none-any.whl.metadata (69 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 69.9/69.9 kB 23.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting fschat==0.2.29 (from axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for fschat==0.2.29 from https://files.pythonhosted.org/packages/af/99/c2be3ae9473a555923af840236cb8aeb6ecc96680b569f47bc9aa9e3bb6a/fschat-0.2.29-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading fschat-0.2.29-py3-none-any.whl.metadata (19 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pandas>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from bert-score==0.3.13->axolotl==0.3.0->-r requirements.txt (line 6)) (2.1.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from bert-score==0.3.13->axolotl==0.3.0->-r requirements.txt (line 6)) (3.8.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate==0.4.0->axolotl==0.3.0->-r requirements.txt (line 6)) (0.3.7)\u001b[0m\n",
      "\u001b[35mCollecting xxhash (from evaluate==0.4.0->axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for xxhash from https://files.pythonhosted.org/packages/80/8a/1dd41557883b6196f8f092011a5c1f72d4d44cf36d7b67d4a5efe3127949/xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate==0.4.0->axolotl==0.3.0->-r requirements.txt (line 6)) (0.70.15)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from evaluate==0.4.0->axolotl==0.3.0->-r requirements.txt (line 6)) (2023.9.2)\u001b[0m\n",
      "\u001b[35mCollecting huggingface-hub>=0.7.0 (from evaluate==0.4.0->axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for huggingface-hub>=0.7.0 from https://files.pythonhosted.org/packages/ef/b5/b6107bd65fa4c96fdf00e4733e2fe5729bb9e5e09997f63074bb43d3ab28/huggingface_hub-0.18.0-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading huggingface_hub-0.18.0-py3-none-any.whl.metadata (13 kB)\u001b[0m\n",
      "\u001b[35mCollecting responses<0.19 (from evaluate==0.4.0->axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[35mDownloading responses-0.18.0-py3-none-any.whl (38 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.6/9.6 MB 59.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting pynvml (from axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading pynvml-11.5.0-py3-none-any.whl (53 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.1/53.1 kB 11.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting art (from axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for art from https://files.pythonhosted.org/packages/fc/53/d8792ac2ebb494db0e0ba3ad3f0a9ee71144a5ced266441166f7d038a37e/art-6.1-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading art-6.1-py3-none-any.whl.metadata (69 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 69.9/69.9 kB 22.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting fschat==0.2.29 (from axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for fschat==0.2.29 from https://files.pythonhosted.org/packages/af/99/c2be3ae9473a555923af840236cb8aeb6ecc96680b569f47bc9aa9e3bb6a/fschat-0.2.29-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading fschat-0.2.29-py3-none-any.whl.metadata (19 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from bert-score==0.3.13->axolotl==0.3.0->-r requirements.txt (line 6)) (2.1.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from bert-score==0.3.13->axolotl==0.3.0->-r requirements.txt (line 6)) (3.8.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate==0.4.0->axolotl==0.3.0->-r requirements.txt (line 6)) (0.3.7)\u001b[0m\n",
      "\u001b[34mCollecting xxhash (from evaluate==0.4.0->axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for xxhash from https://files.pythonhosted.org/packages/80/8a/1dd41557883b6196f8f092011a5c1f72d4d44cf36d7b67d4a5efe3127949/xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate==0.4.0->axolotl==0.3.0->-r requirements.txt (line 6)) (0.70.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from evaluate==0.4.0->axolotl==0.3.0->-r requirements.txt (line 6)) (2023.9.2)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub>=0.7.0 (from evaluate==0.4.0->axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for huggingface-hub>=0.7.0 from https://files.pythonhosted.org/packages/ef/b5/b6107bd65fa4c96fdf00e4733e2fe5729bb9e5e09997f63074bb43d3ab28/huggingface_hub-0.18.0-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.18.0-py3-none-any.whl.metadata (13 kB)\u001b[0m\n",
      "\u001b[34mCollecting responses<0.19 (from evaluate==0.4.0->axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading responses-0.18.0-py3-none-any.whl (38 kB)\u001b[0m\n",
      "\u001b[34mCollecting aiohttp (from fschat==0.2.29->axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for aiohttp from https://files.pythonhosted.org/packages/41/8e/4c48881316bbced3d13089c4d0df4be321ce79a0c695d82dee9996aaf56b/aiohttp-3.8.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading aiohttp-3.8.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\u001b[0m\n",
      "\u001b[35mCollecting aiohttp (from fschat==0.2.29->axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for aiohttp from https://files.pythonhosted.org/packages/41/8e/4c48881316bbced3d13089c4d0df4be321ce79a0c695d82dee9996aaf56b/aiohttp-3.8.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading aiohttp-3.8.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\u001b[0m\n",
      "\u001b[35mCollecting fastapi (from fschat==0.2.29->axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for fastapi from https://files.pythonhosted.org/packages/db/30/b8d323119c37e15b7fa639e65e0eb7d81eb675ba166ac83e695aad3bd321/fastapi-0.104.0-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading fastapi-0.104.0-py3-none-any.whl.metadata (24 kB)\u001b[0m\n",
      "\u001b[35mCollecting httpx (from fschat==0.2.29->axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for httpx from https://files.pythonhosted.org/packages/33/0d/d9ce469af019741c8999711d36b270ff992ceb1a0293f73f9f34fdf131e9/httpx-0.25.0-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading httpx-0.25.0-py3-none-any.whl.metadata (7.6 kB)\u001b[0m\n",
      "\u001b[35mCollecting markdown2[all] (from fschat==0.2.29->axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for markdown2[all] from https://files.pythonhosted.org/packages/f1/98/61276a753f078dd2f3171c9a69fd3f451d220e806b2b1cdca41b8e368b0f/markdown2-2.4.10-py2.py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading markdown2-2.4.10-py2.py3-none-any.whl.metadata (2.0 kB)\u001b[0m\n",
      "\u001b[35mCollecting nh3 (from fschat==0.2.29->axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for nh3 from https://files.pythonhosted.org/packages/b7/cd/7f64121ec731255265867e0d7d782962f2bd1f15fce83f523c8f6b69463b/nh3-0.2.14-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading nh3-0.2.14-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.6 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: prompt-toolkit>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from fschat==0.2.29->axolotl==0.3.0->-r requirements.txt (line 6)) (3.0.39)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: rich>=10.0.0 in /opt/conda/lib/python3.10/site-packages (from fschat==0.2.29->axolotl==0.3.0->-r requirements.txt (line 6)) (13.5.3)\u001b[0m\n",
      "\u001b[35mCollecting shortuuid (from fschat==0.2.29->axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[35mDownloading shortuuid-1.0.11-py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[35mCollecting tiktoken (from fschat==0.2.29->axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for tiktoken from https://files.pythonhosted.org/packages/f4/2e/0adf6e264b996e263b1c57cad6560ffd5492a69beb9fd779ed0463d486bc/tiktoken-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading tiktoken-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\u001b[0m\n",
      "\u001b[35mCollecting uvicorn (from fschat==0.2.29->axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for uvicorn from https://files.pythonhosted.org/packages/79/96/b0882a1c3f7ef3dd86879e041212ae5b62b4bd352320889231cc735a8e8f/uvicorn-0.23.2-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading uvicorn-0.23.2-py3-none-any.whl.metadata (6.2 kB)\u001b[0m\n",
      "\u001b[35mCollecting nltk (from rouge-score==0.1.2->axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[35mDownloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.5/1.5 MB 109.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn==1.2.2->axolotl==0.3.0->-r requirements.txt (line 6)) (1.3.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn==1.2.2->axolotl==0.3.0->-r requirements.txt (line 6)) (3.2.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn==2.2.1->-r requirements.txt (line 2)) (3.12.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn==2.2.1->-r requirements.txt (line 2)) (1.12)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn==2.2.1->-r requirements.txt (line 2)) (3.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn==2.2.1->-r requirements.txt (line 2)) (3.1.2)\u001b[0m\n",
      "\u001b[34mCollecting fastapi (from fschat==0.2.29->axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for fastapi from https://files.pythonhosted.org/packages/db/30/b8d323119c37e15b7fa639e65e0eb7d81eb675ba166ac83e695aad3bd321/fastapi-0.104.0-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading fastapi-0.104.0-py3-none-any.whl.metadata (24 kB)\u001b[0m\n",
      "\u001b[34mCollecting httpx (from fschat==0.2.29->axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for httpx from https://files.pythonhosted.org/packages/33/0d/d9ce469af019741c8999711d36b270ff992ceb1a0293f73f9f34fdf131e9/httpx-0.25.0-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading httpx-0.25.0-py3-none-any.whl.metadata (7.6 kB)\u001b[0m\n",
      "\u001b[34mCollecting markdown2[all] (from fschat==0.2.29->axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for markdown2[all] from https://files.pythonhosted.org/packages/f1/98/61276a753f078dd2f3171c9a69fd3f451d220e806b2b1cdca41b8e368b0f/markdown2-2.4.10-py2.py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading markdown2-2.4.10-py2.py3-none-any.whl.metadata (2.0 kB)\u001b[0m\n",
      "\u001b[34mCollecting nh3 (from fschat==0.2.29->axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for nh3 from https://files.pythonhosted.org/packages/b7/cd/7f64121ec731255265867e0d7d782962f2bd1f15fce83f523c8f6b69463b/nh3-0.2.14-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading nh3-0.2.14-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.6 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: prompt-toolkit>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from fschat==0.2.29->axolotl==0.3.0->-r requirements.txt (line 6)) (3.0.39)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rich>=10.0.0 in /opt/conda/lib/python3.10/site-packages (from fschat==0.2.29->axolotl==0.3.0->-r requirements.txt (line 6)) (13.5.3)\u001b[0m\n",
      "\u001b[34mCollecting shortuuid (from fschat==0.2.29->axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading shortuuid-1.0.11-py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[34mCollecting tiktoken (from fschat==0.2.29->axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for tiktoken from https://files.pythonhosted.org/packages/f4/2e/0adf6e264b996e263b1c57cad6560ffd5492a69beb9fd779ed0463d486bc/tiktoken-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading tiktoken-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\u001b[0m\n",
      "\u001b[34mCollecting uvicorn (from fschat==0.2.29->axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for uvicorn from https://files.pythonhosted.org/packages/79/96/b0882a1c3f7ef3dd86879e041212ae5b62b4bd352320889231cc735a8e8f/uvicorn-0.23.2-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading uvicorn-0.23.2-py3-none-any.whl.metadata (6.2 kB)\u001b[0m\n",
      "\u001b[34mCollecting nltk (from rouge-score==0.1.2->axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.5/1.5 MB 78.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn==1.2.2->axolotl==0.3.0->-r requirements.txt (line 6)) (1.3.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn==1.2.2->axolotl==0.3.0->-r requirements.txt (line 6)) (3.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn==2.2.1->-r requirements.txt (line 2)) (3.12.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn==2.2.1->-r requirements.txt (line 2)) (1.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn==2.2.1->-r requirements.txt (line 2)) (3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn==2.2.1->-r requirements.txt (line 2)) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->axolotl==0.3.0->-r requirements.txt (line 6)) (13.0.0)\u001b[0m\n",
      "\u001b[34mCollecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for cachetools<6.0,>=2.0.0 from https://files.pythonhosted.org/packages/a2/91/2d843adb9fbd911e0da45fbf6f18ca89d07a087c3daa23e955584f90ebf4/cachetools-5.3.2-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading cachetools-5.3.2-py3-none-any.whl.metadata (5.2 kB)\u001b[0m\n",
      "\u001b[34mCollecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 181.3/181.3 kB 42.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 1)) (4.7.2)\u001b[0m\n",
      "\u001b[34mCollecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<2,>=0.5->tensorboard->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 1)) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 1)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 1)) (1.26.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 1)) (2023.7.22)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->axolotl==0.3.0->-r requirements.txt (line 6)) (13.0.0)\u001b[0m\n",
      "\u001b[35mCollecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for cachetools<6.0,>=2.0.0 from https://files.pythonhosted.org/packages/a2/91/2d843adb9fbd911e0da45fbf6f18ca89d07a087c3daa23e955584f90ebf4/cachetools-5.3.2-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading cachetools-5.3.2-py3-none-any.whl.metadata (5.2 kB)\u001b[0m\n",
      "\u001b[35mCollecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[35mDownloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 181.3/181.3 kB 41.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 1)) (4.7.2)\u001b[0m\n",
      "\u001b[35mCollecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<2,>=0.5->tensorboard->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[35mDownloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 1)) (3.1.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 1)) (3.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 1)) (1.26.15)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 1)) (2023.7.22)\u001b[0m\n",
      "\u001b[35mCollecting regex!=2019.12.17 (from transformers->axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for regex!=2019.12.17 from https://files.pythonhosted.org/packages/8f/3e/4b8b40eb3c80aeaf360f0361d956d129bb3d23b2a3ecbe3a04a8f3bdd6d3/regex-2023.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading regex-2023.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.9/40.9 kB 11.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting tokenizers<0.15,>=0.14 (from transformers->axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for tokenizers<0.15,>=0.14 from https://files.pythonhosted.org/packages/a7/7b/c1f643eb086b6c5c33eef0c3752e37624bd23e4cbc9f1332748f1c6252d1/tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\u001b[0m\n",
      "\u001b[35mCollecting safetensors>=0.3.1 (from transformers->axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for safetensors>=0.3.1 from https://files.pythonhosted.org/packages/20/4e/878b080dbda92666233ec6f316a53969edcb58eab1aa399a64d0521cf953/safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard->-r requirements.txt (line 1)) (2.1.3)\u001b[0m\n",
      "\u001b[35mCollecting rouge (from auto-gptq->axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[35mDownloading rouge-1.0.1-py3-none-any.whl (13 kB)\u001b[0m\n",
      "\u001b[34mCollecting regex!=2019.12.17 (from transformers->axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for regex!=2019.12.17 from https://files.pythonhosted.org/packages/8f/3e/4b8b40eb3c80aeaf360f0361d956d129bb3d23b2a3ecbe3a04a8f3bdd6d3/regex-2023.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading regex-2023.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.9/40.9 kB 12.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting tokenizers<0.15,>=0.14 (from transformers->axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for tokenizers<0.15,>=0.14 from https://files.pythonhosted.org/packages/a7/7b/c1f643eb086b6c5c33eef0c3752e37624bd23e4cbc9f1332748f1c6252d1/tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\u001b[0m\n",
      "\u001b[34mCollecting safetensors>=0.3.1 (from transformers->axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for safetensors>=0.3.1 from https://files.pythonhosted.org/packages/20/4e/878b080dbda92666233ec6f316a53969edcb58eab1aa399a64d0521cf953/safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard->-r requirements.txt (line 1)) (2.1.3)\u001b[0m\n",
      "\u001b[34mCollecting rouge (from auto-gptq->axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading rouge-1.0.1-py3-none-any.whl (13 kB)\u001b[0m\n",
      "\u001b[34mCollecting termcolor (from fire->axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: llvmlite<0.41,>=0.40.0dev0 in /opt/conda/lib/python3.10/site-packages (from numba->axolotl==0.3.0->-r requirements.txt (line 6)) (0.40.1)\u001b[0m\n",
      "\u001b[34mCollecting coloredlogs (from optimum->axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 46.0/46.0 kB 11.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: Click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb->axolotl==0.3.0->-r requirements.txt (line 6)) (8.1.7)\u001b[0m\n",
      "\u001b[34mCollecting GitPython!=3.1.29,>=1.0.0 (from wandb->axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for GitPython!=3.1.29,>=1.0.0 from https://files.pythonhosted.org/packages/8d/c4/82b858fb6483dfb5e338123c154d19c043305b01726a67d89532b8f8f01b/GitPython-3.1.40-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading GitPython-3.1.40-py3-none-any.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[35mCollecting termcolor (from fire->axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[35mDownloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: llvmlite<0.41,>=0.40.0dev0 in /opt/conda/lib/python3.10/site-packages (from numba->axolotl==0.3.0->-r requirements.txt (line 6)) (0.40.1)\u001b[0m\n",
      "\u001b[35mCollecting coloredlogs (from optimum->axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[35mDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 46.0/46.0 kB 16.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: Click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb->axolotl==0.3.0->-r requirements.txt (line 6)) (8.1.7)\u001b[0m\n",
      "\u001b[35mCollecting GitPython!=3.1.29,>=1.0.0 (from wandb->axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for GitPython!=3.1.29,>=1.0.0 from https://files.pythonhosted.org/packages/8d/c4/82b858fb6483dfb5e338123c154d19c043305b01726a67d89532b8f8f01b/GitPython-3.1.40-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading GitPython-3.1.40-py3-none-any.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[35mCollecting sentry-sdk>=1.0.0 (from wandb->axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for sentry-sdk>=1.0.0 from https://files.pythonhosted.org/packages/63/25/d22e1e152e4eac10d39d9132d7b5f1ea4bdfa0b9a1d65fc606a7b90aeefb/sentry_sdk-1.32.0-py2.py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading sentry_sdk-1.32.0-py2.py3-none-any.whl.metadata (9.8 kB)\u001b[0m\n",
      "\u001b[35mCollecting docker-pycreds>=0.4.0 (from wandb->axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[35mDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\u001b[0m\n",
      "\u001b[35mCollecting pathtools (from wandb->axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[35mDownloading pathtools-0.1.2.tar.gz (11 kB)\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mCollecting setproctitle (from wandb->axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for setproctitle from https://files.pythonhosted.org/packages/79/e7/54b36be02aee8ad573be68f6f46fd62838735c2f007b22df50eb5e13a20d/setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\u001b[0m\n",
      "\u001b[35mCollecting appdirs>=1.4.3 (from wandb->axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[35mDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\u001b[0m\n",
      "\u001b[34mCollecting sentry-sdk>=1.0.0 (from wandb->axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for sentry-sdk>=1.0.0 from https://files.pythonhosted.org/packages/63/25/d22e1e152e4eac10d39d9132d7b5f1ea4bdfa0b9a1d65fc606a7b90aeefb/sentry_sdk-1.32.0-py2.py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading sentry_sdk-1.32.0-py2.py3-none-any.whl.metadata (9.8 kB)\u001b[0m\n",
      "\u001b[34mCollecting docker-pycreds>=0.4.0 (from wandb->axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\u001b[0m\n",
      "\u001b[34mCollecting pathtools (from wandb->axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading pathtools-0.1.2.tar.gz (11 kB)\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting setproctitle (from wandb->axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for setproctitle from https://files.pythonhosted.org/packages/79/e7/54b36be02aee8ad573be68f6f46fd62838735c2f007b22df50eb5e13a20d/setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\u001b[0m\n",
      "\u001b[34mCollecting appdirs>=1.4.3 (from wandb->axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\u001b[0m\n",
      "\u001b[34mINFO: pip is looking at multiple versions of xformers to determine which version is compatible with other requirements. This could take a while.\u001b[0m\n",
      "\u001b[34mCollecting xformers (from axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for xformers from https://files.pythonhosted.org/packages/c2/2f/0fa3c080f00c2ebf6836bb1109b9d48b03a4f446e89a058b6c08ba3f7ba1/xformers-0.0.22.post4-cp310-cp310-manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading xformers-0.0.22.post4-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.0 kB)\u001b[0m\n",
      "\u001b[34mObtaining dependency information for xformers from https://files.pythonhosted.org/packages/52/ca/82aeee5dcc24a3429ff5de65cc58ae9695f90f49fbba71755e7fab69a706/xformers-0.0.22-cp310-cp310-manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading xformers-0.0.22-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.0 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->fschat==0.2.29->axolotl==0.3.0->-r requirements.txt (line 6)) (23.1.0)\u001b[0m\n",
      "\u001b[35mINFO: pip is looking at multiple versions of xformers to determine which version is compatible with other requirements. This could take a while.\u001b[0m\n",
      "\u001b[35mCollecting xformers (from axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for xformers from https://files.pythonhosted.org/packages/c2/2f/0fa3c080f00c2ebf6836bb1109b9d48b03a4f446e89a058b6c08ba3f7ba1/xformers-0.0.22.post4-cp310-cp310-manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading xformers-0.0.22.post4-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.0 kB)\u001b[0m\n",
      "\u001b[35mObtaining dependency information for xformers from https://files.pythonhosted.org/packages/52/ca/82aeee5dcc24a3429ff5de65cc58ae9695f90f49fbba71755e7fab69a706/xformers-0.0.22-cp310-cp310-manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading xformers-0.0.22-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.0 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->fschat==0.2.29->axolotl==0.3.0->-r requirements.txt (line 6)) (23.1.0)\u001b[0m\n",
      "\u001b[35mCollecting multidict<7.0,>=4.5 (from aiohttp->fschat==0.2.29->axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[35mDownloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 114.5/114.5 kB 33.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->fschat==0.2.29->axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for async-timeout<5.0,>=4.0.0a3 from https://files.pythonhosted.org/packages/a7/fa/e01228c2938de91d47b307831c62ab9e4001e747789d0b05baf779a6488c/async_timeout-4.0.3-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\u001b[0m\n",
      "\u001b[35mCollecting yarl<2.0,>=1.0 (from aiohttp->fschat==0.2.29->axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[35mDownloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 268.8/268.8 kB 58.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting multidict<7.0,>=4.5 (from aiohttp->fschat==0.2.29->axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 114.5/114.5 kB 33.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->fschat==0.2.29->axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for async-timeout<5.0,>=4.0.0a3 from https://files.pythonhosted.org/packages/a7/fa/e01228c2938de91d47b307831c62ab9e4001e747789d0b05baf779a6488c/async_timeout-4.0.3-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\u001b[0m\n",
      "\u001b[34mCollecting yarl<2.0,>=1.0 (from aiohttp->fschat==0.2.29->axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 268.8/268.8 kB 56.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting frozenlist>=1.1.1 (from aiohttp->fschat==0.2.29->axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for frozenlist>=1.1.1 from https://files.pythonhosted.org/packages/1e/28/74b8b6451c89c070d34e753d8b65a1e4ce508a6808b18529f36e8c0e2184/frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\u001b[0m\n",
      "\u001b[34mCollecting aiosignal>=1.1.2 (from aiohttp->fschat==0.2.29->axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\u001b[0m\n",
      "\u001b[34mCollecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb->axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for gitdb<5,>=4.0.1 from https://files.pythonhosted.org/packages/fd/5b/8f0c4a5bb9fd491c277c21eff7ccae71b47d43c4446c9d0c6cff2fe8c2c4/gitdb-4.0.11-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.1->bert-score==0.3.13->axolotl==0.3.0->-r requirements.txt (line 6)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.1->bert-score==0.3.13->axolotl==0.3.0->-r requirements.txt (line 6)) (2023.3.post1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.1->bert-score==0.3.13->axolotl==0.3.0->-r requirements.txt (line 6)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from prompt-toolkit>=3.0.0->fschat==0.2.29->axolotl==0.3.0->-r requirements.txt (line 6)) (0.2.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 1)) (0.5.0)\u001b[0m\n",
      "\u001b[34mCollecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\u001b[0m\n",
      "\u001b[35mCollecting frozenlist>=1.1.1 (from aiohttp->fschat==0.2.29->axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for frozenlist>=1.1.1 from https://files.pythonhosted.org/packages/1e/28/74b8b6451c89c070d34e753d8b65a1e4ce508a6808b18529f36e8c0e2184/frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\u001b[0m\n",
      "\u001b[35mCollecting aiosignal>=1.1.2 (from aiohttp->fschat==0.2.29->axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[35mDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\u001b[0m\n",
      "\u001b[35mCollecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb->axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for gitdb<5,>=4.0.1 from https://files.pythonhosted.org/packages/fd/5b/8f0c4a5bb9fd491c277c21eff7ccae71b47d43c4446c9d0c6cff2fe8c2c4/gitdb-4.0.11-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.1->bert-score==0.3.13->axolotl==0.3.0->-r requirements.txt (line 6)) (2.8.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.1->bert-score==0.3.13->axolotl==0.3.0->-r requirements.txt (line 6)) (2023.3.post1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.0.1->bert-score==0.3.13->axolotl==0.3.0->-r requirements.txt (line 6)) (2023.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from prompt-toolkit>=3.0.0->fschat==0.2.29->axolotl==0.3.0->-r requirements.txt (line 6)) (0.2.6)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 1)) (0.5.0)\u001b[0m\n",
      "\u001b[35mCollecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[35mDownloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 151.7/151.7 kB 41.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.0.0->fschat==0.2.29->axolotl==0.3.0->-r requirements.txt (line 6)) (3.0.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.0.0->fschat==0.2.29->axolotl==0.3.0->-r requirements.txt (line 6)) (2.16.1)\u001b[0m\n",
      "\u001b[35mCollecting huggingface-hub>=0.7.0 (from evaluate==0.4.0->axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for huggingface-hub>=0.7.0 from https://files.pythonhosted.org/packages/aa/f3/3fc97336a0e90516901befd4f500f08d691034d387406fdbde85bea827cc/huggingface_hub-0.17.3-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading huggingface_hub-0.17.3-py3-none-any.whl.metadata (13 kB)\u001b[0m\n",
      "\u001b[35mCollecting humanfriendly>=9.1 (from coloredlogs->optimum->axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[35mDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 86.8/86.8 kB 24.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting anyio<4.0.0,>=3.7.1 (from fastapi->fschat==0.2.29->axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for anyio<4.0.0,>=3.7.1 from https://files.pythonhosted.org/packages/19/24/44299477fe7dcc9cb58d0a57d5a7588d6af2ff403fdd2d47a246c91a3246/anyio-3.7.1-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading anyio-3.7.1-py3-none-any.whl.metadata (4.7 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 151.7/151.7 kB 42.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.0.0->fschat==0.2.29->axolotl==0.3.0->-r requirements.txt (line 6)) (3.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.0.0->fschat==0.2.29->axolotl==0.3.0->-r requirements.txt (line 6)) (2.16.1)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub>=0.7.0 (from evaluate==0.4.0->axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for huggingface-hub>=0.7.0 from https://files.pythonhosted.org/packages/aa/f3/3fc97336a0e90516901befd4f500f08d691034d387406fdbde85bea827cc/huggingface_hub-0.17.3-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.17.3-py3-none-any.whl.metadata (13 kB)\u001b[0m\n",
      "\u001b[34mCollecting humanfriendly>=9.1 (from coloredlogs->optimum->axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 86.8/86.8 kB 26.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting anyio<4.0.0,>=3.7.1 (from fastapi->fschat==0.2.29->axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for anyio<4.0.0,>=3.7.1 from https://files.pythonhosted.org/packages/19/24/44299477fe7dcc9cb58d0a57d5a7588d6af2ff403fdd2d47a246c91a3246/anyio-3.7.1-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading anyio-3.7.1-py3-none-any.whl.metadata (4.7 kB)\u001b[0m\n",
      "\u001b[34mCollecting starlette<0.28.0,>=0.27.0 (from fastapi->fschat==0.2.29->axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for starlette<0.28.0,>=0.27.0 from https://files.pythonhosted.org/packages/58/f8/e2cca22387965584a409795913b774235752be4176d276714e15e1a58884/starlette-0.27.0-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading starlette-0.27.0-py3-none-any.whl.metadata (5.8 kB)\u001b[0m\n",
      "\u001b[34mCollecting httpcore<0.19.0,>=0.18.0 (from httpx->fschat==0.2.29->axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for httpcore<0.19.0,>=0.18.0 from https://files.pythonhosted.org/packages/ac/97/724afbb7925339f6214bf1fdb5714d1a462690466832bf8fb3fd497649f1/httpcore-0.18.0-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading httpcore-0.18.0-py3-none-any.whl.metadata (18 kB)\u001b[0m\n",
      "\u001b[34mCollecting sniffio (from httpx->fschat==0.2.29->axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading sniffio-1.3.0-py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[34mCollecting wavedrom (from markdown2[all]->fschat==0.2.29->axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading wavedrom-2.0.3.post3.tar.gz (137 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 137.7/137.7 kB 35.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[35mCollecting starlette<0.28.0,>=0.27.0 (from fastapi->fschat==0.2.29->axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for starlette<0.28.0,>=0.27.0 from https://files.pythonhosted.org/packages/58/f8/e2cca22387965584a409795913b774235752be4176d276714e15e1a58884/starlette-0.27.0-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading starlette-0.27.0-py3-none-any.whl.metadata (5.8 kB)\u001b[0m\n",
      "\u001b[35mCollecting httpcore<0.19.0,>=0.18.0 (from httpx->fschat==0.2.29->axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for httpcore<0.19.0,>=0.18.0 from https://files.pythonhosted.org/packages/ac/97/724afbb7925339f6214bf1fdb5714d1a462690466832bf8fb3fd497649f1/httpcore-0.18.0-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading httpcore-0.18.0-py3-none-any.whl.metadata (18 kB)\u001b[0m\n",
      "\u001b[35mCollecting sniffio (from httpx->fschat==0.2.29->axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[35mDownloading sniffio-1.3.0-py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[35mCollecting wavedrom (from markdown2[all]->fschat==0.2.29->axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[35mDownloading wavedrom-2.0.3.post3.tar.gz (137 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 137.7/137.7 kB 31.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert-score==0.3.13->axolotl==0.3.0->-r requirements.txt (line 6)) (1.1.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert-score==0.3.13->axolotl==0.3.0->-r requirements.txt (line 6)) (0.11.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert-score==0.3.13->axolotl==0.3.0->-r requirements.txt (line 6)) (4.42.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert-score==0.3.13->axolotl==0.3.0->-r requirements.txt (line 6)) (1.4.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert-score==0.3.13->axolotl==0.3.0->-r requirements.txt (line 6)) (10.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert-score==0.3.13->axolotl==0.3.0->-r requirements.txt (line 6)) (3.1.1)\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert-score==0.3.13->axolotl==0.3.0->-r requirements.txt (line 6)) (1.1.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert-score==0.3.13->axolotl==0.3.0->-r requirements.txt (line 6)) (0.11.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert-score==0.3.13->axolotl==0.3.0->-r requirements.txt (line 6)) (4.42.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert-score==0.3.13->axolotl==0.3.0->-r requirements.txt (line 6)) (1.4.5)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert-score==0.3.13->axolotl==0.3.0->-r requirements.txt (line 6)) (10.0.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->bert-score==0.3.13->axolotl==0.3.0->-r requirements.txt (line 6)) (3.1.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->flash-attn==2.2.1->-r requirements.txt (line 2)) (1.3.0)\u001b[0m\n",
      "\u001b[35mCollecting h11>=0.8 (from uvicorn->fschat==0.2.29->axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[35mDownloading h11-0.14.0-py3-none-any.whl (58 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.3/58.3 kB 15.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from anyio<4.0.0,>=3.7.1->fastapi->fschat==0.2.29->axolotl==0.3.0->-r requirements.txt (line 6)) (1.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->flash-attn==2.2.1->-r requirements.txt (line 2)) (1.3.0)\u001b[0m\n",
      "\u001b[34mCollecting h11>=0.8 (from uvicorn->fschat==0.2.29->axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading h11-0.14.0-py3-none-any.whl (58 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.3/58.3 kB 19.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from anyio<4.0.0,>=3.7.1->fastapi->fschat==0.2.29->axolotl==0.3.0->-r requirements.txt (line 6)) (1.1.3)\u001b[0m\n",
      "\u001b[34mCollecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb->axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mObtaining dependency information for smmap<6,>=3.0.1 from https://files.pythonhosted.org/packages/a7/a5/10f97f73544edcdef54409f1d839f6049a0d79df68adbc1ceb24d1aaca42/smmap-5.0.1-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[34mDownloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.0.0->fschat==0.2.29->axolotl==0.3.0->-r requirements.txt (line 6)) (0.1.0)\u001b[0m\n",
      "\u001b[34mCollecting svgwrite (from wavedrom->markdown2[all]->fschat==0.2.29->axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading svgwrite-1.4.3-py3-none-any.whl (67 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 67.1/67.1 kB 19.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb->axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[35mObtaining dependency information for smmap<6,>=3.0.1 from https://files.pythonhosted.org/packages/a7/a5/10f97f73544edcdef54409f1d839f6049a0d79df68adbc1ceb24d1aaca42/smmap-5.0.1-py3-none-any.whl.metadata\u001b[0m\n",
      "\u001b[35mDownloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.0.0->fschat==0.2.29->axolotl==0.3.0->-r requirements.txt (line 6)) (0.1.0)\u001b[0m\n",
      "\u001b[35mCollecting svgwrite (from wavedrom->markdown2[all]->fschat==0.2.29->axolotl==0.3.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[35mDownloading svgwrite-1.4.3-py3-none-any.whl (67 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 67.1/67.1 kB 17.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading tensorboard-2.15.0-py3-none-any.whl (5.6 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.6/5.6 MB 117.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading pydantic-1.10.13-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.1/3.1 MB 119.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading fschat-0.2.29-py3-none-any.whl (200 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 200.7/200.7 kB 48.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading absl_py-2.0.0-py3-none-any.whl (130 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 130.2/130.2 kB 40.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading bitsandbytes-0.41.1-py3-none-any.whl (92.6 MB)\u001b[0m\n",
      "\u001b[34mDownloading tensorboard-2.15.0-py3-none-any.whl (5.6 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.6/5.6 MB 85.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading pydantic-1.10.13-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.1/3.1 MB 96.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading fschat-0.2.29-py3-none-any.whl (200 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 200.7/200.7 kB 46.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading absl_py-2.0.0-py3-none-any.whl (130 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 130.2/130.2 kB 35.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading bitsandbytes-0.41.1-py3-none-any.whl (92.6 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 92.6/92.6 MB 25.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading datasets-2.14.6-py3-none-any.whl (493 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 493.7/493.7 kB 60.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading google_auth-2.23.3-py2.py3-none-any.whl (182 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 182.3/182.3 kB 44.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading google_auth_oauthlib-1.1.0-py2.py3-none-any.whl (19 kB)\u001b[0m\n",
      "\u001b[35mDownloading grpcio-1.59.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.3/5.3 MB 43.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading Markdown-3.5-py3-none-any.whl (101 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 101.7/101.7 kB 30.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.6/6.6 MB 110.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading transformers-4.34.1-py3-none-any.whl (7.7 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.7/7.7 MB 111.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading art-6.1-py3-none-any.whl (599 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 599.8/599.8 kB 65.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading auto_gptq-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.8/1.8 MB 91.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading peft-0.5.0-py3-none-any.whl (85 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 92.6/92.6 MB 23.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.14.6-py3-none-any.whl (493 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 493.7/493.7 kB 65.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading google_auth-2.23.3-py2.py3-none-any.whl (182 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 182.3/182.3 kB 49.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading google_auth_oauthlib-1.1.0-py2.py3-none-any.whl (19 kB)\u001b[0m\n",
      "\u001b[34mDownloading grpcio-1.59.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.3/5.3 MB 123.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading Markdown-3.5-py3-none-any.whl (101 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 101.7/101.7 kB 22.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.6/6.6 MB 124.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.34.1-py3-none-any.whl (7.7 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.7/7.7 MB 116.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading art-6.1-py3-none-any.whl (599 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 599.8/599.8 kB 63.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading auto_gptq-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.8/1.8 MB 118.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading peft-0.5.0-py3-none-any.whl (85 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 85.6/85.6 kB 28.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading wandb-0.15.12-py3-none-any.whl (2.1 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.1/2.1 MB 119.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading xformers-0.0.22-cp310-cp310-manylinux2014_x86_64.whl (211.6 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 85.6/85.6 kB 26.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading wandb-0.15.12-py3-none-any.whl (2.1 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.1/2.1 MB 48.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading xformers-0.0.22-cp310-cp310-manylinux2014_x86_64.whl (211.6 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 211.6/211.6 MB 11.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading cachetools-5.3.2-py3-none-any.whl (9.3 kB)\u001b[0m\n",
      "\u001b[35mDownloading aiohttp-3.8.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 82.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading GitPython-3.1.40-py3-none-any.whl (190 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 190.6/190.6 kB 35.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading regex-2023.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 773.9/773.9 kB 85.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 211.6/211.6 MB 11.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading cachetools-5.3.2-py3-none-any.whl (9.3 kB)\u001b[0m\n",
      "\u001b[34mDownloading aiohttp-3.8.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 95.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading GitPython-3.1.40-py3-none-any.whl (190 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 190.6/190.6 kB 47.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading regex-2023.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 773.9/773.9 kB 97.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 99.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading sentry_sdk-1.32.0-py2.py3-none-any.whl (240 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 241.0/241.0 kB 43.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.8/3.8 MB 121.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 295.0/295.0 kB 55.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading fastapi-0.104.0-py3-none-any.whl (92 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 92.9/92.9 kB 25.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading httpx-0.25.0-py3-none-any.whl (75 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 75.7/75.7 kB 24.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading nh3-0.2.14-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/1.7 MB 98.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\u001b[0m\n",
      "\u001b[34mDownloading tiktoken-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.0/2.0 MB 103.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading uvicorn-0.23.2-py3-none-any.whl (59 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 59.5/59.5 kB 19.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 194.1/194.1 kB 48.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading anyio-3.7.1-py3-none-any.whl (80 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 80.9/80.9 kB 29.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\u001b[0m\n",
      "\u001b[34mDownloading frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (225 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 225.7/225.7 kB 47.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading gitdb-4.0.11-py3-none-any.whl (62 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.7/62.7 kB 21.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading httpcore-0.18.0-py3-none-any.whl (76 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 26.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading starlette-0.27.0-py3-none-any.whl (66 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 67.0/67.0 kB 25.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading markdown2-2.4.10-py2.py3-none-any.whl (39 kB)\u001b[0m\n",
      "\u001b[34mDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: flash-attn, rouge-score, fire, optimum, pathtools, wavedrom\u001b[0m\n",
      "\u001b[34mBuilding wheel for flash-attn (setup.py): started\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 89.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading sentry_sdk-1.32.0-py2.py3-none-any.whl (240 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 241.0/241.0 kB 40.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.8/3.8 MB 96.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 295.0/295.0 kB 54.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading fastapi-0.104.0-py3-none-any.whl (92 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 92.9/92.9 kB 28.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading httpx-0.25.0-py3-none-any.whl (75 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 75.7/75.7 kB 25.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading nh3-0.2.14-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/1.7 MB 107.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\u001b[0m\n",
      "\u001b[35mDownloading tiktoken-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.0/2.0 MB 102.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading uvicorn-0.23.2-py3-none-any.whl (59 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 59.5/59.5 kB 12.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 194.1/194.1 kB 41.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading anyio-3.7.1-py3-none-any.whl (80 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 80.9/80.9 kB 25.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\u001b[0m\n",
      "\u001b[35mDownloading frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (225 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 225.7/225.7 kB 47.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading gitdb-4.0.11-py3-none-any.whl (62 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.7/62.7 kB 21.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading httpcore-0.18.0-py3-none-any.whl (76 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.0/76.0 kB 21.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading starlette-0.27.0-py3-none-any.whl (66 kB)\u001b[0m\n",
      "\u001b[35m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 67.0/67.0 kB 20.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mDownloading markdown2-2.4.10-py2.py3-none-any.whl (39 kB)\u001b[0m\n",
      "\u001b[35mDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\u001b[0m\n",
      "\u001b[35mBuilding wheels for collected packages: flash-attn, rouge-score, fire, optimum, pathtools, wavedrom\u001b[0m\n",
      "\u001b[35mBuilding wheel for flash-attn (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for flash-attn (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for flash-attn: filename=flash_attn-2.2.1-cp310-cp310-linux_x86_64.whl size=193960420 sha256=90f73bf1b0351f988d545836abd0f27aca8f72040720003809003b0ed40685a0\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/29/3d/d0/ca80c9c6061afa997cd0246a982fd905ef3a1837c8442a0050\u001b[0m\n",
      "\u001b[34mBuilding wheel for rouge-score (setup.py): started\u001b[0m\n",
      "\u001b[35mBuilding wheel for flash-attn (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mCreated wheel for flash-attn: filename=flash_attn-2.2.1-cp310-cp310-linux_x86_64.whl size=193960420 sha256=90f73bf1b0351f988d545836abd0f27aca8f72040720003809003b0ed40685a0\u001b[0m\n",
      "\u001b[35mStored in directory: /root/.cache/pip/wheels/29/3d/d0/ca80c9c6061afa997cd0246a982fd905ef3a1837c8442a0050\u001b[0m\n",
      "\u001b[35mBuilding wheel for rouge-score (setup.py): started\u001b[0m\n",
      "\u001b[35mBuilding wheel for rouge-score (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mCreated wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=48a0a48ca54aa213b0b2d29d25ac033164238c509c0ce91c7beca2294239db03\u001b[0m\n",
      "\u001b[35mStored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\u001b[0m\n",
      "\u001b[35mBuilding wheel for fire (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for rouge-score (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=48a0a48ca54aa213b0b2d29d25ac033164238c509c0ce91c7beca2294239db03\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\u001b[0m\n",
      "\u001b[34mBuilding wheel for fire (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for fire (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116932 sha256=27b41838e294f3feef012e89bfd5c67662b7f806ffef49c433a47c0ec11a66c0\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/90/d4/f7/9404e5db0116bd4d43e5666eaa3e70ab53723e1e3ea40c9a95\u001b[0m\n",
      "\u001b[34mBuilding wheel for optimum (pyproject.toml): started\u001b[0m\n",
      "\u001b[35mBuilding wheel for fire (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mCreated wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116932 sha256=27b41838e294f3feef012e89bfd5c67662b7f806ffef49c433a47c0ec11a66c0\u001b[0m\n",
      "\u001b[35mStored in directory: /root/.cache/pip/wheels/90/d4/f7/9404e5db0116bd4d43e5666eaa3e70ab53723e1e3ea40c9a95\u001b[0m\n",
      "\u001b[35mBuilding wheel for optimum (pyproject.toml): started\u001b[0m\n",
      "\u001b[35mBuilding wheel for optimum (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[35mCreated wheel for optimum: filename=optimum-1.13.2-py3-none-any.whl size=395599 sha256=62138071cf6acdb8b651005575498f569e3b9217ac5c6bb8363966e8c413bc64\u001b[0m\n",
      "\u001b[35mStored in directory: /root/.cache/pip/wheels/6e/b7/2c/79405d98f0943373d8546daeae25a3d377f7659ca0cbe48699\u001b[0m\n",
      "\u001b[35mBuilding wheel for pathtools (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for optimum (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for optimum: filename=optimum-1.13.2-py3-none-any.whl size=395599 sha256=62138071cf6acdb8b651005575498f569e3b9217ac5c6bb8363966e8c413bc64\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/6e/b7/2c/79405d98f0943373d8546daeae25a3d377f7659ca0cbe48699\u001b[0m\n",
      "\u001b[34mBuilding wheel for pathtools (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for pathtools (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=d34be300d9379e991ca3d498366bc757a9f7bec3869ba7f250b887ede1cc9af9\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\u001b[0m\n",
      "\u001b[34mBuilding wheel for wavedrom (setup.py): started\u001b[0m\n",
      "\u001b[35mBuilding wheel for pathtools (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mCreated wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=d34be300d9379e991ca3d498366bc757a9f7bec3869ba7f250b887ede1cc9af9\u001b[0m\n",
      "\u001b[35mStored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\u001b[0m\n",
      "\u001b[35mBuilding wheel for wavedrom (setup.py): started\u001b[0m\n",
      "\u001b[35mBuilding wheel for wavedrom (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mCreated wheel for wavedrom: filename=wavedrom-2.0.3.post3-py2.py3-none-any.whl size=30053 sha256=77c1d61efac77f05fddf67202c267a656e8df3bab2d131e65b6e8dfd37ac1843\u001b[0m\n",
      "\u001b[35mStored in directory: /root/.cache/pip/wheels/9c/52/8c/38b454b42f712f325e26f633287484c7dc1ad469e1580c5954\u001b[0m\n",
      "\u001b[35mSuccessfully built flash-attn rouge-score fire optimum pathtools wavedrom\u001b[0m\n",
      "\u001b[34mBuilding wheel for wavedrom (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for wavedrom: filename=wavedrom-2.0.3.post3-py2.py3-none-any.whl size=30053 sha256=77c1d61efac77f05fddf67202c267a656e8df3bab2d131e65b6e8dfd37ac1843\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/9c/52/8c/38b454b42f712f325e26f633287484c7dc1ad469e1580c5954\u001b[0m\n",
      "\u001b[34mSuccessfully built flash-attn rouge-score fire optimum pathtools wavedrom\u001b[0m\n",
      "\u001b[35mInstalling collected packages: sentencepiece, pathtools, nh3, bitsandbytes, appdirs, addict, xxhash, termcolor, tensorboard-data-server, svgwrite, sniffio, smmap, shortuuid, setproctitle, sentry-sdk, safetensors, rouge, regex, pynvml, pydantic, pyasn1-modules, oauthlib, multidict, markdown2, markdown, humanfriendly, hf_transfer, h11, grpcio, frozenlist, docker-pycreds, cachetools, async-timeout, art, absl-py, yarl, wavedrom, uvicorn, tiktoken, scikit-learn, responses, requests-oauthlib, nltk, huggingface-hub, google-auth, gitdb, fire, coloredlogs, anyio, aiosignal, xformers, tokenizers, starlette, rouge-score, httpcore, google-auth-oauthlib, GitPython, flash-attn, deepspeed, aiohttp, wandb, transformers, tensorboard, httpx, fastapi, peft, fschat, datasets, bert-score, optimum, evaluate, auto-gptq, axolotl\u001b[0m\n",
      "\u001b[34mInstalling collected packages: sentencepiece, pathtools, nh3, bitsandbytes, appdirs, addict, xxhash, termcolor, tensorboard-data-server, svgwrite, sniffio, smmap, shortuuid, setproctitle, sentry-sdk, safetensors, rouge, regex, pynvml, pydantic, pyasn1-modules, oauthlib, multidict, markdown2, markdown, humanfriendly, hf_transfer, h11, grpcio, frozenlist, docker-pycreds, cachetools, async-timeout, art, absl-py, yarl, wavedrom, uvicorn, tiktoken, scikit-learn, responses, requests-oauthlib, nltk, huggingface-hub, google-auth, gitdb, fire, coloredlogs, anyio, aiosignal, xformers, tokenizers, starlette, rouge-score, httpcore, google-auth-oauthlib, GitPython, flash-attn, deepspeed, aiohttp, wandb, transformers, tensorboard, httpx, fastapi, peft, fschat, datasets, bert-score, optimum, evaluate, auto-gptq, axolotl\u001b[0m\n",
      "\u001b[35mAttempting uninstall: pydantic\u001b[0m\n",
      "\u001b[35mFound existing installation: pydantic 2.4.1\u001b[0m\n",
      "\u001b[35mUninstalling pydantic-2.4.1:\u001b[0m\n",
      "\u001b[35mSuccessfully uninstalled pydantic-2.4.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: pydantic\u001b[0m\n",
      "\u001b[34mFound existing installation: pydantic 2.4.1\u001b[0m\n",
      "\u001b[34mUninstalling pydantic-2.4.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled pydantic-2.4.1\u001b[0m\n",
      "\u001b[35mAttempting uninstall: scikit-learn\u001b[0m\n",
      "\u001b[35mFound existing installation: scikit-learn 1.3.1\u001b[0m\n",
      "\u001b[35mUninstalling scikit-learn-1.3.1:\u001b[0m\n",
      "\u001b[34mAttempting uninstall: scikit-learn\u001b[0m\n",
      "\u001b[34mFound existing installation: scikit-learn 1.3.1\u001b[0m\n",
      "\u001b[34mUninstalling scikit-learn-1.3.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled scikit-learn-1.3.1\u001b[0m\n",
      "\u001b[35mSuccessfully uninstalled scikit-learn-1.3.1\u001b[0m\n",
      "\u001b[35mAttempting uninstall: flash-attn\u001b[0m\n",
      "\u001b[35mFound existing installation: flash-attn 0.2.8\u001b[0m\n",
      "\u001b[35mUninstalling flash-attn-0.2.8:\u001b[0m\n",
      "\u001b[35mSuccessfully uninstalled flash-attn-0.2.8\u001b[0m\n",
      "\u001b[34mAttempting uninstall: flash-attn\u001b[0m\n",
      "\u001b[34mFound existing installation: flash-attn 0.2.8\u001b[0m\n",
      "\u001b[34mUninstalling flash-attn-0.2.8:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled flash-attn-0.2.8\u001b[0m\n",
      "\u001b[35mAttempting uninstall: deepspeed\u001b[0m\n",
      "\u001b[35mFound existing installation: deepspeed 0.6.1+1ea3d4b\u001b[0m\n",
      "\u001b[35mUninstalling deepspeed-0.6.1+1ea3d4b:\u001b[0m\n",
      "\u001b[35mSuccessfully uninstalled deepspeed-0.6.1+1ea3d4b\u001b[0m\n",
      "\u001b[34mAttempting uninstall: deepspeed\u001b[0m\n",
      "\u001b[34mFound existing installation: deepspeed 0.6.1+1ea3d4b\u001b[0m\n",
      "\u001b[34mUninstalling deepspeed-0.6.1+1ea3d4b:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled deepspeed-0.6.1+1ea3d4b\u001b[0m\n",
      "\u001b[35mRunning setup.py develop for axolotl\u001b[0m\n",
      "\u001b[34mRunning setup.py develop for axolotl\u001b[0m\n",
      "\u001b[35mSuccessfully installed GitPython-3.1.40 absl-py-2.0.0 addict-2.4.0 aiohttp-3.8.6 aiosignal-1.3.1 anyio-3.7.1 appdirs-1.4.4 art-6.1 async-timeout-4.0.3 auto-gptq-0.4.2 axolotl-0.3.0 bert-score-0.3.13 bitsandbytes-0.41.1 cachetools-5.3.2 coloredlogs-15.0.1 datasets-2.14.6 deepspeed-0.11.1 docker-pycreds-0.4.0 evaluate-0.4.0 fastapi-0.104.0 fire-0.5.0 flash-attn-2.2.1 frozenlist-1.4.0 fschat-0.2.29 gitdb-4.0.11 google-auth-2.23.3 google-auth-oauthlib-1.1.0 grpcio-1.59.0 h11-0.14.0 hf_transfer-0.1.3 httpcore-0.18.0 httpx-0.25.0 huggingface-hub-0.17.3 humanfriendly-10.0 markdown-3.5 markdown2-2.4.10 multidict-6.0.4 nh3-0.2.14 nltk-3.8.1 oauthlib-3.2.2 optimum-1.13.2 pathtools-0.1.2 peft-0.5.0 pyasn1-modules-0.3.0 pydantic-1.10.13 pynvml-11.5.0 regex-2023.10.3 requests-oauthlib-1.3.1 responses-0.18.0 rouge-1.0.1 rouge-score-0.1.2 safetensors-0.4.0 scikit-learn-1.2.2 sentencepiece-0.1.99 sentry-sdk-1.32.0 setproctitle-1.3.3 shortuuid-1.0.11 smmap-5.0.1 sniffio-1.3.0 starlette-0.27.0 svgwrite-1.4.3 tensorboard-2.15.0 tensorboard-data-server-0.7.2 termcolor-2.3.0 tiktoken-0.5.1 tokenizers-0.14.1 transformers-4.34.1 uvicorn-0.23.2 wandb-0.15.12 wavedrom-2.0.3.post3 xformers-0.0.22 xxhash-3.4.1 yarl-1.9.2\u001b[0m\n",
      "\u001b[35mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[35m[notice] A new release of pip is available: 23.2.1 -> 23.3.1\u001b[0m\n",
      "\u001b[35m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[35m2023-10-27 21:36:06,254 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[35m2023-10-27 21:36:06,254 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[35m2023-10-27 21:36:06,291 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2023-10-27 21:36:06,314 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2023-10-27 21:36:06,323 sagemaker-training-toolkit INFO     Starting distributed training through torchrun\u001b[0m\n",
      "\u001b[35m2023-10-27 21:36:06,337 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34mSuccessfully installed GitPython-3.1.40 absl-py-2.0.0 addict-2.4.0 aiohttp-3.8.6 aiosignal-1.3.1 anyio-3.7.1 appdirs-1.4.4 art-6.1 async-timeout-4.0.3 auto-gptq-0.4.2 axolotl-0.3.0 bert-score-0.3.13 bitsandbytes-0.41.1 cachetools-5.3.2 coloredlogs-15.0.1 datasets-2.14.6 deepspeed-0.11.1 docker-pycreds-0.4.0 evaluate-0.4.0 fastapi-0.104.0 fire-0.5.0 flash-attn-2.2.1 frozenlist-1.4.0 fschat-0.2.29 gitdb-4.0.11 google-auth-2.23.3 google-auth-oauthlib-1.1.0 grpcio-1.59.0 h11-0.14.0 hf_transfer-0.1.3 httpcore-0.18.0 httpx-0.25.0 huggingface-hub-0.17.3 humanfriendly-10.0 markdown-3.5 markdown2-2.4.10 multidict-6.0.4 nh3-0.2.14 nltk-3.8.1 oauthlib-3.2.2 optimum-1.13.2 pathtools-0.1.2 peft-0.5.0 pyasn1-modules-0.3.0 pydantic-1.10.13 pynvml-11.5.0 regex-2023.10.3 requests-oauthlib-1.3.1 responses-0.18.0 rouge-1.0.1 rouge-score-0.1.2 safetensors-0.4.0 scikit-learn-1.2.2 sentencepiece-0.1.99 sentry-sdk-1.32.0 setproctitle-1.3.3 shortuuid-1.0.11 smmap-5.0.1 sniffio-1.3.0 starlette-0.27.0 svgwrite-1.4.3 tensorboard-2.15.0 tensorboard-data-server-0.7.2 termcolor-2.3.0 tiktoken-0.5.1 tokenizers-0.14.1 transformers-4.34.1 uvicorn-0.23.2 wandb-0.15.12 wavedrom-2.0.3.post3 xformers-0.0.22 xxhash-3.4.1 yarl-1.9.2\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.2.1 -> 23.3.1\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2023-10-27 21:36:06,845 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-10-27 21:36:06,845 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-10-27 21:36:06,883 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2023-10-27 21:36:06,346 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[35mTraining Env:\u001b[0m\n",
      "\u001b[35m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_instance_type\": \"ml.g5.2xlarge\",\n",
      "        \"sagemaker_torch_distributed_enabled\": true\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"model\": \"/opt/ml/input/data/model\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-2\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.2xlarge\",\n",
      "    \"distribution_hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"distribution_instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"config\": \"code-llama-7b-qlora.yml\",\n",
      "        \"deepspeed\": \"axolotl/deepspeed/zero2.json\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"model\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.2xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\",\n",
      "                \"algo-2\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": false,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"pytorch-training-2023-10-27-21-24-37-446\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-376678947624/pytorch-training-2023-10-27-21-24-37-446/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"axolotl/src/axolotl/cli/train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-2\",\n",
      "        \"current_instance_type\": \"ml.g5.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\",\n",
      "                    \"algo-2\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"axolotl/src/axolotl/cli/train.py\"\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35mEnvironment variables:\u001b[0m\n",
      "\u001b[35mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[35mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[35mSM_HPS={\"config\":\"code-llama-7b-qlora.yml\",\"deepspeed\":\"axolotl/deepspeed/zero2.json\"}\u001b[0m\n",
      "\u001b[35mSM_USER_ENTRY_POINT=axolotl/src/axolotl/cli/train.py\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_PARAMS={\"sagemaker_instance_type\":\"ml.g5.2xlarge\",\"sagemaker_torch_distributed_enabled\":true}\u001b[0m\n",
      "\u001b[35mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-2\",\"current_instance_type\":\"ml.g5.2xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[35mSM_INPUT_DATA_CONFIG={\"model\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[35mSM_CHANNELS=[\"model\",\"train\"]\u001b[0m\n",
      "\u001b[35mSM_CURRENT_HOST=algo-2\u001b[0m\n",
      "\u001b[35mSM_CURRENT_INSTANCE_TYPE=ml.g5.2xlarge\u001b[0m\n",
      "\u001b[35mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[35mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[35mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[35mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}}\u001b[0m\n",
      "\u001b[35mSM_DISTRIBUTION_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[35mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[35mSM_MODULE_NAME=axolotl/src/axolotl/cli/train\u001b[0m\n",
      "\u001b[35mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[35mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[35mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[35mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[35mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[35mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[35mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[35mSM_MODULE_DIR=s3://sagemaker-us-west-2-376678947624/pytorch-training-2023-10-27-21-24-37-446/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[35mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_instance_type\":\"ml.g5.2xlarge\",\"sagemaker_torch_distributed_enabled\":true},\"channel_input_dirs\":{\"model\":\"/opt/ml/input/data/model\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-2\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\",\"algo-2\"],\"current_instance_type\":\"ml.g5.2xlarge\",\"distribution_hosts\":[\"algo-1\",\"algo-2\"],\"distribution_instance_groups\":[\"homogeneousCluster\"],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"config\":\"code-llama-7b-qlora.yml\",\"deepspeed\":\"axolotl/deepspeed/zero2.json\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"model\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}},\"is_hetero\":false,\"is_master\":false,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"pytorch-training-2023-10-27-21-24-37-446\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-376678947624/pytorch-training-2023-10-27-21-24-37-446/source/sourcedir.tar.gz\",\"module_name\":\"axolotl/src/axolotl/cli/train\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-2\",\"current_instance_type\":\"ml.g5.2xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"axolotl/src/axolotl/cli/train.py\"}\u001b[0m\n",
      "\u001b[35mSM_USER_ARGS=[\"--config\",\"code-llama-7b-qlora.yml\",\"--deepspeed\",\"axolotl/deepspeed/zero2.json\"]\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[35mSM_CHANNEL_MODEL=/opt/ml/input/data/model\u001b[0m\n",
      "\u001b[35mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[35mSM_HP_CONFIG=code-llama-7b-qlora.yml\u001b[0m\n",
      "\u001b[35mSM_HP_DEEPSPEED=axolotl/deepspeed/zero2.json\u001b[0m\n",
      "\u001b[35mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[35mInvoking script with the following command:\u001b[0m\n",
      "\u001b[35mtorchrun --nnodes 2 --nproc_per_node 1 --master_addr algo-1 --master_port 7777 --node_rank 1 axolotl/src/axolotl/cli/train.py --config code-llama-7b-qlora.yml --deepspeed axolotl/deepspeed/zero2.json\u001b[0m\n",
      "\u001b[34m2023-10-27 21:36:06,906 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-10-27 21:36:06,915 sagemaker-training-toolkit INFO     Starting distributed training through torchrun\u001b[0m\n",
      "\u001b[34m2023-10-27 21:36:06,929 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-10-27 21:36:06,938 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_instance_type\": \"ml.g5.2xlarge\",\n",
      "        \"sagemaker_torch_distributed_enabled\": true\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"model\": \"/opt/ml/input/data/model\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.2xlarge\",\n",
      "    \"distribution_hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"distribution_instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"config\": \"code-llama-7b-qlora.yml\",\n",
      "        \"deepspeed\": \"axolotl/deepspeed/zero2.json\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"model\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.2xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\",\n",
      "                \"algo-2\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"pytorch-training-2023-10-27-21-24-37-446\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-376678947624/pytorch-training-2023-10-27-21-24-37-446/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"axolotl/src/axolotl/cli/train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\",\n",
      "                    \"algo-2\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"axolotl/src/axolotl/cli/train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"config\":\"code-llama-7b-qlora.yml\",\"deepspeed\":\"axolotl/deepspeed/zero2.json\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=axolotl/src/axolotl/cli/train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={\"sagemaker_instance_type\":\"ml.g5.2xlarge\",\"sagemaker_torch_distributed_enabled\":true}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.2xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"model\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"model\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g5.2xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=axolotl/src/axolotl/cli/train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-west-2-376678947624/pytorch-training-2023-10-27-21-24-37-446/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_instance_type\":\"ml.g5.2xlarge\",\"sagemaker_torch_distributed_enabled\":true},\"channel_input_dirs\":{\"model\":\"/opt/ml/input/data/model\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\",\"algo-2\"],\"current_instance_type\":\"ml.g5.2xlarge\",\"distribution_hosts\":[\"algo-1\",\"algo-2\"],\"distribution_instance_groups\":[\"homogeneousCluster\"],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"config\":\"code-llama-7b-qlora.yml\",\"deepspeed\":\"axolotl/deepspeed/zero2.json\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"model\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"pytorch-training-2023-10-27-21-24-37-446\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-376678947624/pytorch-training-2023-10-27-21-24-37-446/source/sourcedir.tar.gz\",\"module_name\":\"axolotl/src/axolotl/cli/train\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.2xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"axolotl/src/axolotl/cli/train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--config\",\"code-llama-7b-qlora.yml\",\"--deepspeed\",\"axolotl/deepspeed/zero2.json\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_MODEL=/opt/ml/input/data/model\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_CONFIG=code-llama-7b-qlora.yml\u001b[0m\n",
      "\u001b[34mSM_HP_DEEPSPEED=axolotl/deepspeed/zero2.json\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34mtorchrun --nnodes 2 --nproc_per_node 1 --master_addr algo-1 --master_port 7777 --node_rank 0 axolotl/src/axolotl/cli/train.py --config code-llama-7b-qlora.yml --deepspeed axolotl/deepspeed/zero2.json\u001b[0m\n",
      "\u001b[35mThe cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\u001b[0m\n",
      "\u001b[35m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[35m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34mThe cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\u001b[0m\n",
      "\u001b[34m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[34m0it [00:00, ?it/s]\u001b[0m\n",
      "\u001b[35m[2023-10-27 21:36:10,929] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2023-10-27 21:36:10,930] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34mdP            dP   dP \n",
      "                              88            88   88 \n",
      "   .d8888b. dP.  .dP .d8888b. 88 .d8888b. d8888P 88 \n",
      "   88'  `88  `8bd8'  88'  `88 88 88'  `88   88   88 \n",
      "   88.  .88  .d88b.  88.  .88 88 88.  .88   88   88 \n",
      "   `88888P8 dP'  `dP `88888P' dP `88888P'   dP   dP\u001b[0m\n",
      "\u001b[34m#033[33m[2023-10-27 21:36:17,678] [WARNING] [axolotl.validate_config:163] [PID:197] [RANK:0] eval_batch_size != micro_batch_size. This can lead to VRAM instability.#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 21:36:17,680] [INFO] [axolotl.normalize_config:122] [PID:197] [RANK:0] GPU memory usage baseline: 0.000GB (+0.302GB misc)#033[39m\u001b[0m\n",
      "\u001b[34m#033[33m[2023-10-27 21:36:17,680] [WARNING] [axolotl.scripts.check_user_token:261] [PID:197] [RANK:0] Error verifying HuggingFace token. Remember to log in using `huggingface-cli login` and get your access token from https://huggingface.co/settings/tokens if you want to use gated models or datasets.#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 21:36:17,816] [DEBUG] [axolotl.load_tokenizer:74] [PID:197] [RANK:0] EOS: 2 / </s>#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 21:36:17,816] [DEBUG] [axolotl.load_tokenizer:75] [PID:197] [RANK:0] BOS: 1 / <s>#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 21:36:17,816] [DEBUG] [axolotl.load_tokenizer:76] [PID:197] [RANK:0] PAD: 2 / </s>#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 21:36:17,816] [DEBUG] [axolotl.load_tokenizer:77] [PID:197] [RANK:0] UNK: 0 / <unk>#033[39m\u001b[0m\n",
      "\u001b[35m#033[33m[2023-10-27 21:36:17,682] [WARNING] [axolotl.validate_config:163] [PID:196] [RANK:0] eval_batch_size != micro_batch_size. This can lead to VRAM instability.#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 21:36:17,684] [INFO] [axolotl.normalize_config:122] [PID:196] [RANK:0] GPU memory usage baseline: 0.000GB (+0.302GB misc)#033[39m\u001b[0m\n",
      "\u001b[35m#033[33m[2023-10-27 21:36:17,684] [WARNING] [axolotl.scripts.check_user_token:261] [PID:196] [RANK:0] Error verifying HuggingFace token. Remember to log in using `huggingface-cli login` and get your access token from https://huggingface.co/settings/tokens if you want to use gated models or datasets.#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 21:36:17,818] [DEBUG] [axolotl.load_tokenizer:74] [PID:196] [RANK:0] EOS: 2 / </s>#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 21:36:17,818] [DEBUG] [axolotl.load_tokenizer:75] [PID:196] [RANK:0] BOS: 1 / <s>#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 21:36:17,818] [DEBUG] [axolotl.load_tokenizer:76] [PID:196] [RANK:0] PAD: 2 / </s>#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 21:36:17,818] [DEBUG] [axolotl.load_tokenizer:77] [PID:196] [RANK:0] UNK: 0 / <unk>#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 21:36:17,958] [INFO] [axolotl.load_tokenized_prepared_datasets:130] [PID:197] [RANK:0] Unable to find prepared dataset in /tmp/last_run_prepared/602d3a7380f73b0eb426ee7159f348f4#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 21:36:17,958] [INFO] [axolotl.load_tokenized_prepared_datasets:131] [PID:197] [RANK:0] Loading raw datasets...#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 21:36:17,958] [INFO] [axolotl.load_tokenized_prepared_datasets:136] [PID:197] [RANK:0] No seed provided, using default seed of 42#033[39m\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/datasets/load.py:2089: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\u001b[0m\n",
      "\u001b[34mYou can remove this warning by passing 'token=None' instead.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mDownloading data files:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading data files: 100%|██████████| 1/1 [00:00<00:00, 14074.85it/s]\u001b[0m\n",
      "\u001b[34mExtracting data files:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mExtracting data files: 100%|██████████| 1/1 [00:00<00:00, 1000.31it/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 78577 examples [00:00, 1301421.26 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):   0%|          | 0/78577 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):   0%|          | 16/78577 [00:00<17:29, 74.82 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):   0%|          | 226/78577 [00:00<01:32, 844.74 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):   1%|          | 461/78577 [00:00<00:58, 1339.13 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):   1%|          | 635/78577 [00:00<00:53, 1449.19 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):   1%|          | 844/78577 [00:00<00:49, 1557.39 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):   1%|▏         | 1041/78577 [00:00<00:46, 1677.70 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):   2%|▏         | 1228/78577 [00:00<00:47, 1640.21 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):   2%|▏         | 1422/78577 [00:00<00:45, 1703.26 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):   2%|▏         | 1617/78577 [00:01<00:43, 1770.62 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):   2%|▏         | 1801/78577 [00:01<00:43, 1753.93 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):   3%|▎         | 1992/78577 [00:01<00:43, 1759.85 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):   3%|▎         | 2183/78577 [00:01<00:42, 1800.30 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):   3%|▎         | 2368/78577 [00:01<00:42, 1777.12 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):   3%|▎         | 2554/78577 [00:01<00:42, 1780.51 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):   3%|▎         | 2738/78577 [00:01<00:42, 1791.85 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):   4%|▎         | 2923/78577 [00:01<00:41, 1806.62 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):   4%|▍         | 3112/78577 [00:01<00:42, 1778.34 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):   4%|▍         | 3303/78577 [00:02<00:42, 1775.81 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):   4%|▍         | 3481/78577 [00:02<00:42, 1773.54 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):   5%|▍         | 3671/78577 [00:02<00:43, 1730.30 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):   5%|▍         | 3876/78577 [00:02<00:41, 1796.28 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):   5%|▌         | 4058/78577 [00:02<00:43, 1731.79 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):   5%|▌         | 4241/78577 [00:02<00:42, 1734.84 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):   6%|▌         | 4449/78577 [00:02<00:41, 1772.00 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):   6%|▌         | 4650/78577 [00:02<00:40, 1817.36 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):   6%|▌         | 4846/78577 [00:02<00:40, 1799.96 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):   6%|▋         | 5030/78577 [00:03<00:42, 1725.30 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):   7%|▋         | 5254/78577 [00:03<00:41, 1779.23 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):   7%|▋         | 5452/78577 [00:03<00:40, 1812.37 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):   7%|▋         | 5637/78577 [00:03<00:41, 1764.11 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):   7%|▋         | 5849/78577 [00:03<00:40, 1781.01 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):   8%|▊         | 6072/78577 [00:03<00:38, 1899.21 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):   8%|▊         | 6277/78577 [00:03<00:40, 1789.69 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):   8%|▊         | 6474/78577 [00:03<00:40, 1777.10 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):   8%|▊         | 6670/78577 [00:03<00:39, 1815.98 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):   9%|▊         | 6856/78577 [00:04<00:41, 1744.13 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):   9%|▉         | 7039/78577 [00:04<00:42, 1698.83 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):   9%|▉         | 7231/78577 [00:04<00:40, 1744.07 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):   9%|▉         | 7429/78577 [00:04<00:40, 1762.05 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  10%|▉         | 7610/78577 [00:04<00:44, 1593.38 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  10%|▉         | 7782/78577 [00:04<00:43, 1609.33 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  10%|█         | 7953/78577 [00:04<00:44, 1586.28 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  10%|█         | 8117/78577 [00:04<00:51, 1355.11 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  11%|█         | 8262/78577 [00:04<00:53, 1318.01 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  11%|█         | 8419/78577 [00:05<00:51, 1373.14 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  11%|█         | 8609/78577 [00:05<00:47, 1481.70 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  11%|█         | 8802/78577 [00:05<00:44, 1569.06 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  11%|█▏        | 8994/78577 [00:05<00:43, 1591.28 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  12%|█▏        | 9190/78577 [00:05<00:41, 1690.84 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  12%|█▏        | 9362/78577 [00:05<00:41, 1664.91 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  12%|█▏        | 9549/78577 [00:05<00:40, 1684.71 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  12%|█▏        | 9749/78577 [00:05<00:38, 1773.60 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  13%|█▎        | 9932/78577 [00:05<00:39, 1730.50 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  13%|█▎        | 10128/78577 [00:06<00:39, 1735.21 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  13%|█▎        | 10325/78577 [00:06<00:38, 1784.92 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  13%|█▎        | 10517/78577 [00:06<00:38, 1758.17 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  14%|█▎        | 10698/78577 [00:06<00:38, 1755.03 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  14%|█▍        | 10908/78577 [00:06<00:36, 1834.90 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  14%|█▍        | 11103/78577 [00:06<00:37, 1805.42 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  14%|█▍        | 11296/78577 [00:06<00:38, 1737.52 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  15%|█▍        | 11493/78577 [00:06<00:38, 1746.90 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  15%|█▍        | 11694/78577 [00:06<00:37, 1800.50 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  15%|█▌        | 11877/78577 [00:07<00:37, 1787.21 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  15%|█▌        | 12062/78577 [00:07<00:38, 1744.99 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  16%|█▌        | 12257/78577 [00:07<00:37, 1781.68 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  16%|█▌        | 12441/78577 [00:07<00:37, 1780.12 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  16%|█▌        | 12627/78577 [00:07<00:37, 1739.80 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  16%|█▋        | 12818/78577 [00:07<00:37, 1774.06 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  17%|█▋        | 13008/78577 [00:07<00:37, 1769.89 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  17%|█▋        | 13216/78577 [00:07<00:36, 1802.24 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  17%|█▋        | 13402/78577 [00:07<00:37, 1749.79 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  17%|█▋        | 13594/78577 [00:08<00:36, 1760.32 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  18%|█▊        | 13793/78577 [00:08<00:36, 1768.51 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  18%|█▊        | 13978/78577 [00:08<00:36, 1783.68 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  18%|█▊        | 14171/78577 [00:08<00:37, 1731.25 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  18%|█▊        | 14375/78577 [00:08<00:35, 1811.39 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  19%|█▊        | 14559/78577 [00:08<00:35, 1800.75 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  19%|█▉        | 14743/78577 [00:08<00:36, 1755.26 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  19%|█▉        | 14926/78577 [00:08<00:37, 1717.31 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  19%|█▉        | 15099/78577 [00:08<00:37, 1701.77 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  19%|█▉        | 15284/78577 [00:09<00:37, 1670.69 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  20%|█▉        | 15481/78577 [00:09<00:36, 1711.59 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  20%|█▉        | 15655/78577 [00:09<00:37, 1691.00 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  20%|██        | 15832/78577 [00:09<00:37, 1688.70 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  20%|██        | 16016/78577 [00:09<00:41, 1525.65 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  21%|██        | 16183/78577 [00:09<00:44, 1410.14 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  21%|██        | 16351/78577 [00:09<00:42, 1468.76 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  21%|██        | 16542/78577 [00:09<00:39, 1581.14 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  21%|██▏       | 16708/78577 [00:09<00:38, 1588.93 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  21%|██▏       | 16881/78577 [00:10<00:38, 1605.95 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  22%|██▏       | 17052/78577 [00:10<00:39, 1565.92 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  22%|██▏       | 17226/78577 [00:10<00:38, 1576.81 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  22%|██▏       | 17424/78577 [00:10<00:36, 1656.22 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  22%|██▏       | 17607/78577 [00:10<00:35, 1702.69 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  23%|██▎       | 17783/78577 [00:10<00:35, 1704.29 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  23%|██▎       | 17979/78577 [00:10<00:34, 1749.52 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  23%|██▎       | 18155/78577 [00:10<00:35, 1697.00 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  23%|██▎       | 18342/78577 [00:10<00:35, 1718.39 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  24%|██▎       | 18527/78577 [00:10<00:34, 1752.00 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  24%|██▍       | 18712/78577 [00:11<00:34, 1749.65 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  24%|██▍       | 18895/78577 [00:11<00:34, 1736.22 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  24%|██▍       | 19073/78577 [00:11<00:34, 1746.71 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  25%|██▍       | 19271/78577 [00:11<00:34, 1732.74 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  25%|██▍       | 19457/78577 [00:11<00:34, 1716.65 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  25%|██▌       | 19646/78577 [00:11<00:35, 1658.72 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  25%|██▌       | 19843/78577 [00:11<00:33, 1737.89 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  25%|██▌       | 20034/78577 [00:11<00:34, 1713.58 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  26%|██▌       | 20212/78577 [00:11<00:33, 1725.10 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  26%|██▌       | 20411/78577 [00:12<00:33, 1760.90 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  26%|██▌       | 20607/78577 [00:12<00:33, 1752.92 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  26%|██▋       | 20794/78577 [00:12<00:32, 1767.12 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  27%|██▋       | 20993/78577 [00:12<00:32, 1769.21 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  27%|██▋       | 21174/78577 [00:12<00:32, 1745.54 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  27%|██▋       | 21362/78577 [00:12<00:32, 1744.72 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  27%|██▋       | 21574/78577 [00:12<00:32, 1757.83 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  28%|██▊       | 21791/78577 [00:12<00:31, 1784.54 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  28%|██▊       | 21973/78577 [00:12<00:32, 1738.35 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  28%|██▊       | 22180/78577 [00:13<00:31, 1769.76 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  28%|██▊       | 22368/78577 [00:13<00:31, 1780.29 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  29%|██▊       | 22554/78577 [00:13<00:31, 1786.60 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  29%|██▉       | 22744/78577 [00:13<00:32, 1718.61 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  29%|██▉       | 22930/78577 [00:13<00:32, 1720.48 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  29%|██▉       | 23114/78577 [00:13<00:32, 1731.94 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  30%|██▉       | 23323/78577 [00:13<00:31, 1777.17 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  30%|██▉       | 23502/78577 [00:13<00:31, 1723.67 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  30%|███       | 23685/78577 [00:13<00:32, 1665.91 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  30%|███       | 23856/78577 [00:14<00:36, 1492.71 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  31%|███       | 24024/78577 [00:14<00:35, 1520.16 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  31%|███       | 24188/78577 [00:14<00:38, 1429.13 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  31%|███       | 24385/78577 [00:14<00:34, 1559.37 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  31%|███▏      | 24567/78577 [00:14<00:33, 1594.68 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  31%|███▏      | 24746/78577 [00:14<00:32, 1631.43 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  32%|███▏      | 24959/78577 [00:14<00:31, 1692.33 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  32%|███▏      | 25166/78577 [00:14<00:31, 1722.53 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  32%|███▏      | 25352/78577 [00:14<00:30, 1738.63 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  33%|███▎      | 25549/78577 [00:15<00:29, 1801.60 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  33%|███▎      | 25739/78577 [00:15<00:28, 1829.01 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  33%|███▎      | 25929/78577 [00:15<00:30, 1733.42 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  33%|███▎      | 26111/78577 [00:15<00:30, 1731.68 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  33%|███▎      | 26300/78577 [00:15<00:31, 1684.98 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  34%|███▎      | 26492/78577 [00:15<00:30, 1711.67 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  34%|███▍      | 26676/78577 [00:15<00:31, 1672.36 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  34%|███▍      | 26877/78577 [00:15<00:29, 1763.12 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  34%|███▍      | 27055/78577 [00:15<00:29, 1730.42 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  35%|███▍      | 27239/78577 [00:16<00:29, 1727.53 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  35%|███▍      | 27438/78577 [00:16<00:29, 1744.65 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  35%|███▌      | 27625/78577 [00:16<00:28, 1769.17 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  35%|███▌      | 27811/78577 [00:16<00:29, 1742.54 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  36%|███▌      | 27989/78577 [00:16<00:28, 1749.68 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  36%|███▌      | 28179/78577 [00:16<00:28, 1784.06 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  36%|███▌      | 28358/78577 [00:16<00:30, 1661.65 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  36%|███▋      | 28583/78577 [00:16<00:27, 1824.88 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  37%|███▋      | 28771/78577 [00:16<00:28, 1778.79 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  37%|███▋      | 28958/78577 [00:17<00:28, 1767.39 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  37%|███▋      | 29148/78577 [00:17<00:28, 1729.60 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  37%|███▋      | 29323/78577 [00:17<00:28, 1734.92 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  38%|███▊      | 29550/78577 [00:17<00:27, 1785.78 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  38%|███▊      | 29737/78577 [00:17<00:27, 1788.29 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  38%|███▊      | 29925/78577 [00:17<00:27, 1754.69 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  38%|███▊      | 30143/78577 [00:17<00:27, 1752.70 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  39%|███▊      | 30363/78577 [00:17<00:25, 1870.56 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  39%|███▉      | 30565/78577 [00:17<00:28, 1684.66 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  39%|███▉      | 30751/78577 [00:18<00:27, 1720.69 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  39%|███▉      | 30940/78577 [00:18<00:27, 1761.61 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  40%|███▉      | 31123/78577 [00:18<00:27, 1723.92 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  40%|███▉      | 31306/78577 [00:18<00:27, 1745.06 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  40%|████      | 31493/78577 [00:18<00:29, 1588.88 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  40%|████      | 31658/78577 [00:18<00:29, 1597.15 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  41%|████      | 31824/78577 [00:18<00:29, 1604.81 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  41%|████      | 32020/78577 [00:18<00:27, 1686.12 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  41%|████      | 32193/78577 [00:18<00:29, 1589.14 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  41%|████      | 32361/78577 [00:19<00:30, 1539.06 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  41%|████▏     | 32551/78577 [00:19<00:29, 1583.17 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  42%|████▏     | 32754/78577 [00:19<00:26, 1702.26 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  42%|████▏     | 32929/78577 [00:19<00:26, 1703.50 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  42%|████▏     | 33103/78577 [00:19<00:27, 1651.28 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  42%|████▏     | 33316/78577 [00:19<00:25, 1761.48 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  43%|████▎     | 33505/78577 [00:19<00:25, 1760.06 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  43%|████▎     | 33686/78577 [00:19<00:25, 1753.25 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  43%|████▎     | 33870/78577 [00:19<00:25, 1741.39 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  43%|████▎     | 34045/78577 [00:20<00:25, 1723.22 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  44%|████▎     | 34242/78577 [00:20<00:25, 1714.05 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  44%|████▍     | 34448/78577 [00:20<00:24, 1792.80 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  44%|████▍     | 34645/78577 [00:20<00:24, 1823.46 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  44%|████▍     | 34836/78577 [00:20<00:24, 1817.37 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  45%|████▍     | 35024/78577 [00:20<00:24, 1742.46 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  45%|████▍     | 35214/78577 [00:20<00:24, 1764.56 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  45%|████▌     | 35397/78577 [00:20<00:24, 1728.19 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  45%|████▌     | 35588/78577 [00:20<00:24, 1757.99 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  46%|████▌     | 35771/78577 [00:21<00:24, 1719.42 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  46%|████▌     | 35965/78577 [00:21<00:24, 1741.61 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  46%|████▌     | 36151/78577 [00:21<00:24, 1740.32 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  46%|████▌     | 36334/78577 [00:21<00:23, 1760.33 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  46%|████▋     | 36519/78577 [00:21<00:24, 1747.10 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  47%|████▋     | 36725/78577 [00:21<00:23, 1759.07 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  47%|████▋     | 36928/78577 [00:21<00:23, 1810.58 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  47%|████▋     | 37119/78577 [00:21<00:23, 1783.00 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  47%|████▋     | 37311/78577 [00:21<00:22, 1796.72 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  48%|████▊     | 37517/78577 [00:21<00:22, 1832.53 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  48%|████▊     | 37701/78577 [00:22<00:22, 1789.08 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  48%|████▊     | 37905/78577 [00:22<00:23, 1747.36 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  48%|████▊     | 38109/78577 [00:22<00:22, 1786.38 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  49%|████▊     | 38301/78577 [00:22<00:23, 1747.34 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  49%|████▉     | 38484/78577 [00:22<00:23, 1739.53 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  49%|████▉     | 38684/78577 [00:22<00:23, 1707.59 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  49%|████▉     | 38888/78577 [00:22<00:22, 1760.09 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  50%|████▉     | 39066/78577 [00:22<00:23, 1707.74 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  50%|████▉     | 39250/78577 [00:22<00:22, 1738.65 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  50%|█████     | 39429/78577 [00:23<00:23, 1666.52 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  50%|█████     | 39607/78577 [00:23<00:23, 1673.54 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  51%|█████     | 39775/78577 [00:23<00:24, 1609.73 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  51%|█████     | 39938/78577 [00:23<00:23, 1615.17 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  51%|█████     | 40109/78577 [00:23<00:24, 1585.13 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  51%|█████▏    | 40273/78577 [00:23<00:24, 1549.24 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  52%|█████▏    | 40474/78577 [00:23<00:22, 1660.51 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  52%|█████▏    | 40649/78577 [00:23<00:23, 1630.24 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  52%|█████▏    | 40853/78577 [00:23<00:22, 1696.20 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  52%|█████▏    | 41066/78577 [00:24<00:20, 1786.60 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  52%|█████▏    | 41249/78577 [00:24<00:20, 1783.15 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  53%|█████▎    | 41433/78577 [00:24<00:20, 1772.51 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  53%|█████▎    | 41622/78577 [00:24<00:20, 1791.87 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  53%|█████▎    | 41809/78577 [00:24<00:20, 1802.51 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  53%|█████▎    | 42002/78577 [00:24<00:20, 1796.00 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  54%|█████▎    | 42195/78577 [00:24<00:20, 1805.75 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  54%|█████▍    | 42386/78577 [00:24<00:19, 1812.79 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  54%|█████▍    | 42582/78577 [00:24<00:20, 1794.35 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  54%|█████▍    | 42774/78577 [00:25<00:20, 1768.72 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  55%|█████▍    | 42985/78577 [00:25<00:19, 1792.77 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  55%|█████▍    | 43175/78577 [00:25<00:19, 1816.96 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  55%|█████▌    | 43368/78577 [00:25<00:19, 1767.38 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  55%|█████▌    | 43555/78577 [00:25<00:19, 1753.96 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  56%|█████▌    | 43744/78577 [00:25<00:19, 1790.45 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  56%|█████▌    | 43937/78577 [00:25<00:20, 1699.93 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  56%|█████▌    | 44143/78577 [00:25<00:19, 1777.07 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  56%|█████▋    | 44356/78577 [00:25<00:18, 1834.73 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  57%|█████▋    | 44550/78577 [00:26<00:18, 1804.25 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  57%|█████▋    | 44744/78577 [00:26<00:18, 1800.16 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  57%|█████▋    | 44937/78577 [00:26<00:18, 1822.55 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  57%|█████▋    | 45124/78577 [00:26<00:18, 1809.08 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  58%|█████▊    | 45319/78577 [00:26<00:17, 1847.75 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  58%|█████▊    | 45517/78577 [00:26<00:18, 1817.15 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  58%|█████▊    | 45702/78577 [00:26<00:18, 1813.68 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  58%|█████▊    | 45885/78577 [00:26<00:18, 1759.24 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  59%|█████▊    | 46075/78577 [00:26<00:18, 1738.30 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  59%|█████▉    | 46260/78577 [00:27<00:18, 1755.34 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  59%|█████▉    | 46448/78577 [00:27<00:18, 1747.74 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  59%|█████▉    | 46638/78577 [00:27<00:18, 1772.12 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  60%|█████▉    | 46829/78577 [00:27<00:18, 1703.35 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  60%|█████▉    | 47011/78577 [00:27<00:18, 1728.11 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  60%|██████    | 47186/78577 [00:27<00:18, 1677.74 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  60%|██████    | 47409/78577 [00:27<00:17, 1753.99 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  61%|██████    | 47594/78577 [00:27<00:18, 1644.20 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  61%|██████    | 47776/78577 [00:27<00:18, 1625.66 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  61%|██████    | 47955/78577 [00:28<00:18, 1639.96 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  61%|██████▏   | 48165/78577 [00:28<00:18, 1688.99 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  62%|██████▏   | 48343/78577 [00:28<00:18, 1624.95 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  62%|██████▏   | 48519/78577 [00:28<00:18, 1627.32 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  62%|██████▏   | 48698/78577 [00:28<00:18, 1617.42 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  62%|██████▏   | 48914/78577 [00:28<00:17, 1676.54 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  63%|██████▎   | 49122/78577 [00:28<00:17, 1716.06 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  63%|██████▎   | 49304/78577 [00:28<00:16, 1740.07 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  63%|██████▎   | 49519/78577 [00:28<00:16, 1769.48 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  63%|██████▎   | 49733/78577 [00:29<00:15, 1835.76 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  64%|██████▎   | 49923/78577 [00:29<00:15, 1814.77 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  64%|██████▍   | 50116/78577 [00:29<00:15, 1823.28 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  64%|██████▍   | 50305/78577 [00:29<00:16, 1765.23 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  64%|██████▍   | 50523/78577 [00:29<00:15, 1860.77 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  65%|██████▍   | 50714/78577 [00:29<00:15, 1831.24 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  65%|██████▍   | 50908/78577 [00:29<00:14, 1858.82 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  65%|██████▌   | 51100/78577 [00:29<00:15, 1793.87 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  65%|██████▌   | 51284/78577 [00:29<00:15, 1791.24 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  66%|██████▌   | 51475/78577 [00:30<00:15, 1764.39 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  66%|██████▌   | 51657/78577 [00:30<00:15, 1756.77 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  66%|██████▌   | 51837/78577 [00:30<00:15, 1747.17 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  66%|██████▌   | 52027/78577 [00:30<00:14, 1787.91 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  66%|██████▋   | 52224/78577 [00:30<00:15, 1754.03 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  67%|██████▋   | 52423/78577 [00:30<00:14, 1811.56 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  67%|██████▋   | 52609/78577 [00:30<00:14, 1763.19 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  67%|██████▋   | 52829/78577 [00:30<00:14, 1828.61 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  67%|██████▋   | 53022/78577 [00:30<00:14, 1793.51 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  68%|██████▊   | 53212/78577 [00:30<00:14, 1791.31 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  68%|██████▊   | 53407/78577 [00:31<00:13, 1832.93 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  68%|██████▊   | 53592/78577 [00:31<00:13, 1811.66 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  68%|██████▊   | 53783/78577 [00:31<00:13, 1779.21 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  69%|██████▊   | 53971/78577 [00:31<00:14, 1734.76 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  69%|██████▉   | 54161/78577 [00:31<00:13, 1775.30 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  69%|██████▉   | 54346/78577 [00:31<00:13, 1759.82 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  69%|██████▉   | 54540/78577 [00:31<00:13, 1768.91 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  70%|██████▉   | 54723/78577 [00:31<00:13, 1745.57 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  70%|██████▉   | 54913/78577 [00:31<00:13, 1767.49 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  70%|███████   | 55096/78577 [00:32<00:13, 1768.80 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  70%|███████   | 55286/78577 [00:32<00:13, 1745.20 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  71%|███████   | 55464/78577 [00:32<00:13, 1697.12 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  71%|███████   | 55635/78577 [00:32<00:13, 1644.57 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  71%|███████   | 55812/78577 [00:32<00:13, 1644.18 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  71%|███████   | 55984/78577 [00:32<00:13, 1644.51 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  71%|███████▏  | 56182/78577 [00:32<00:13, 1683.17 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  72%|███████▏  | 56359/78577 [00:32<00:13, 1604.78 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  72%|███████▏  | 56523/78577 [00:32<00:13, 1576.05 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  72%|███████▏  | 56714/78577 [00:33<00:13, 1649.04 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  72%|███████▏  | 56890/78577 [00:33<00:13, 1664.32 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  73%|███████▎  | 57096/78577 [00:33<00:12, 1755.72 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  73%|███████▎  | 57281/78577 [00:33<00:12, 1760.91 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  73%|███████▎  | 57478/78577 [00:33<00:11, 1781.29 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  73%|███████▎  | 57676/78577 [00:33<00:11, 1816.14 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  74%|███████▎  | 57859/78577 [00:33<00:11, 1817.56 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  74%|███████▍  | 58054/78577 [00:33<00:11, 1790.49 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  74%|███████▍  | 58241/78577 [00:33<00:11, 1739.30 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  74%|███████▍  | 58449/78577 [00:33<00:11, 1817.50 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  75%|███████▍  | 58639/78577 [00:34<00:11, 1811.97 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  75%|███████▍  | 58835/78577 [00:34<00:10, 1848.66 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  75%|███████▌  | 59033/78577 [00:34<00:11, 1754.10 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  75%|███████▌  | 59237/78577 [00:34<00:10, 1816.04 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  76%|███████▌  | 59425/78577 [00:34<00:10, 1750.78 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  76%|███████▌  | 59614/78577 [00:34<00:10, 1736.05 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  76%|███████▌  | 59794/78577 [00:34<00:11, 1678.88 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  76%|███████▋  | 59991/78577 [00:34<00:10, 1737.33 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  77%|███████▋  | 60184/78577 [00:34<00:10, 1714.86 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  77%|███████▋  | 60376/78577 [00:35<00:10, 1753.17 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  77%|███████▋  | 60573/78577 [00:35<00:10, 1792.03 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  77%|███████▋  | 60763/78577 [00:35<00:09, 1807.06 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  78%|███████▊  | 60950/78577 [00:35<00:10, 1754.07 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  78%|███████▊  | 61160/78577 [00:35<00:09, 1760.69 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  78%|███████▊  | 61387/78577 [00:35<00:09, 1821.80 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  78%|███████▊  | 61570/78577 [00:35<00:09, 1821.76 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  79%|███████▊  | 61761/78577 [00:35<00:09, 1751.04 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  79%|███████▉  | 61952/78577 [00:35<00:09, 1772.45 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  79%|███████▉  | 62134/78577 [00:36<00:09, 1748.46 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  79%|███████▉  | 62349/78577 [00:36<00:09, 1779.93 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  80%|███████▉  | 62538/78577 [00:36<00:08, 1793.09 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  80%|███████▉  | 62725/78577 [00:36<00:09, 1714.41 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  80%|████████  | 62918/78577 [00:36<00:09, 1701.39 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  80%|████████  | 63123/78577 [00:36<00:08, 1793.79 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  81%|████████  | 63317/78577 [00:36<00:08, 1777.07 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  81%|████████  | 63502/78577 [00:36<00:09, 1665.84 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  81%|████████  | 63685/78577 [00:36<00:08, 1657.31 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  81%|████████▏ | 63885/78577 [00:37<00:08, 1644.66 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  82%|████████▏ | 64081/78577 [00:37<00:08, 1690.97 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  82%|████████▏ | 64251/78577 [00:37<00:08, 1657.46 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  82%|████████▏ | 64420/78577 [00:37<00:08, 1608.42 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  82%|████████▏ | 64591/78577 [00:37<00:08, 1622.60 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  82%|████████▏ | 64793/78577 [00:37<00:08, 1680.05 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  83%|████████▎ | 64994/78577 [00:37<00:07, 1772.17 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  83%|████████▎ | 65177/78577 [00:37<00:07, 1786.91 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  83%|████████▎ | 65372/78577 [00:37<00:07, 1770.90 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  83%|████████▎ | 65561/78577 [00:38<00:07, 1746.86 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  84%|████████▎ | 65759/78577 [00:38<00:07, 1790.63 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  84%|████████▍ | 65941/78577 [00:38<00:07, 1780.44 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  84%|████████▍ | 66138/78577 [00:38<00:06, 1815.03 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  84%|████████▍ | 66328/78577 [00:38<00:06, 1793.46 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  85%|████████▍ | 66532/78577 [00:38<00:06, 1855.48 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  85%|████████▍ | 66723/78577 [00:38<00:06, 1822.57 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  85%|████████▌ | 66917/78577 [00:38<00:06, 1792.42 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  85%|████████▌ | 67104/78577 [00:38<00:06, 1717.58 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  86%|████████▌ | 67325/78577 [00:39<00:06, 1831.90 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  86%|████████▌ | 67516/78577 [00:39<00:06, 1811.46 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  86%|████████▌ | 67703/78577 [00:39<00:06, 1717.62 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  86%|████████▋ | 67888/78577 [00:39<00:06, 1709.85 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  87%|████████▋ | 68072/78577 [00:39<00:06, 1729.25 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  87%|████████▋ | 68256/78577 [00:39<00:05, 1739.85 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  87%|████████▋ | 68460/78577 [00:39<00:05, 1727.07 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  87%|████████▋ | 68675/78577 [00:39<00:05, 1810.83 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  88%|████████▊ | 68865/78577 [00:39<00:05, 1785.46 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  88%|████████▊ | 69047/78577 [00:40<00:05, 1787.94 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  88%|████████▊ | 69236/78577 [00:40<00:05, 1792.75 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  88%|████████▊ | 69427/78577 [00:40<00:05, 1723.73 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  89%|████████▊ | 69615/78577 [00:40<00:05, 1730.27 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  89%|████████▉ | 69810/78577 [00:40<00:05, 1717.44 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  89%|████████▉ | 70022/78577 [00:40<00:04, 1768.12 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  89%|████████▉ | 70202/78577 [00:40<00:04, 1776.42 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  90%|████████▉ | 70397/78577 [00:40<00:04, 1783.16 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  90%|████████▉ | 70584/78577 [00:40<00:04, 1730.75 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  90%|█████████ | 70764/78577 [00:41<00:04, 1675.83 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  90%|█████████ | 70983/78577 [00:41<00:04, 1781.28 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  91%|█████████ | 71176/78577 [00:41<00:04, 1794.33 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  91%|█████████ | 71369/78577 [00:41<00:04, 1740.84 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  91%|█████████ | 71546/78577 [00:41<00:04, 1741.91 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  91%|█████████▏| 71730/78577 [00:41<00:03, 1716.30 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  92%|█████████▏| 71912/78577 [00:41<00:03, 1689.48 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  92%|█████████▏| 72093/78577 [00:41<00:03, 1631.14 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  92%|█████████▏| 72296/78577 [00:41<00:03, 1703.32 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  92%|█████████▏| 72472/78577 [00:42<00:03, 1596.23 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  92%|█████████▏| 72637/78577 [00:42<00:03, 1556.67 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  93%|█████████▎| 72844/78577 [00:42<00:03, 1650.55 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  93%|█████████▎| 73024/78577 [00:42<00:03, 1666.41 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  93%|█████████▎| 73229/78577 [00:42<00:03, 1767.36 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  93%|█████████▎| 73408/78577 [00:42<00:02, 1724.09 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  94%|█████████▎| 73609/78577 [00:42<00:02, 1763.19 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  94%|█████████▍| 73790/78577 [00:42<00:02, 1733.62 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  94%|█████████▍| 73970/78577 [00:42<00:02, 1717.69 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  94%|█████████▍| 74175/78577 [00:43<00:02, 1772.11 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  95%|█████████▍| 74380/78577 [00:43<00:02, 1810.78 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  95%|█████████▍| 74566/78577 [00:43<00:02, 1795.48 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  95%|█████████▌| 74754/78577 [00:43<00:02, 1785.11 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  95%|█████████▌| 74953/78577 [00:43<00:02, 1782.24 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  96%|█████████▌| 75139/78577 [00:43<00:01, 1781.67 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  96%|█████████▌| 75330/78577 [00:43<00:01, 1760.84 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  96%|█████████▌| 75533/78577 [00:43<00:01, 1816.96 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  96%|█████████▋| 75726/78577 [00:43<00:01, 1771.06 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  97%|█████████▋| 75910/78577 [00:44<00:01, 1705.34 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  97%|█████████▋| 76094/78577 [00:44<00:01, 1713.61 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  97%|█████████▋| 76270/78577 [00:44<00:01, 1631.23 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  97%|█████████▋| 76468/78577 [00:44<00:01, 1688.94 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  98%|█████████▊| 76685/78577 [00:44<00:01, 1793.22 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  98%|█████████▊| 76874/78577 [00:44<00:00, 1716.98 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  98%|█████████▊| 77047/78577 [00:44<00:00, 1699.82 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  98%|█████████▊| 77247/78577 [00:44<00:00, 1748.13 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  99%|█████████▊| 77463/78577 [00:44<00:00, 1733.82 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  99%|█████████▉| 77686/78577 [00:45<00:00, 1780.63 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  99%|█████████▉| 77871/78577 [00:45<00:00, 1778.51 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  99%|█████████▉| 78051/78577 [00:45<00:00, 1728.76 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8): 100%|█████████▉| 78263/78577 [00:45<00:00, 1614.93 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8): 100%|█████████▉| 78485/78577 [00:45<00:00, 1568.84 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8): 100%|██████████| 78577/78577 [00:45<00:00, 1719.60 examples/s]\u001b[0m\n",
      "\u001b[34m[2023-10-27 21:37:04,188] [INFO] [axolotl.load_tokenized_prepared_datasets:354] [PID:197] [RANK:0] merging datasets#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 21:37:04,190] [INFO] [axolotl.load_tokenized_prepared_datasets:361] [PID:197] [RANK:0] Saving merged prepared dataset to disk... /tmp/last_run_prepared/602d3a7380f73b0eb426ee7159f348f4#033[39m\u001b[0m\n",
      "\u001b[34mSaving the dataset (0/1 shards):   0%|          | 0/78577 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (1/1 shards): 100%|██████████| 78577/78577 [00:00<00:00, 1032654.53 examples/s]\u001b[0m\n",
      "\u001b[34mSaving the dataset (1/1 shards): 100%|██████████| 78577/78577 [00:00<00:00, 1029724.95 examples/s]\u001b[0m\n",
      "\u001b[34mNCCL version 2.17.1+cuda11.8\u001b[0m\n",
      "\u001b[34malgo-1:197:277 [0] nccl_net_ofi_init:1472 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[35malgo-2:196:219 [0] nccl_net_ofi_init:1472 NCCL WARN NET/OFI aws-ofi-nccl initialization failed\u001b[0m\n",
      "\u001b[35m[2023-10-27 21:37:05,208] [INFO] [axolotl.load_tokenized_prepared_datasets:130] [PID:196] [RANK:0] Unable to find prepared dataset in /tmp/last_run_prepared/602d3a7380f73b0eb426ee7159f348f4#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 21:37:05,209] [INFO] [axolotl.load_tokenized_prepared_datasets:131] [PID:196] [RANK:0] Loading raw datasets...#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 21:37:05,209] [INFO] [axolotl.load_tokenized_prepared_datasets:136] [PID:196] [RANK:0] No seed provided, using default seed of 42#033[39m\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.10/site-packages/datasets/load.py:2089: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\u001b[0m\n",
      "\u001b[35mYou can remove this warning by passing 'token=None' instead.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[35mDownloading data files:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mDownloading data files: 100%|██████████| 1/1 [00:00<00:00, 14563.56it/s]\u001b[0m\n",
      "\u001b[35mExtracting data files:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mExtracting data files: 100%|██████████| 1/1 [00:00<00:00, 1021.75it/s]\u001b[0m\n",
      "\u001b[35mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[35mGenerating train split: 78577 examples [00:00, 1305276.64 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):   0%|          | 0/78577 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):   0%|          | 33/78577 [00:00<08:26, 155.05 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):   0%|          | 205/78577 [00:00<01:43, 756.08 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):   1%|          | 445/78577 [00:00<00:59, 1306.23 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):   1%|          | 617/78577 [00:00<00:54, 1425.42 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):   1%|          | 809/78577 [00:00<00:49, 1578.96 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):   1%|▏         | 1012/78577 [00:00<00:46, 1678.51 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):   2%|▏         | 1192/78577 [00:00<00:45, 1704.04 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):   2%|▏         | 1384/78577 [00:00<00:45, 1711.78 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):   2%|▏         | 1572/78577 [00:01<00:44, 1744.24 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):   2%|▏         | 1753/78577 [00:01<00:43, 1750.80 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):   2%|▏         | 1938/78577 [00:01<00:44, 1737.83 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):   3%|▎         | 2153/78577 [00:01<00:43, 1737.51 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):   3%|▎         | 2366/78577 [00:01<00:42, 1814.53 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):   3%|▎         | 2560/78577 [00:01<00:41, 1818.76 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):   3%|▎         | 2747/78577 [00:01<00:42, 1774.48 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):   4%|▎         | 2931/78577 [00:01<00:43, 1733.37 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):   4%|▍         | 3142/78577 [00:01<00:42, 1780.27 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):   4%|▍         | 3339/78577 [00:02<00:42, 1750.99 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):   5%|▍         | 3547/78577 [00:02<00:41, 1802.13 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):   5%|▍         | 3733/78577 [00:02<00:41, 1796.31 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):   5%|▍         | 3915/78577 [00:02<00:41, 1786.30 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):   5%|▌         | 4108/78577 [00:02<00:41, 1791.06 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):   5%|▌         | 4294/78577 [00:02<00:42, 1749.94 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):   6%|▌         | 4485/78577 [00:02<00:41, 1793.20 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):   6%|▌         | 4675/78577 [00:02<00:41, 1763.28 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):   6%|▌         | 4856/78577 [00:02<00:41, 1758.92 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):   6%|▋         | 5055/78577 [00:03<00:40, 1821.10 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):   7%|▋         | 5249/78577 [00:03<00:41, 1755.14 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):   7%|▋         | 5452/78577 [00:03<00:40, 1811.76 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):   7%|▋         | 5637/78577 [00:03<00:40, 1800.28 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):   7%|▋         | 5831/78577 [00:03<00:42, 1708.22 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):   8%|▊         | 6044/78577 [00:03<00:41, 1762.20 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):   8%|▊         | 6249/78577 [00:03<00:40, 1804.81 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):   8%|▊         | 6437/78577 [00:03<00:39, 1804.42 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):   8%|▊         | 6632/78577 [00:03<00:39, 1803.68 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):   9%|▊         | 6824/78577 [00:04<00:39, 1814.36 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):   9%|▉         | 7014/78577 [00:04<00:41, 1742.14 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):   9%|▉         | 7200/78577 [00:04<00:40, 1771.52 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):   9%|▉         | 7395/78577 [00:04<00:40, 1766.84 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  10%|▉         | 7590/78577 [00:04<00:39, 1779.31 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  10%|▉         | 7769/78577 [00:04<00:43, 1631.94 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  10%|█         | 7935/78577 [00:04<00:48, 1452.44 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  10%|█         | 8086/78577 [00:04<00:52, 1334.79 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  10%|█         | 8239/78577 [00:04<00:52, 1340.11 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  11%|█         | 8402/78577 [00:05<00:49, 1411.05 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  11%|█         | 8592/78577 [00:05<00:45, 1525.64 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  11%|█         | 8769/78577 [00:05<00:45, 1543.85 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  11%|█▏        | 8968/78577 [00:05<00:42, 1648.32 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  12%|█▏        | 9143/78577 [00:05<00:41, 1660.80 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  12%|█▏        | 9339/78577 [00:05<00:39, 1743.91 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  12%|█▏        | 9515/78577 [00:05<00:40, 1705.99 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  12%|█▏        | 9699/78577 [00:05<00:39, 1727.18 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  13%|█▎        | 9891/78577 [00:05<00:38, 1765.69 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  13%|█▎        | 10075/78577 [00:06<00:38, 1758.43 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  13%|█▎        | 10258/78577 [00:06<00:39, 1727.40 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  13%|█▎        | 10473/78577 [00:06<00:38, 1765.34 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  14%|█▎        | 10671/78577 [00:06<00:37, 1807.41 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  14%|█▍        | 10861/78577 [00:06<00:37, 1789.39 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  14%|█▍        | 11050/78577 [00:06<00:39, 1718.87 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  14%|█▍        | 11244/78577 [00:06<00:38, 1760.33 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  15%|█▍        | 11432/78577 [00:06<00:37, 1770.77 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  15%|█▍        | 11613/78577 [00:06<00:38, 1746.45 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  15%|█▌        | 11800/78577 [00:06<00:37, 1778.72 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  15%|█▌        | 11983/78577 [00:07<00:37, 1772.48 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  15%|█▌        | 12172/78577 [00:07<00:37, 1783.48 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  16%|█▌        | 12365/78577 [00:07<00:36, 1793.30 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  16%|█▌        | 12559/78577 [00:07<00:37, 1770.10 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  16%|█▌        | 12741/78577 [00:07<00:38, 1728.12 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  16%|█▋        | 12956/78577 [00:07<00:36, 1807.69 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  17%|█▋        | 13138/78577 [00:07<00:36, 1786.17 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  17%|█▋        | 13326/78577 [00:07<00:37, 1738.37 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  17%|█▋        | 13524/78577 [00:07<00:36, 1788.33 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  17%|█▋        | 13706/78577 [00:08<00:36, 1756.48 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  18%|█▊        | 13887/78577 [00:08<00:37, 1709.09 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  18%|█▊        | 14088/78577 [00:08<00:35, 1792.99 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  18%|█▊        | 14270/78577 [00:08<00:36, 1741.47 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  18%|█▊        | 14476/78577 [00:08<00:35, 1820.64 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  19%|█▊        | 14669/78577 [00:08<00:34, 1830.49 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  19%|█▉        | 14854/78577 [00:08<00:37, 1692.83 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  19%|█▉        | 15047/78577 [00:08<00:36, 1737.05 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  19%|█▉        | 15243/78577 [00:08<00:35, 1764.75 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  20%|█▉        | 15443/78577 [00:09<00:35, 1766.53 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  20%|█▉        | 15627/78577 [00:09<00:36, 1721.15 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  20%|██        | 15801/78577 [00:09<00:42, 1473.38 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  20%|██        | 15964/78577 [00:09<00:43, 1451.99 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  21%|██        | 16124/78577 [00:09<00:42, 1476.50 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  21%|██        | 16283/78577 [00:09<00:43, 1443.87 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  21%|██        | 16505/78577 [00:09<00:39, 1576.17 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  21%|██▏       | 16699/78577 [00:09<00:37, 1655.76 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  21%|██▏       | 16881/78577 [00:09<00:36, 1676.43 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  22%|██▏       | 17062/78577 [00:10<00:36, 1703.81 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  22%|██▏       | 17236/78577 [00:10<00:37, 1647.90 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  22%|██▏       | 17412/78577 [00:10<00:36, 1678.77 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  22%|██▏       | 17591/78577 [00:10<00:36, 1693.91 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  23%|██▎       | 17792/78577 [00:10<00:36, 1668.16 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  23%|██▎       | 18019/78577 [00:10<00:35, 1721.63 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  23%|██▎       | 18238/78577 [00:10<00:33, 1808.40 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  23%|██▎       | 18424/78577 [00:10<00:34, 1743.77 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  24%|██▎       | 18643/78577 [00:11<00:33, 1809.06 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  24%|██▍       | 18839/78577 [00:11<00:33, 1809.59 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  24%|██▍       | 19023/78577 [00:11<00:33, 1759.58 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  24%|██▍       | 19210/78577 [00:11<00:34, 1710.57 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  25%|██▍       | 19413/78577 [00:11<00:33, 1778.43 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  25%|██▍       | 19592/78577 [00:11<00:33, 1766.76 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  25%|██▌       | 19775/78577 [00:11<00:34, 1707.35 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  25%|██▌       | 19979/78577 [00:11<00:33, 1762.28 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  26%|██▌       | 20156/78577 [00:11<00:33, 1763.04 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  26%|██▌       | 20353/78577 [00:11<00:32, 1803.71 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  26%|██▌       | 20549/78577 [00:12<00:31, 1814.44 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  26%|██▋       | 20739/78577 [00:12<00:33, 1725.94 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  27%|██▋       | 20960/78577 [00:12<00:31, 1824.19 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  27%|██▋       | 21155/78577 [00:12<00:31, 1801.28 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  27%|██▋       | 21344/78577 [00:12<00:31, 1799.77 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  27%|██▋       | 21532/78577 [00:12<00:31, 1802.51 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  28%|██▊       | 21732/78577 [00:12<00:31, 1800.55 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  28%|██▊       | 21917/78577 [00:12<00:32, 1753.60 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  28%|██▊       | 22107/78577 [00:12<00:31, 1773.14 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  28%|██▊       | 22303/78577 [00:13<00:31, 1784.91 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  29%|██▊       | 22493/78577 [00:13<00:31, 1784.13 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  29%|██▉       | 22701/78577 [00:13<00:30, 1824.51 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  29%|██▉       | 22893/78577 [00:13<00:31, 1772.46 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  29%|██▉       | 23071/78577 [00:13<00:32, 1714.21 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  30%|██▉       | 23253/78577 [00:13<00:32, 1693.90 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  30%|██▉       | 23424/78577 [00:13<00:34, 1617.24 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  30%|███       | 23591/78577 [00:13<00:35, 1533.02 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  30%|███       | 23812/78577 [00:13<00:32, 1710.27 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  31%|███       | 23993/78577 [00:14<00:33, 1617.30 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  31%|███       | 24166/78577 [00:14<00:35, 1517.15 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  31%|███       | 24354/78577 [00:14<00:35, 1518.17 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  31%|███       | 24555/78577 [00:14<00:34, 1566.90 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  32%|███▏      | 24758/78577 [00:14<00:32, 1669.04 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  32%|███▏      | 24946/78577 [00:14<00:32, 1651.92 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  32%|███▏      | 25158/78577 [00:14<00:30, 1761.09 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  32%|███▏      | 25344/78577 [00:14<00:30, 1757.75 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  33%|███▎      | 25541/78577 [00:15<00:30, 1736.21 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  33%|███▎      | 25748/78577 [00:15<00:29, 1790.93 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  33%|███▎      | 25940/78577 [00:15<00:29, 1774.74 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  33%|███▎      | 26130/78577 [00:15<00:29, 1792.52 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  33%|███▎      | 26311/78577 [00:15<00:30, 1736.28 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  34%|███▎      | 26493/78577 [00:15<00:30, 1694.92 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  34%|███▍      | 26680/78577 [00:15<00:29, 1732.76 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  34%|███▍      | 26865/78577 [00:15<00:29, 1745.89 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  34%|███▍      | 27047/78577 [00:15<00:29, 1747.13 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  35%|███▍      | 27233/78577 [00:15<00:28, 1773.08 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  35%|███▍      | 27418/78577 [00:16<00:29, 1705.65 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  35%|███▌      | 27610/78577 [00:16<00:29, 1743.97 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  35%|███▌      | 27811/78577 [00:16<00:28, 1770.87 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  36%|███▌      | 27998/78577 [00:16<00:28, 1766.48 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  36%|███▌      | 28177/78577 [00:16<00:28, 1751.11 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  36%|███▌      | 28361/78577 [00:16<00:28, 1773.68 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  36%|███▋      | 28552/78577 [00:16<00:28, 1754.81 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  37%|███▋      | 28729/78577 [00:16<00:28, 1741.81 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  37%|███▋      | 28929/78577 [00:16<00:28, 1755.95 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  37%|███▋      | 29124/78577 [00:17<00:27, 1808.64 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  37%|███▋      | 29319/78577 [00:17<00:27, 1766.47 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  38%|███▊      | 29500/78577 [00:17<00:28, 1744.03 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  38%|███▊      | 29682/78577 [00:17<00:28, 1744.15 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  38%|███▊      | 29869/78577 [00:17<00:27, 1773.68 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  38%|███▊      | 30052/78577 [00:17<00:27, 1777.17 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  38%|███▊      | 30233/78577 [00:17<00:27, 1752.19 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  39%|███▊      | 30420/78577 [00:17<00:27, 1781.06 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  39%|███▉      | 30611/78577 [00:17<00:27, 1745.02 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  39%|███▉      | 30795/78577 [00:18<00:28, 1690.19 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  39%|███▉      | 30967/78577 [00:18<00:28, 1689.56 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  40%|███▉      | 31146/78577 [00:18<00:29, 1596.92 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  40%|███▉      | 31317/78577 [00:18<00:30, 1542.74 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  40%|████      | 31512/78577 [00:18<00:28, 1636.50 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  40%|████      | 31691/78577 [00:18<00:27, 1677.71 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  41%|████      | 31865/78577 [00:18<00:27, 1694.85 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  41%|████      | 32046/78577 [00:18<00:27, 1688.09 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  41%|████      | 32230/78577 [00:18<00:27, 1657.15 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  41%|████      | 32408/78577 [00:19<00:29, 1587.30 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  41%|████▏     | 32599/78577 [00:19<00:27, 1663.57 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  42%|████▏     | 32767/78577 [00:19<00:28, 1626.43 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  42%|████▏     | 32943/78577 [00:19<00:27, 1643.95 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  42%|████▏     | 33155/78577 [00:19<00:25, 1755.86 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  42%|████▏     | 33343/78577 [00:19<00:26, 1734.87 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  43%|████▎     | 33532/78577 [00:19<00:26, 1684.61 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  43%|████▎     | 33737/78577 [00:19<00:26, 1690.65 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  43%|████▎     | 33967/78577 [00:19<00:24, 1834.59 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  43%|████▎     | 34165/78577 [00:20<00:25, 1760.28 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  44%|████▎     | 34359/78577 [00:20<00:25, 1751.11 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  44%|████▍     | 34559/78577 [00:20<00:24, 1800.54 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  44%|████▍     | 34743/78577 [00:20<00:25, 1751.26 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  44%|████▍     | 34942/78577 [00:20<00:24, 1814.69 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  45%|████▍     | 35140/78577 [00:20<00:24, 1784.86 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  45%|████▍     | 35334/78577 [00:20<00:24, 1788.37 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  45%|████▌     | 35533/78577 [00:20<00:23, 1801.68 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  45%|████▌     | 35722/78577 [00:20<00:24, 1733.48 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  46%|████▌     | 35905/78577 [00:21<00:25, 1685.76 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  46%|████▌     | 36107/78577 [00:21<00:24, 1753.79 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  46%|████▌     | 36288/78577 [00:21<00:24, 1732.01 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  46%|████▋     | 36468/78577 [00:21<00:24, 1748.81 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  47%|████▋     | 36652/78577 [00:21<00:24, 1736.58 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  47%|████▋     | 36840/78577 [00:21<00:24, 1718.99 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  47%|████▋     | 37043/78577 [00:21<00:23, 1768.26 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  47%|████▋     | 37222/78577 [00:21<00:23, 1754.58 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  48%|████▊     | 37416/78577 [00:21<00:23, 1786.43 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  48%|████▊     | 37611/78577 [00:21<00:22, 1795.53 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  48%|████▊     | 37809/78577 [00:22<00:22, 1783.94 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  48%|████▊     | 38001/78577 [00:22<00:22, 1805.53 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  49%|████▊     | 38184/78577 [00:22<00:22, 1788.06 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  49%|████▉     | 38373/78577 [00:22<00:22, 1807.76 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  49%|████▉     | 38570/78577 [00:22<00:23, 1671.17 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  49%|████▉     | 38745/78577 [00:22<00:24, 1620.52 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  50%|████▉     | 38915/78577 [00:22<00:24, 1593.10 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  50%|████▉     | 39080/78577 [00:22<00:24, 1605.61 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  50%|████▉     | 39286/78577 [00:22<00:24, 1628.55 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  50%|█████     | 39477/78577 [00:23<00:23, 1666.07 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  50%|█████     | 39672/78577 [00:23<00:22, 1725.68 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  51%|█████     | 39853/78577 [00:23<00:22, 1693.43 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  51%|█████     | 40033/78577 [00:23<00:22, 1703.17 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  51%|█████     | 40213/78577 [00:23<00:22, 1728.46 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  51%|█████▏    | 40402/78577 [00:23<00:24, 1590.55 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  52%|█████▏    | 40573/78577 [00:23<00:23, 1618.48 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  52%|█████▏    | 40783/78577 [00:23<00:22, 1646.96 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  52%|█████▏    | 40997/78577 [00:23<00:21, 1780.50 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  52%|█████▏    | 41185/78577 [00:24<00:21, 1728.56 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  53%|█████▎    | 41360/78577 [00:24<00:21, 1732.34 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  53%|█████▎    | 41564/78577 [00:24<00:21, 1747.59 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  53%|█████▎    | 41752/78577 [00:24<00:20, 1763.77 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  53%|█████▎    | 41936/78577 [00:24<00:20, 1757.64 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  54%|█████▎    | 42119/78577 [00:24<00:20, 1766.51 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  54%|█████▍    | 42324/78577 [00:24<00:20, 1745.21 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  54%|█████▍    | 42539/78577 [00:24<00:19, 1842.69 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  54%|█████▍    | 42732/78577 [00:24<00:19, 1842.39 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  55%|█████▍    | 42934/78577 [00:25<00:19, 1789.45 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  55%|█████▍    | 43155/78577 [00:25<00:18, 1898.94 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  55%|█████▌    | 43352/78577 [00:25<00:19, 1786.15 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  55%|█████▌    | 43544/78577 [00:25<00:20, 1750.38 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  56%|█████▌    | 43750/78577 [00:25<00:19, 1762.88 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  56%|█████▌    | 43949/78577 [00:25<00:19, 1795.84 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  56%|█████▌    | 44149/78577 [00:25<00:18, 1815.36 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  56%|█████▋    | 44341/78577 [00:25<00:19, 1742.09 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  57%|█████▋    | 44553/78577 [00:25<00:19, 1760.11 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  57%|█████▋    | 44766/78577 [00:26<00:19, 1733.44 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  57%|█████▋    | 44970/78577 [00:26<00:18, 1790.00 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  57%|█████▋    | 45156/78577 [00:26<00:18, 1790.02 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  58%|█████▊    | 45356/78577 [00:26<00:18, 1797.23 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  58%|█████▊    | 45557/78577 [00:26<00:18, 1777.76 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  58%|█████▊    | 45777/78577 [00:26<00:18, 1795.39 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  59%|█████▊    | 45984/78577 [00:26<00:17, 1858.56 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  59%|█████▉    | 46172/78577 [00:26<00:18, 1797.83 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  59%|█████▉    | 46359/78577 [00:26<00:18, 1708.09 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  59%|█████▉    | 46534/78577 [00:27<00:19, 1638.03 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  59%|█████▉    | 46702/78577 [00:27<00:19, 1624.56 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  60%|█████▉    | 46879/78577 [00:27<00:19, 1626.48 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  60%|█████▉    | 47053/78577 [00:27<00:19, 1627.74 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  60%|██████    | 47250/78577 [00:27<00:18, 1699.58 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  60%|██████    | 47437/78577 [00:27<00:18, 1728.15 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  61%|██████    | 47625/78577 [00:27<00:17, 1749.01 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  61%|██████    | 47808/78577 [00:27<00:17, 1765.58 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  61%|██████    | 47992/78577 [00:27<00:17, 1716.13 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  61%|██████▏   | 48167/78577 [00:28<00:17, 1703.48 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  62%|██████▏   | 48353/78577 [00:28<00:17, 1709.32 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  62%|██████▏   | 48531/78577 [00:28<00:18, 1641.96 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  62%|██████▏   | 48711/78577 [00:28<00:17, 1681.00 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  62%|██████▏   | 48889/78577 [00:28<00:17, 1691.62 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  62%|██████▏   | 49085/78577 [00:28<00:17, 1711.39 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  63%|██████▎   | 49272/78577 [00:28<00:16, 1732.09 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  63%|██████▎   | 49447/78577 [00:28<00:16, 1726.75 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  63%|██████▎   | 49661/78577 [00:28<00:16, 1800.73 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  63%|██████▎   | 49846/78577 [00:29<00:16, 1774.43 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  64%|██████▎   | 50034/78577 [00:29<00:16, 1779.14 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  64%|██████▍   | 50228/78577 [00:29<00:15, 1802.28 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  64%|██████▍   | 50423/78577 [00:29<00:15, 1810.11 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  64%|██████▍   | 50614/78577 [00:29<00:15, 1814.97 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  65%|██████▍   | 50796/78577 [00:29<00:15, 1798.86 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  65%|██████▍   | 50995/78577 [00:29<00:15, 1789.91 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  65%|██████▌   | 51201/78577 [00:29<00:14, 1825.95 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  65%|██████▌   | 51392/78577 [00:29<00:15, 1784.11 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  66%|██████▌   | 51586/78577 [00:29<00:14, 1811.66 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  66%|██████▌   | 51788/78577 [00:30<00:14, 1827.96 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  66%|██████▌   | 52001/78577 [00:30<00:14, 1849.35 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  66%|██████▋   | 52189/78577 [00:30<00:14, 1836.74 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  67%|██████▋   | 52373/78577 [00:30<00:14, 1802.80 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  67%|██████▋   | 52558/78577 [00:30<00:15, 1731.76 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  67%|██████▋   | 52764/78577 [00:30<00:14, 1802.87 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  67%|██████▋   | 52949/78577 [00:30<00:14, 1790.11 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  68%|██████▊   | 53140/78577 [00:30<00:14, 1792.58 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  68%|██████▊   | 53323/78577 [00:30<00:14, 1784.99 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  68%|██████▊   | 53517/78577 [00:31<00:14, 1773.74 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  68%|██████▊   | 53728/78577 [00:31<00:13, 1797.01 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  69%|██████▊   | 53920/78577 [00:31<00:13, 1796.22 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  69%|██████▉   | 54130/78577 [00:31<00:13, 1838.48 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  69%|██████▉   | 54317/78577 [00:31<00:14, 1721.90 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  69%|██████▉   | 54497/78577 [00:31<00:14, 1634.69 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  70%|██████▉   | 54670/78577 [00:31<00:14, 1626.04 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  70%|██████▉   | 54848/78577 [00:31<00:14, 1588.96 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  70%|███████   | 55046/78577 [00:31<00:13, 1685.83 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  70%|███████   | 55226/78577 [00:32<00:13, 1671.84 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  71%|███████   | 55424/78577 [00:32<00:13, 1734.97 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  71%|███████   | 55609/78577 [00:32<00:13, 1752.83 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  71%|███████   | 55791/78577 [00:32<00:13, 1709.52 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  71%|███████▏  | 56001/78577 [00:32<00:12, 1799.39 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  72%|███████▏  | 56197/78577 [00:32<00:13, 1707.65 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  72%|███████▏  | 56374/78577 [00:32<00:13, 1671.71 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  72%|███████▏  | 56553/78577 [00:32<00:13, 1672.24 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  72%|███████▏  | 56734/78577 [00:32<00:12, 1696.96 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  72%|███████▏  | 56905/78577 [00:33<00:13, 1652.52 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  73%|███████▎  | 57119/78577 [00:33<00:12, 1696.79 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  73%|███████▎  | 57306/78577 [00:33<00:12, 1724.83 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  73%|███████▎  | 57485/78577 [00:33<00:12, 1685.24 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  73%|███████▎  | 57696/78577 [00:33<00:11, 1782.99 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  74%|███████▎  | 57904/78577 [00:33<00:11, 1808.39 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  74%|███████▍  | 58098/78577 [00:33<00:11, 1805.98 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  74%|███████▍  | 58283/78577 [00:33<00:11, 1718.08 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  74%|███████▍  | 58493/78577 [00:33<00:11, 1806.16 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  75%|███████▍  | 58680/78577 [00:34<00:10, 1815.89 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  75%|███████▍  | 58866/78577 [00:34<00:10, 1793.98 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  75%|███████▌  | 59055/78577 [00:34<00:10, 1813.72 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  75%|███████▌  | 59245/78577 [00:34<00:10, 1836.13 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  76%|███████▌  | 59436/78577 [00:34<00:10, 1763.74 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  76%|███████▌  | 59621/78577 [00:34<00:10, 1752.51 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  76%|███████▌  | 59810/78577 [00:34<00:10, 1783.62 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  76%|███████▋  | 60002/78577 [00:34<00:10, 1766.42 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  77%|███████▋  | 60211/78577 [00:34<00:10, 1796.53 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  77%|███████▋  | 60397/78577 [00:35<00:10, 1809.95 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  77%|███████▋  | 60580/78577 [00:35<00:10, 1741.53 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  77%|███████▋  | 60757/78577 [00:35<00:10, 1706.15 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  78%|███████▊  | 60951/78577 [00:35<00:10, 1738.06 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  78%|███████▊  | 61152/78577 [00:35<00:09, 1801.35 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  78%|███████▊  | 61341/78577 [00:35<00:09, 1758.53 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  78%|███████▊  | 61527/78577 [00:35<00:09, 1751.77 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  79%|███████▊  | 61729/78577 [00:35<00:09, 1825.61 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  79%|███████▉  | 61925/78577 [00:35<00:09, 1783.02 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  79%|███████▉  | 62113/78577 [00:36<00:09, 1715.33 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  79%|███████▉  | 62299/78577 [00:36<00:09, 1662.98 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  80%|███████▉  | 62484/78577 [00:36<00:09, 1711.52 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  80%|███████▉  | 62664/78577 [00:36<00:09, 1607.89 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  80%|███████▉  | 62849/78577 [00:36<00:09, 1668.20 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  80%|████████  | 63020/78577 [00:36<00:09, 1643.30 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  80%|████████  | 63211/78577 [00:36<00:09, 1669.28 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  81%|████████  | 63413/78577 [00:36<00:08, 1748.42 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  81%|████████  | 63591/78577 [00:36<00:08, 1752.05 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  81%|████████  | 63789/78577 [00:36<00:08, 1808.30 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  81%|████████▏ | 63981/78577 [00:37<00:08, 1772.28 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  82%|████████▏ | 64168/78577 [00:37<00:08, 1723.04 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  82%|████████▏ | 64362/78577 [00:37<00:08, 1671.27 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  82%|████████▏ | 64567/78577 [00:37<00:08, 1695.06 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  82%|████████▏ | 64767/78577 [00:37<00:08, 1708.47 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  83%|████████▎ | 64942/78577 [00:37<00:08, 1666.38 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  83%|████████▎ | 65115/78577 [00:37<00:08, 1663.46 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  83%|████████▎ | 65325/78577 [00:37<00:07, 1711.02 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  83%|████████▎ | 65546/78577 [00:38<00:07, 1790.40 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  84%|████████▎ | 65736/78577 [00:38<00:07, 1776.49 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  84%|████████▍ | 65919/78577 [00:38<00:07, 1773.00 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  84%|████████▍ | 66116/78577 [00:38<00:06, 1821.81 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  84%|████████▍ | 66310/78577 [00:38<00:06, 1760.13 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  85%|████████▍ | 66504/78577 [00:38<00:06, 1808.80 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  85%|████████▍ | 66688/78577 [00:38<00:06, 1801.14 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  85%|████████▌ | 66883/78577 [00:38<00:06, 1782.12 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  85%|████████▌ | 67074/78577 [00:38<00:06, 1759.40 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  86%|████████▌ | 67275/78577 [00:38<00:06, 1812.25 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  86%|████████▌ | 67470/78577 [00:39<00:06, 1767.69 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  86%|████████▌ | 67687/78577 [00:39<00:05, 1820.95 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  86%|████████▋ | 67879/78577 [00:39<00:05, 1828.37 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  87%|████████▋ | 68071/78577 [00:39<00:05, 1792.06 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  87%|████████▋ | 68260/78577 [00:39<00:05, 1800.46 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  87%|████████▋ | 68442/78577 [00:39<00:05, 1786.54 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  87%|████████▋ | 68624/78577 [00:39<00:05, 1774.14 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  88%|████████▊ | 68817/78577 [00:39<00:05, 1775.90 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  88%|████████▊ | 68996/78577 [00:39<00:05, 1716.29 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  88%|████████▊ | 69185/78577 [00:40<00:05, 1729.62 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  88%|████████▊ | 69367/78577 [00:40<00:05, 1737.59 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  89%|████████▊ | 69543/78577 [00:40<00:05, 1687.10 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  89%|████████▉ | 69757/78577 [00:40<00:04, 1793.36 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  89%|████████▉ | 69949/78577 [00:40<00:04, 1730.23 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  89%|████████▉ | 70134/78577 [00:40<00:05, 1628.83 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  89%|████████▉ | 70302/78577 [00:40<00:05, 1598.43 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  90%|████████▉ | 70473/78577 [00:40<00:05, 1609.02 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  90%|████████▉ | 70653/78577 [00:40<00:04, 1660.98 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  90%|█████████ | 70824/78577 [00:41<00:04, 1663.61 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  90%|█████████ | 71008/78577 [00:41<00:04, 1691.31 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  91%|█████████ | 71190/78577 [00:41<00:04, 1712.57 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  91%|█████████ | 71366/78577 [00:41<00:04, 1701.75 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  91%|█████████ | 71559/78577 [00:41<00:03, 1756.13 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  91%|█████████▏| 71746/78577 [00:41<00:03, 1775.03 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  92%|█████████▏| 71933/78577 [00:41<00:03, 1768.31 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  92%|█████████▏| 72125/78577 [00:41<00:03, 1694.25 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  92%|█████████▏| 72311/78577 [00:41<00:03, 1723.07 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  92%|█████████▏| 72488/78577 [00:42<00:03, 1662.59 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  92%|█████████▏| 72676/78577 [00:42<00:03, 1667.44 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  93%|█████████▎| 72874/78577 [00:42<00:03, 1735.83 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  93%|█████████▎| 73058/78577 [00:42<00:03, 1722.35 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  93%|█████████▎| 73231/78577 [00:42<00:03, 1669.88 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  93%|█████████▎| 73407/78577 [00:42<00:03, 1695.13 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  94%|█████████▎| 73580/78577 [00:42<00:02, 1681.49 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  94%|█████████▍| 73780/78577 [00:42<00:02, 1715.73 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  94%|█████████▍| 74002/78577 [00:42<00:02, 1819.75 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  94%|█████████▍| 74193/78577 [00:42<00:02, 1802.26 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  95%|█████████▍| 74381/78577 [00:43<00:02, 1730.64 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  95%|█████████▍| 74580/78577 [00:43<00:02, 1781.39 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  95%|█████████▌| 74772/78577 [00:43<00:02, 1774.63 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  95%|█████████▌| 74974/78577 [00:43<00:01, 1827.31 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  96%|█████████▌| 75164/78577 [00:43<00:01, 1828.21 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  96%|█████████▌| 75356/78577 [00:43<00:01, 1779.56 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  96%|█████████▌| 75566/78577 [00:43<00:01, 1864.49 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  96%|█████████▋| 75761/78577 [00:43<00:01, 1784.87 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  97%|█████████▋| 75984/78577 [00:43<00:01, 1862.58 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  97%|█████████▋| 76172/78577 [00:44<00:01, 1795.00 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  97%|█████████▋| 76360/78577 [00:44<00:01, 1810.60 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  97%|█████████▋| 76549/78577 [00:44<00:01, 1721.95 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  98%|█████████▊| 76723/78577 [00:44<00:01, 1659.92 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  98%|█████████▊| 76903/78577 [00:44<00:01, 1539.22 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  98%|█████████▊| 77066/78577 [00:44<00:00, 1517.80 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  98%|█████████▊| 77274/78577 [00:44<00:00, 1593.28 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  99%|█████████▊| 77500/78577 [00:44<00:00, 1702.91 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  99%|█████████▉| 77721/78577 [00:45<00:00, 1742.50 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  99%|█████████▉| 77913/78577 [00:45<00:00, 1725.22 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  99%|█████████▉| 78108/78577 [00:45<00:00, 1782.37 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8): 100%|█████████▉| 78306/78577 [00:45<00:00, 1664.67 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8): 100%|█████████▉| 78534/78577 [00:45<00:00, 1610.00 examples/s]\u001b[0m\n",
      "\u001b[34mFilter (num_proc=8):   0%|          | 0/77791 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8): 100%|██████████| 78577/78577 [00:45<00:00, 1721.36 examples/s]\u001b[0m\n",
      "\u001b[35m[2023-10-27 21:37:51,486] [INFO] [axolotl.load_tokenized_prepared_datasets:354] [PID:196] [RANK:0] merging datasets#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 21:37:51,489] [INFO] [axolotl.load_tokenized_prepared_datasets:361] [PID:196] [RANK:0] Saving merged prepared dataset to disk... /tmp/last_run_prepared/602d3a7380f73b0eb426ee7159f348f4#033[39m\u001b[0m\n",
      "\u001b[35mSaving the dataset (0/1 shards):   0%|          | 0/78577 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[35mSaving the dataset (1/1 shards): 100%|██████████| 78577/78577 [00:00<00:00, 1053701.90 examples/s]\u001b[0m\n",
      "\u001b[35mSaving the dataset (1/1 shards): 100%|██████████| 78577/78577 [00:00<00:00, 1050678.64 examples/s]\u001b[0m\n",
      "\u001b[34mFilter (num_proc=8):   1%|▏         | 1000/77791 [00:00<00:57, 1331.91 examples/s]\u001b[0m\n",
      "\u001b[34mFilter (num_proc=8):   5%|▌         | 4000/77791 [00:00<00:13, 5662.60 examples/s]\u001b[0m\n",
      "\u001b[34mFilter (num_proc=8):  12%|█▏        | 9000/77791 [00:01<00:10, 6698.54 examples/s]\u001b[0m\n",
      "\u001b[34mFilter (num_proc=8):  18%|█▊        | 14000/77791 [00:01<00:06, 10598.87 examples/s]\u001b[0m\n",
      "\u001b[34mFilter (num_proc=8):  22%|██▏       | 17000/77791 [00:02<00:07, 8481.52 examples/s]\u001b[0m\n",
      "\u001b[34mFilter (num_proc=8):  24%|██▍       | 19000/77791 [00:02<00:06, 9500.31 examples/s]\u001b[0m\n",
      "\u001b[34mFilter (num_proc=8):  28%|██▊       | 22000/77791 [00:02<00:04, 11834.20 examples/s]\u001b[0m\n",
      "\u001b[34mFilter (num_proc=8):  32%|███▏      | 25000/77791 [00:03<00:05, 8872.16 examples/s]\u001b[0m\n",
      "\u001b[34mFilter (num_proc=8):  35%|███▍      | 27000/77791 [00:03<00:05, 9857.37 examples/s]\u001b[0m\n",
      "\u001b[34mFilter (num_proc=8):  39%|███▊      | 30000/77791 [00:03<00:03, 12257.74 examples/s]\u001b[0m\n",
      "\u001b[34mFilter (num_proc=8):  41%|████      | 32000/77791 [00:03<00:03, 13041.60 examples/s]\u001b[0m\n",
      "\u001b[34mFilter (num_proc=8):  44%|████▎     | 34000/77791 [00:03<00:04, 8848.61 examples/s]\u001b[0m\n",
      "\u001b[34mFilter (num_proc=8):  48%|████▊     | 37000/77791 [00:03<00:03, 11536.61 examples/s]\u001b[0m\n",
      "\u001b[34mFilter (num_proc=8):  50%|█████     | 39000/77791 [00:04<00:03, 11676.16 examples/s]\u001b[0m\n",
      "\u001b[34mFilter (num_proc=8):  53%|█████▎    | 41000/77791 [00:04<00:04, 8997.27 examples/s]\u001b[0m\n",
      "\u001b[34mFilter (num_proc=8):  55%|█████▌    | 43000/77791 [00:04<00:03, 10145.86 examples/s]\u001b[0m\n",
      "\u001b[34mFilter (num_proc=8):  58%|█████▊    | 45000/77791 [00:04<00:02, 11574.43 examples/s]\u001b[0m\n",
      "\u001b[34mFilter (num_proc=8):  60%|██████    | 47000/77791 [00:04<00:02, 11229.35 examples/s]\u001b[0m\n",
      "\u001b[34mFilter (num_proc=8):  63%|██████▎   | 49000/77791 [00:05<00:03, 9184.00 examples/s]\u001b[0m\n",
      "\u001b[34mFilter (num_proc=8):  66%|██████▌   | 51000/77791 [00:05<00:02, 10186.90 examples/s]\u001b[0m\n",
      "\u001b[34mFilter (num_proc=8):  69%|██████▉   | 54000/77791 [00:05<00:01, 11988.46 examples/s]\u001b[0m\n",
      "\u001b[34mFilter (num_proc=8):  72%|███████▏  | 56000/77791 [00:05<00:02, 9569.38 examples/s]\u001b[0m\n",
      "\u001b[34mFilter (num_proc=8):  75%|███████▍  | 58000/77791 [00:06<00:01, 10221.72 examples/s]\u001b[0m\n",
      "\u001b[34mFilter (num_proc=8):  77%|███████▋  | 60000/77791 [00:06<00:01, 11439.79 examples/s]\u001b[0m\n",
      "\u001b[34mFilter (num_proc=8):  80%|███████▉  | 62000/77791 [00:06<00:01, 12602.73 examples/s]\u001b[0m\n",
      "\u001b[34mFilter (num_proc=8):  82%|████████▏ | 64000/77791 [00:06<00:01, 8397.81 examples/s]\u001b[0m\n",
      "\u001b[34mFilter (num_proc=8):  86%|████████▌ | 67000/77791 [00:06<00:01, 10115.13 examples/s]\u001b[0m\n",
      "\u001b[34mFilter (num_proc=8):  90%|████████▉ | 70000/77791 [00:07<00:00, 12864.94 examples/s]\u001b[0m\n",
      "\u001b[34mFilter (num_proc=8):  92%|█████████▏| 71724/77791 [00:07<00:00, 10138.81 examples/s]\u001b[0m\n",
      "\u001b[34mFilter (num_proc=8):  95%|█████████▍| 73895/77791 [00:07<00:00, 11468.25 examples/s]\u001b[0m\n",
      "\u001b[34mFilter (num_proc=8):  99%|█████████▉| 77067/77791 [00:07<00:00, 14286.40 examples/s]\u001b[0m\n",
      "\u001b[34mFilter (num_proc=8): 100%|██████████| 77791/77791 [00:07<00:00, 10148.87 examples/s]\u001b[0m\n",
      "\u001b[34mFilter (num_proc=8):   0%|          | 0/786 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mFilter (num_proc=8):  13%|█▎        | 99/786 [00:00<00:00, 717.40 examples/s]\u001b[0m\n",
      "\u001b[34mFilter (num_proc=8): 100%|██████████| 786/786 [00:00<00:00, 3479.65 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):   0%|          | 0/77791 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):   0%|          | 40/77791 [00:00<05:31, 234.55 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):   2%|▏         | 1306/77791 [00:00<00:12, 5897.85 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):   3%|▎         | 2292/77791 [00:00<00:10, 7449.54 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):   4%|▍         | 3357/77791 [00:00<00:09, 8244.48 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):   5%|▌         | 4275/77791 [00:00<00:08, 8448.03 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):   7%|▋         | 5195/77791 [00:00<00:08, 8564.34 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):   8%|▊         | 6126/77791 [00:00<00:08, 8698.85 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):   9%|▉         | 7066/77791 [00:00<00:08, 8203.72 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  10%|█         | 7923/77791 [00:01<00:12, 5506.51 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  11%|█         | 8642/77791 [00:01<00:12, 5606.75 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  12%|█▏        | 9568/77791 [00:01<00:10, 6315.08 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  14%|█▎        | 10608/77791 [00:01<00:09, 7227.08 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  15%|█▍        | 11464/77791 [00:01<00:09, 7343.28 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  16%|█▌        | 12486/77791 [00:01<00:08, 8007.40 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  17%|█▋        | 13501/77791 [00:01<00:07, 8494.09 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  19%|█▊        | 14457/77791 [00:01<00:07, 8325.36 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  20%|█▉        | 15322/77791 [00:02<00:07, 8344.13 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  21%|██        | 16240/77791 [00:02<00:11, 5464.59 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  22%|██▏       | 17050/77791 [00:02<00:10, 5857.13 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  23%|██▎       | 18069/77791 [00:02<00:08, 6746.44 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  24%|██▍       | 18862/77791 [00:02<00:08, 6963.20 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  26%|██▌       | 19867/77791 [00:02<00:07, 7509.51 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  27%|██▋       | 20956/77791 [00:02<00:07, 8056.09 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  28%|██▊       | 21968/77791 [00:03<00:06, 8464.18 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  29%|██▉       | 22906/77791 [00:03<00:06, 8379.35 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  31%|███       | 23800/77791 [00:03<00:08, 6665.19 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  32%|███▏      | 24609/77791 [00:03<00:09, 5902.89 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  33%|███▎      | 25477/77791 [00:03<00:08, 6399.38 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  34%|███▍      | 26422/77791 [00:03<00:07, 7047.88 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  35%|███▌      | 27370/77791 [00:03<00:06, 7582.48 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  36%|███▋      | 28340/77791 [00:03<00:06, 8049.76 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  38%|███▊      | 29358/77791 [00:04<00:05, 8266.89 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  39%|███▉      | 30346/77791 [00:04<00:05, 8610.42 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  40%|████      | 31301/77791 [00:04<00:05, 7828.29 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  41%|████▏     | 32143/77791 [00:04<00:07, 6439.21 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  42%|████▏     | 32859/77791 [00:04<00:07, 6004.12 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  43%|████▎     | 33821/77791 [00:04<00:06, 6717.32 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  45%|████▍     | 34765/77791 [00:04<00:05, 7370.38 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  46%|████▌     | 35662/77791 [00:04<00:05, 7718.18 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  47%|████▋     | 36632/77791 [00:05<00:05, 8074.93 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  48%|████▊     | 37589/77791 [00:05<00:04, 8386.60 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  49%|████▉     | 38493/77791 [00:05<00:04, 8293.26 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  51%|█████     | 39396/77791 [00:05<00:04, 8132.18 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  52%|█████▏    | 40240/77791 [00:05<00:06, 5980.56 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  53%|█████▎    | 40938/77791 [00:05<00:06, 6099.08 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  54%|█████▍    | 41875/77791 [00:05<00:05, 6747.92 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  55%|█████▌    | 42833/77791 [00:05<00:04, 7382.43 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  56%|█████▋    | 43840/77791 [00:06<00:04, 7839.36 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  58%|█████▊    | 44799/77791 [00:06<00:04, 8214.53 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  59%|█████▉    | 45711/77791 [00:06<00:03, 8197.58 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  60%|█████▉    | 46634/77791 [00:06<00:03, 8385.69 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  61%|██████    | 47547/77791 [00:06<00:03, 8012.90 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  62%|██████▏   | 48372/77791 [00:06<00:04, 6096.01 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  63%|██████▎   | 49065/77791 [00:06<00:04, 6263.96 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  64%|██████▍   | 49903/77791 [00:06<00:04, 6767.99 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  65%|██████▌   | 50848/77791 [00:07<00:03, 7307.08 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  67%|██████▋   | 51761/77791 [00:07<00:03, 7549.20 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  68%|██████▊   | 52853/77791 [00:07<00:02, 8361.70 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  69%|██████▉   | 53767/77791 [00:07<00:02, 8306.77 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  70%|███████   | 54647/77791 [00:07<00:02, 8084.64 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  71%|███████▏  | 55537/77791 [00:07<00:02, 7877.11 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  72%|███████▏  | 56386/77791 [00:07<00:03, 5785.18 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  74%|███████▎  | 57316/77791 [00:07<00:03, 6441.93 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  75%|███████▍  | 58135/77791 [00:08<00:02, 6722.96 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  76%|███████▌  | 59311/77791 [00:08<00:02, 7453.84 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  78%|███████▊  | 60484/77791 [00:08<00:02, 7958.89 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  79%|███████▉  | 61518/77791 [00:08<00:02, 7996.31 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  80%|████████  | 62603/77791 [00:08<00:01, 8218.07 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  82%|████████▏ | 63575/77791 [00:08<00:01, 8027.09 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  83%|████████▎ | 64410/77791 [00:08<00:02, 6089.68 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  84%|████████▍ | 65218/77791 [00:09<00:01, 6294.97 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  85%|████████▌ | 66232/77791 [00:09<00:01, 6917.46 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  87%|████████▋ | 67377/77791 [00:09<00:01, 7638.43 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  88%|████████▊ | 68329/77791 [00:09<00:01, 7868.00 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  89%|████████▉ | 69246/77791 [00:09<00:01, 8114.57 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  90%|█████████ | 70133/77791 [00:09<00:00, 8218.92 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  91%|█████████▏| 71110/77791 [00:09<00:00, 8413.69 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  93%|█████████▎| 71983/77791 [00:09<00:00, 7477.07 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  94%|█████████▎| 72815/77791 [00:10<00:00, 5981.59 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  95%|█████████▍| 73570/77791 [00:10<00:00, 6270.06 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  96%|█████████▌| 74436/77791 [00:10<00:00, 6713.68 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  97%|█████████▋| 75344/77791 [00:10<00:00, 7192.75 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  98%|█████████▊| 76335/77791 [00:10<00:00, 7865.51 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  99%|█████████▉| 77264/77791 [00:10<00:00, 7986.32 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8): 100%|██████████| 77791/77791 [00:10<00:00, 7153.52 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):   0%|          | 0/786 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[35mFilter (num_proc=8):   0%|          | 0/77791 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8):  13%|█▎        | 99/786 [00:00<00:01, 670.19 examples/s]\u001b[0m\n",
      "\u001b[34mMap (num_proc=8): 100%|██████████| 786/786 [00:00<00:00, 2804.82 examples/s]\u001b[0m\n",
      "\u001b[34m[2023-10-27 21:38:11,274] [INFO] [axolotl.calculate_total_num_steps:438] [PID:197] [RANK:0] calculating total_num_tokens#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 21:38:11,325] [INFO] [axolotl.calculate_total_num_steps:445] [PID:197] [RANK:0] total_num_tokens: 5981037#033[39m\u001b[0m\n",
      "\u001b[35mFilter (num_proc=8):   1%|▏         | 1000/77791 [00:00<00:51, 1487.01 examples/s]\u001b[0m\n",
      "\u001b[35mFilter (num_proc=8):   4%|▍         | 3000/77791 [00:00<00:16, 4410.02 examples/s]\u001b[0m\n",
      "\u001b[35mFilter (num_proc=8):  10%|█         | 8000/77791 [00:00<00:05, 12595.45 examples/s]\u001b[0m\n",
      "\u001b[34m[2023-10-27 21:38:12,023] [INFO] [axolotl.calculate_total_num_steps:455] [PID:197] [RANK:0] `total_supervised_tokens: 2375856`#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 21:38:12,266] [INFO] [axolotl.utils.dataloader.generate_batches:181] [PID:197] [RANK:0] generating packed batches#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 21:38:12,277] [INFO] [axolotl.utils.dataloader.generate_batches:187] [PID:197] [RANK:0] d63c4b02fd273fcfca4f35f9e74d4e7d950b89370bffa79889d002b36cecbf8c#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 21:38:12,462] [INFO] [axolotl.utils.dataloader.len_w_stats:295] [PID:197] [RANK:0] packing_efficiency_estimate: 1.0 actual packing efficiency: 0.9944785635217984#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 21:38:12,462] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:197] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 2990518#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 21:38:12,462] [INFO] [axolotl.calculate_total_num_steps:504] [PID:197] [RANK:0] data_loader_len: 360#033[39m\u001b[0m\n",
      "\u001b[35mFilter (num_proc=8):  14%|█▍        | 11000/77791 [00:01<00:07, 8360.88 examples/s]\u001b[0m\n",
      "\u001b[35mFilter (num_proc=8):  19%|█▉        | 15000/77791 [00:01<00:05, 12271.42 examples/s]\u001b[0m\n",
      "\u001b[35mFilter (num_proc=8):  23%|██▎       | 18000/77791 [00:02<00:06, 8790.64 examples/s]\u001b[0m\n",
      "\u001b[35mFilter (num_proc=8):  28%|██▊       | 22000/77791 [00:02<00:04, 12282.76 examples/s]\u001b[0m\n",
      "\u001b[35mFilter (num_proc=8):  32%|███▏      | 25000/77791 [00:02<00:05, 9102.10 examples/s]\u001b[0m\n",
      "\u001b[35mFilter (num_proc=8):  37%|███▋      | 29000/77791 [00:02<00:03, 12488.59 examples/s]\u001b[0m\n",
      "\u001b[35mFilter (num_proc=8):  41%|████      | 32000/77791 [00:03<00:03, 13745.17 examples/s]\u001b[0m\n",
      "\u001b[35mFilter (num_proc=8):  45%|████▍     | 35000/77791 [00:03<00:04, 10174.32 examples/s]\u001b[0m\n",
      "\u001b[35mFilter (num_proc=8):  50%|█████     | 39000/77791 [00:03<00:02, 13109.12 examples/s]\u001b[0m\n",
      "\u001b[35mFilter (num_proc=8):  54%|█████▍    | 42000/77791 [00:04<00:03, 10276.62 examples/s]\u001b[0m\n",
      "\u001b[35mFilter (num_proc=8):  57%|█████▋    | 44000/77791 [00:04<00:02, 11317.00 examples/s]\u001b[0m\n",
      "\u001b[35mFilter (num_proc=8):  60%|██████    | 47000/77791 [00:04<00:02, 13102.21 examples/s]\u001b[0m\n",
      "\u001b[35mFilter (num_proc=8):  63%|██████▎   | 49000/77791 [00:04<00:02, 9747.39 examples/s]\u001b[0m\n",
      "\u001b[35mFilter (num_proc=8):  66%|██████▌   | 51000/77791 [00:04<00:02, 11049.41 examples/s]\u001b[0m\n",
      "\u001b[35mFilter (num_proc=8):  71%|███████   | 55000/77791 [00:05<00:01, 13061.91 examples/s]\u001b[0m\n",
      "\u001b[35mFilter (num_proc=8):  73%|███████▎  | 57000/77791 [00:05<00:02, 10243.05 examples/s]\u001b[0m\n",
      "\u001b[35mFilter (num_proc=8):  76%|███████▌  | 59000/77791 [00:05<00:01, 11389.13 examples/s]\u001b[0m\n",
      "\u001b[35mFilter (num_proc=8):  81%|████████  | 63000/77791 [00:05<00:01, 12441.52 examples/s]\u001b[0m\n",
      "\u001b[35mFilter (num_proc=8):  84%|████████▎ | 65000/77791 [00:06<00:01, 10456.64 examples/s]\u001b[0m\n",
      "\u001b[35mFilter (num_proc=8):  87%|████████▋ | 68000/77791 [00:06<00:00, 12709.42 examples/s]\u001b[0m\n",
      "\u001b[35mFilter (num_proc=8):  91%|█████████▏| 71000/77791 [00:06<00:00, 11959.85 examples/s]\u001b[0m\n",
      "\u001b[35mFilter (num_proc=8):  93%|█████████▎| 72448/77791 [00:06<00:00, 12173.80 examples/s]\u001b[0m\n",
      "\u001b[35mFilter (num_proc=8):  98%|█████████▊| 76343/77791 [00:06<00:00, 16632.72 examples/s]\u001b[0m\n",
      "\u001b[35mFilter (num_proc=8): 100%|██████████| 77791/77791 [00:06<00:00, 11183.86 examples/s]\u001b[0m\n",
      "\u001b[35mFilter (num_proc=8):   0%|          | 0/786 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[35mFilter (num_proc=8):  13%|█▎        | 99/786 [00:00<00:00, 797.42 examples/s]\u001b[0m\n",
      "\u001b[35mFilter (num_proc=8): 100%|██████████| 786/786 [00:00<00:00, 3572.41 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):   0%|          | 0/77791 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):   0%|          | 70/77791 [00:00<03:00, 430.79 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):   2%|▏         | 1197/77791 [00:00<00:13, 5498.36 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):   3%|▎         | 2449/77791 [00:00<00:09, 8068.47 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):   4%|▍         | 3418/77791 [00:00<00:09, 8071.40 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):   6%|▌         | 4323/77791 [00:00<00:08, 8282.14 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):   7%|▋         | 5256/77791 [00:00<00:08, 8501.29 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):   8%|▊         | 6195/77791 [00:00<00:08, 8667.33 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):   9%|▉         | 7124/77791 [00:00<00:08, 8443.79 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  10%|█         | 8048/77791 [00:01<00:12, 5440.43 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  11%|█▏        | 8780/77791 [00:01<00:12, 5732.51 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  13%|█▎        | 9724/77791 [00:01<00:10, 6544.87 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  14%|█▎        | 10696/77791 [00:01<00:09, 7211.20 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  15%|█▍        | 11594/77791 [00:01<00:08, 7585.32 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  16%|█▌        | 12576/77791 [00:01<00:08, 8087.77 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  17%|█▋        | 13467/77791 [00:01<00:07, 8225.41 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  19%|█▊        | 14494/77791 [00:01<00:07, 8602.76 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  20%|█▉        | 15381/77791 [00:02<00:07, 8045.30 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  21%|██        | 16252/77791 [00:02<00:10, 5656.38 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  22%|██▏       | 17046/77791 [00:02<00:09, 6085.08 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  23%|██▎       | 18034/77791 [00:02<00:08, 6767.88 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  25%|██▍       | 19087/77791 [00:02<00:07, 7604.90 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  26%|██▌       | 19997/77791 [00:02<00:07, 7904.16 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  27%|██▋       | 21054/77791 [00:02<00:06, 8263.09 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  28%|██▊       | 22158/77791 [00:02<00:06, 8906.51 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  30%|██▉       | 23134/77791 [00:03<00:06, 8662.61 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  31%|███       | 24039/77791 [00:03<00:08, 6118.55 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  32%|███▏      | 24787/77791 [00:03<00:08, 6003.02 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  33%|███▎      | 25800/77791 [00:03<00:07, 6795.12 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  34%|███▍      | 26741/77791 [00:03<00:07, 7276.74 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  36%|███▌      | 27709/77791 [00:03<00:06, 7801.44 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  37%|███▋      | 28682/77791 [00:03<00:05, 8215.64 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  38%|███▊      | 29576/77791 [00:04<00:05, 8319.57 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  39%|███▉      | 30634/77791 [00:04<00:05, 8299.56 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  41%|████      | 31574/77791 [00:04<00:05, 7998.47 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  42%|████▏     | 32428/77791 [00:04<00:07, 5908.25 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  43%|████▎     | 33249/77791 [00:04<00:07, 6281.15 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  44%|████▍     | 34214/77791 [00:04<00:06, 6944.57 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  45%|████▌     | 35339/77791 [00:04<00:05, 7769.49 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  47%|████▋     | 36404/77791 [00:04<00:04, 8331.73 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  48%|████▊     | 37346/77791 [00:05<00:04, 8519.01 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  49%|████▉     | 38239/77791 [00:05<00:04, 8547.39 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  50%|█████     | 39154/77791 [00:05<00:04, 8438.11 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  52%|█████▏    | 40084/77791 [00:05<00:06, 5968.42 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  52%|█████▏    | 40827/77791 [00:05<00:06, 6022.64 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  54%|█████▎    | 41741/77791 [00:05<00:05, 6726.20 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  55%|█████▍    | 42689/77791 [00:05<00:04, 7319.26 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  56%|█████▌    | 43662/77791 [00:05<00:04, 7790.45 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  57%|█████▋    | 44672/77791 [00:06<00:04, 8140.11 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  59%|█████▉    | 45715/77791 [00:06<00:03, 8656.77 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  60%|█████▉    | 46612/77791 [00:06<00:03, 8493.69 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  61%|██████    | 47532/77791 [00:06<00:03, 7809.11 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  62%|██████▏   | 48345/77791 [00:06<00:05, 5765.91 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  63%|██████▎   | 49247/77791 [00:06<00:04, 6206.18 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  65%|██████▍   | 50390/77791 [00:06<00:03, 7072.70 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  66%|██████▋   | 51543/77791 [00:07<00:03, 7556.03 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  68%|██████▊   | 52730/77791 [00:07<00:03, 8047.71 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  69%|██████▉   | 53855/77791 [00:07<00:02, 8263.02 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  71%|███████   | 54913/77791 [00:07<00:02, 8261.80 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  72%|███████▏  | 55779/77791 [00:07<00:02, 7680.57 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  73%|███████▎  | 56574/77791 [00:07<00:03, 6177.01 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  74%|███████▍  | 57429/77791 [00:07<00:03, 6637.18 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  75%|███████▍  | 58230/77791 [00:07<00:02, 6959.12 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  76%|███████▋  | 59386/77791 [00:08<00:02, 7727.18 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  78%|███████▊  | 60490/77791 [00:08<00:02, 8238.24 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  79%|███████▉  | 61385/77791 [00:08<00:01, 8364.45 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  80%|████████  | 62257/77791 [00:08<00:01, 8166.48 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  81%|████████  | 63146/77791 [00:08<00:01, 8361.37 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  82%|████████▏ | 64006/77791 [00:08<00:02, 6768.10 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  83%|████████▎ | 64781/77791 [00:08<00:02, 6118.03 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  84%|████████▍ | 65487/77791 [00:09<00:01, 6301.46 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  85%|████████▌ | 66507/77791 [00:09<00:01, 7242.96 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  87%|████████▋ | 67444/77791 [00:09<00:01, 7714.96 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  88%|████████▊ | 68421/77791 [00:09<00:01, 8181.89 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  89%|████████▉ | 69310/77791 [00:09<00:01, 8282.19 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  90%|█████████ | 70210/77791 [00:09<00:00, 8394.23 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  91%|█████████▏| 71084/77791 [00:09<00:00, 8120.94 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  92%|█████████▏| 71950/77791 [00:09<00:00, 6944.42 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  94%|█████████▎| 72735/77791 [00:09<00:00, 6427.01 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  94%|█████████▍| 73475/77791 [00:10<00:00, 6451.00 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  96%|█████████▌| 74359/77791 [00:10<00:00, 6978.46 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  97%|█████████▋| 75214/77791 [00:10<00:00, 7316.81 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  98%|█████████▊| 76181/77791 [00:10<00:00, 7939.20 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  99%|█████████▉| 77059/77791 [00:10<00:00, 7595.73 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8): 100%|██████████| 77791/77791 [00:10<00:00, 7235.47 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):   0%|          | 0/786 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8):  13%|█▎        | 99/786 [00:00<00:01, 630.16 examples/s]\u001b[0m\n",
      "\u001b[35mMap (num_proc=8): 100%|██████████| 786/786 [00:00<00:00, 2867.10 examples/s]\u001b[0m\n",
      "\u001b[35m[2023-10-27 21:38:30,136] [INFO] [axolotl.calculate_total_num_steps:438] [PID:196] [RANK:0] calculating total_num_tokens#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 21:38:30,186] [INFO] [axolotl.calculate_total_num_steps:445] [PID:196] [RANK:0] total_num_tokens: 5981037#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 21:38:30,869] [INFO] [axolotl.calculate_total_num_steps:455] [PID:196] [RANK:0] `total_supervised_tokens: 2375856`#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 21:38:31,107] [INFO] [axolotl.utils.dataloader.generate_batches:181] [PID:196] [RANK:0] generating packed batches#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 21:38:31,117] [INFO] [axolotl.utils.dataloader.generate_batches:187] [PID:196] [RANK:0] 46d2bcf7ad43bd02a8a37789a863e9ba5017c1f42d04a1cbf7dec01c0747c14f#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 21:38:31,294] [INFO] [axolotl.utils.dataloader.len_w_stats:295] [PID:196] [RANK:0] packing_efficiency_estimate: 1.0 actual packing efficiency: 0.9949349135728883#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 21:38:31,295] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:196] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 2990518#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 21:38:31,295] [INFO] [axolotl.calculate_total_num_steps:504] [PID:196] [RANK:0] data_loader_len: 360#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 21:38:31,456] [INFO] [axolotl.calc_sample_packing_eff_est:510] [PID:197] [RANK:0] sample_packing_eff_est across ranks: [0.9944785833358765, 0.9949349164962769]#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 21:38:31,457] [INFO] [axolotl.calculate_total_num_steps:521] [PID:197] [RANK:0] sample_packing_eff_est: 1.0#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 21:38:31,457] [INFO] [axolotl.calculate_total_num_steps:526] [PID:197] [RANK:0] total_num_steps: 1080#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 21:38:31,463] [INFO] [axolotl.train.train:48] [PID:197] [RANK:0] loading tokenizer... /opt/ml/input/data/model#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 21:38:31,588] [DEBUG] [axolotl.load_tokenizer:74] [PID:197] [RANK:0] EOS: 2 / </s>#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 21:38:31,588] [DEBUG] [axolotl.load_tokenizer:75] [PID:197] [RANK:0] BOS: 1 / <s>#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 21:38:31,588] [DEBUG] [axolotl.load_tokenizer:76] [PID:197] [RANK:0] PAD: 2 / </s>#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 21:38:31,588] [DEBUG] [axolotl.load_tokenizer:77] [PID:197] [RANK:0] UNK: 0 / <unk>#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 21:38:31,731] [INFO] [axolotl.train.train:56] [PID:197] [RANK:0] loading model and (optionally) peft_config...#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 21:38:31,737] [INFO] [axolotl.load_model:122] [PID:197] [RANK:0] patching with flash attention for sample packing#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 21:38:31,737] [INFO] [axolotl.load_model:175] [PID:197] [RANK:0] patching _expand_mask#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 21:38:31,460] [INFO] [axolotl.calculate_total_num_steps:521] [PID:196] [RANK:0] sample_packing_eff_est: 1.0#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 21:38:31,460] [INFO] [axolotl.calculate_total_num_steps:526] [PID:196] [RANK:0] total_num_steps: 1080#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 21:38:31,466] [INFO] [axolotl.train.train:48] [PID:196] [RANK:0] loading tokenizer... /opt/ml/input/data/model#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 21:38:31,587] [DEBUG] [axolotl.load_tokenizer:74] [PID:196] [RANK:0] EOS: 2 / </s>#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 21:38:31,587] [DEBUG] [axolotl.load_tokenizer:75] [PID:196] [RANK:0] BOS: 1 / <s>#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 21:38:31,587] [DEBUG] [axolotl.load_tokenizer:76] [PID:196] [RANK:0] PAD: 2 / </s>#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 21:38:31,587] [DEBUG] [axolotl.load_tokenizer:77] [PID:196] [RANK:0] UNK: 0 / <unk>#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 21:38:31,724] [INFO] [axolotl.train.train:56] [PID:196] [RANK:0] loading model and (optionally) peft_config...#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 21:38:31,730] [INFO] [axolotl.load_model:122] [PID:196] [RANK:0] patching with flash attention for sample packing#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 21:38:31,731] [INFO] [axolotl.load_model:175] [PID:196] [RANK:0] patching _expand_mask#033[39m\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.72s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.74s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 2/2 [00:19<00:00,  9.48s/it]\u001b[0m\n",
      "\u001b[35mLoading checkpoint shards: 100%|██████████| 2/2 [00:19<00:00,  9.52s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:19<00:00,  9.49s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:19<00:00,  9.52s/it]\u001b[0m\n",
      "\u001b[34m[2023-10-27 21:38:51,549] [INFO] [axolotl.load_model:358] [PID:197] [RANK:0] GPU memory usage after model load: 3.873GB (+0.120GB cache, +0.684GB misc)#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 21:38:51,553] [INFO] [axolotl.load_model:375] [PID:197] [RANK:0] converting PEFT model w/ prepare_model_for_kbit_training#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 21:38:51,557] [INFO] [axolotl.load_model:386] [PID:197] [RANK:0] converting modules to torch.bfloat16 for flash attention#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 21:38:51,560] [INFO] [axolotl.load_lora:491] [PID:197] [RANK:0] found linear modules: ['k_proj', 'down_proj', 'q_proj', 'o_proj', 'up_proj', 'gate_proj', 'v_proj']#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 21:38:51,521] [INFO] [axolotl.load_model:358] [PID:196] [RANK:0] GPU memory usage after model load: 3.873GB (+0.120GB cache, +0.684GB misc)#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 21:38:51,525] [INFO] [axolotl.load_model:375] [PID:196] [RANK:0] converting PEFT model w/ prepare_model_for_kbit_training#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 21:38:51,529] [INFO] [axolotl.load_model:386] [PID:196] [RANK:0] converting modules to torch.bfloat16 for flash attention#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 21:38:51,533] [INFO] [axolotl.load_lora:491] [PID:196] [RANK:0] found linear modules: ['q_proj', 'down_proj', 'up_proj', 'gate_proj', 'o_proj', 'v_proj', 'k_proj']#033[39m\u001b[0m\n",
      "\u001b[35mtrainable params: 79,953,920 || all params: 6,818,500,608 || trainable%: 1.172602667310637\u001b[0m\n",
      "\u001b[35m[2023-10-27 21:39:38,392] [INFO] [axolotl.load_model:422] [PID:196] [RANK:0] GPU memory usage after adapters: 4.175GB (+0.969GB cache, +0.684GB misc)#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 21:39:38,400] [INFO] [axolotl.train.train:84] [PID:196] [RANK:0] Pre-saving adapter config to /opt/ml/model#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 21:39:38,402] [INFO] [axolotl.train.train:108] [PID:196] [RANK:0] Starting trainer...#033[39m\u001b[0m\n",
      "\u001b[34mtrainable params: 79,953,920 || all params: 6,818,500,608 || trainable%: 1.172602667310637\u001b[0m\n",
      "\u001b[34m[2023-10-27 21:39:38,753] [INFO] [axolotl.load_model:422] [PID:197] [RANK:0] GPU memory usage after adapters: 4.175GB (+0.969GB cache, +0.684GB misc)#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 21:39:38,762] [INFO] [axolotl.train.train:84] [PID:197] [RANK:0] Pre-saving adapter config to /opt/ml/model#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 21:39:38,764] [INFO] [axolotl.train.train:108] [PID:197] [RANK:0] Starting trainer...#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 21:39:38,786] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:196] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 2990518#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 21:39:38,786] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:196] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 2990518#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 21:39:39,154] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:197] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 2990518#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 21:39:39,154] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:197] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 2990518#033[39m\u001b[0m\n",
      "\u001b[34mUsing /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[34mCreating extension directory /root/.cache/torch_extensions/py310_cu118/cpu_adam...\u001b[0m\n",
      "\u001b[34mDetected CUDA files, patching ldflags\u001b[0m\n",
      "\u001b[34mEmitting ninja build file /root/.cache/torch_extensions/py310_cu118/cpu_adam/build.ninja...\u001b[0m\n",
      "\u001b[34mBuilding extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\u001b[0m\n",
      "\u001b[35mUsing /root/.cache/torch_extensions/py310_cu118 as PyTorch extensions root...\u001b[0m\n",
      "\u001b[35mCreating extension directory /root/.cache/torch_extensions/py310_cu118/cpu_adam...\u001b[0m\n",
      "\u001b[35mDetected CUDA files, patching ldflags\u001b[0m\n",
      "\u001b[35mEmitting ninja build file /root/.cache/torch_extensions/py310_cu118/cpu_adam/build.ninja...\u001b[0m\n",
      "\u001b[35mBuilding extension module cpu_adam...\u001b[0m\n",
      "\u001b[35mAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\u001b[0m\n",
      "\u001b[34m[1/4] /opt/conda/bin/nvcc  -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -I/opt/conda/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.10/site-packages/torch/include/THC -isystem /opt/conda/include -isystem /opt/conda/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_86,code=compute_86 -DBF16_AVAILABLE -c /opt/conda/lib/python3.10/site-packages/deepspeed/ops/csrc/common/custom_cuda_kernel.cu -o custom_cuda_kernel.cuda.o\u001b[0m\n",
      "\u001b[35m[1/4] /opt/conda/bin/nvcc  -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -I/opt/conda/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.10/site-packages/torch/include/THC -isystem /opt/conda/include -isystem /opt/conda/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -gencode=arch=compute_86,code=sm_86 -gencode=arch=compute_86,code=compute_86 -DBF16_AVAILABLE -c /opt/conda/lib/python3.10/site-packages/deepspeed/ops/csrc/common/custom_cuda_kernel.cu -o custom_cuda_kernel.cuda.o\u001b[0m\n",
      "\u001b[35m[2/4] c++ -MMD -MF cpu_adam.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -I/opt/conda/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.10/site-packages/torch/include/THC -isystem /opt/conda/include -isystem /opt/conda/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -L/opt/conda/lib -lcudart -lcublas -g -march=native -fopenmp -D__AVX256__ -D__ENABLE_CUDA__ -DBF16_AVAILABLE -c /opt/conda/lib/python3.10/site-packages/deepspeed/ops/csrc/adam/cpu_adam.cpp -o cpu_adam.o\u001b[0m\n",
      "\u001b[35m[3/4] c++ -MMD -MF cpu_adam_impl.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -I/opt/conda/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.10/site-packages/torch/include/THC -isystem /opt/conda/include -isystem /opt/conda/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -L/opt/conda/lib -lcudart -lcublas -g -march=native -fopenmp -D__AVX256__ -D__ENABLE_CUDA__ -DBF16_AVAILABLE -c /opt/conda/lib/python3.10/site-packages/deepspeed/ops/csrc/adam/cpu_adam_impl.cpp -o cpu_adam_impl.o\u001b[0m\n",
      "\u001b[34m[2/4] c++ -MMD -MF cpu_adam.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -I/opt/conda/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.10/site-packages/torch/include/THC -isystem /opt/conda/include -isystem /opt/conda/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -L/opt/conda/lib -lcudart -lcublas -g -march=native -fopenmp -D__AVX256__ -D__ENABLE_CUDA__ -DBF16_AVAILABLE -c /opt/conda/lib/python3.10/site-packages/deepspeed/ops/csrc/adam/cpu_adam.cpp -o cpu_adam.o\u001b[0m\n",
      "\u001b[34m[3/4] c++ -MMD -MF cpu_adam_impl.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.10/site-packages/deepspeed/ops/csrc/includes -I/opt/conda/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.10/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.10/site-packages/torch/include/THC -isystem /opt/conda/include -isystem /opt/conda/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -L/opt/conda/lib -lcudart -lcublas -g -march=native -fopenmp -D__AVX256__ -D__ENABLE_CUDA__ -DBF16_AVAILABLE -c /opt/conda/lib/python3.10/site-packages/deepspeed/ops/csrc/adam/cpu_adam_impl.cpp -o cpu_adam_impl.o\u001b[0m\n",
      "\u001b[34m[4/4] c++ cpu_adam.o cpu_adam_impl.o custom_cuda_kernel.cuda.o -shared -lcurand -L/opt/conda/lib/python3.10/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/opt/conda/lib64 -lcudart -o cpu_adam.so\u001b[0m\n",
      "\u001b[34mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[34mTime to load cpu_adam op: 32.61504888534546 seconds\u001b[0m\n",
      "\u001b[35m[4/4] c++ cpu_adam.o cpu_adam_impl.o custom_cuda_kernel.cuda.o -shared -lcurand -L/opt/conda/lib/python3.10/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/opt/conda/lib64 -lcudart -o cpu_adam.so\u001b[0m\n",
      "\u001b[35mLoading extension module cpu_adam...\u001b[0m\n",
      "\u001b[35mTime to load cpu_adam op: 32.301244020462036 seconds\u001b[0m\n",
      "\u001b[35m0%|          | 0/270 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[2023-10-27 21:40:14,162] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:196] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 2990518#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 21:40:14,162] [INFO] [axolotl.utils.dataloader.__iter__:213] [PID:196] [RANK:0] calling sampler.set_epoch(1)#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 21:40:14,162] [INFO] [axolotl.utils.dataloader.generate_batches:181] [PID:196] [RANK:0] generating packed batches#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 21:40:14,172] [INFO] [axolotl.utils.dataloader.generate_batches:187] [PID:196] [RANK:0] 5feec6a8ac3dd0bb814736764b03247e92040db99b39ad425dd1c232292f47f8#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 21:40:14,347] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:196] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 2990518#033[39m\u001b[0m\n",
      "\u001b[34m0%|          | 0/270 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[2023-10-27 21:40:14,985] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:197] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 2990518#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 21:40:14,986] [INFO] [axolotl.utils.dataloader.__iter__:213] [PID:197] [RANK:0] calling sampler.set_epoch(1)#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 21:40:14,986] [INFO] [axolotl.utils.dataloader.generate_batches:181] [PID:197] [RANK:0] generating packed batches#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 21:40:14,997] [INFO] [axolotl.utils.dataloader.generate_batches:187] [PID:197] [RANK:0] 080fa726e0a9d6d143081fa75aefe107c5504c8e614347cbeb3a8d0c7196ffac#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 21:40:15,194] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:197] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 2990518#033[39m\u001b[0m\n",
      "\u001b[34m0%|          | 1/270 [00:34<2:35:48, 34.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0993, 'learning_rate': 0.0, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m0%|          | 1/270 [00:34<2:35:48, 34.75s/it]\u001b[0m\n",
      "\u001b[35m0%|          | 1/270 [00:35<2:39:29, 35.58s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 1.0993, 'learning_rate': 0.0, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[35m0%|          | 1/270 [00:35<2:39:29, 35.58s/it]\u001b[0m\n",
      "\u001b[35m[2023-10-27 21:41:24,140] [INFO] [axolotl.callbacks.on_step_end:122] [PID:196] [RANK:0] GPU memory usage while training: 4.038GB (+9.194GB cache, +0.721GB misc)#033[39m\u001b[0m\n",
      "\u001b[35m1%|          | 2/270 [01:09<2:35:49, 34.89s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 1.0618, 'learning_rate': 2e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[35m1%|          | 2/270 [01:09<2:35:49, 34.89s/it]\u001b[0m\n",
      "\u001b[34m[2023-10-27 21:41:24,140] [INFO] [axolotl.callbacks.on_step_end:122] [PID:197] [RANK:0] GPU memory usage while training: 4.038GB (+9.194GB cache, +0.721GB misc)#033[39m\u001b[0m\n",
      "\u001b[34m1%|          | 2/270 [01:09<2:34:18, 34.55s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0618, 'learning_rate': 2e-05, 'epoch': 0.02}\u001b[0m\n",
      "\u001b[34m1%|          | 2/270 [01:09<2:34:18, 34.55s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 3/270 [01:43<2:33:22, 34.47s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0393, 'learning_rate': 4e-05, 'epoch': 0.03}\u001b[0m\n",
      "\u001b[34m1%|          | 3/270 [01:43<2:33:22, 34.47s/it]\u001b[0m\n",
      "\u001b[35m1%|          | 3/270 [01:44<2:34:11, 34.65s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 1.0393, 'learning_rate': 4e-05, 'epoch': 0.03}\u001b[0m\n",
      "\u001b[35m1%|          | 3/270 [01:44<2:34:11, 34.65s/it]\u001b[0m\n",
      "\u001b[35m1%|▏         | 4/270 [02:18<2:33:29, 34.62s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 1.0422, 'learning_rate': 6e-05, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[35m1%|▏         | 4/270 [02:18<2:33:29, 34.62s/it]\u001b[0m\n",
      "\u001b[34m1%|▏         | 4/270 [02:18<2:32:59, 34.51s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 1.0422, 'learning_rate': 6e-05, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m1%|▏         | 4/270 [02:18<2:32:59, 34.51s/it]\u001b[0m\n",
      "\u001b[35m2%|▏         | 5/270 [02:53<2:32:31, 34.53s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.9991, 'learning_rate': 8e-05, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[35m2%|▏         | 5/270 [02:53<2:32:31, 34.53s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 5/270 [02:52<2:32:12, 34.46s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9991, 'learning_rate': 8e-05, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m2%|▏         | 5/270 [02:52<2:32:12, 34.46s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 6/270 [03:26<2:31:32, 34.44s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.9678, 'learning_rate': 0.0001, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[34m2%|▏         | 6/270 [03:26<2:31:32, 34.44s/it]\u001b[0m\n",
      "\u001b[35m2%|▏         | 6/270 [03:27<2:31:45, 34.49s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.9678, 'learning_rate': 0.0001, 'epoch': 0.07}\u001b[0m\n",
      "\u001b[35m2%|▏         | 6/270 [03:27<2:31:45, 34.49s/it]\u001b[0m\n",
      "\u001b[35m3%|▎         | 7/270 [04:02<2:31:03, 34.46s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.8547, 'learning_rate': 0.00012, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[35m3%|▎         | 7/270 [04:02<2:31:03, 34.46s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 7/270 [04:01<2:30:54, 34.43s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.8547, 'learning_rate': 0.00012, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m3%|▎         | 7/270 [04:01<2:30:54, 34.43s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 8/270 [04:35<2:30:18, 34.42s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.7092, 'learning_rate': 0.00014, 'epoch': 0.09}\u001b[0m\n",
      "\u001b[34m3%|▎         | 8/270 [04:35<2:30:18, 34.42s/it]\u001b[0m\n",
      "\u001b[35m3%|▎         | 8/270 [04:36<2:30:23, 34.44s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.7092, 'learning_rate': 0.00014, 'epoch': 0.09}\u001b[0m\n",
      "\u001b[35m3%|▎         | 8/270 [04:36<2:30:23, 34.44s/it]\u001b[0m\n",
      "\u001b[35m3%|▎         | 9/270 [05:10<2:29:46, 34.43s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.5071, 'learning_rate': 0.00016, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[35m3%|▎         | 9/270 [05:10<2:29:46, 34.43s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 9/270 [05:10<2:29:42, 34.42s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.5071, 'learning_rate': 0.00016, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m3%|▎         | 9/270 [05:10<2:29:42, 34.42s/it]\u001b[0m\n",
      "\u001b[35m4%|▎         | 10/270 [05:45<2:29:10, 34.42s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.3633, 'learning_rate': 0.00018, 'epoch': 0.11}\u001b[0m\n",
      "\u001b[35m4%|▎         | 10/270 [05:45<2:29:10, 34.42s/it]\u001b[0m\n",
      "\u001b[34m4%|▎         | 10/270 [05:44<2:29:07, 34.41s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.3633, 'learning_rate': 0.00018, 'epoch': 0.11}\u001b[0m\n",
      "\u001b[34m4%|▎         | 10/270 [05:44<2:29:07, 34.41s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 11/270 [06:18<2:28:31, 34.41s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.2919, 'learning_rate': 0.0002, 'epoch': 0.12}\u001b[0m\n",
      "\u001b[34m4%|▍         | 11/270 [06:18<2:28:31, 34.41s/it]\u001b[0m\n",
      "\u001b[35m4%|▍         | 11/270 [06:19<2:28:33, 34.42s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.2919, 'learning_rate': 0.0002, 'epoch': 0.12}\u001b[0m\n",
      "\u001b[35m4%|▍         | 11/270 [06:19<2:28:33, 34.42s/it]\u001b[0m\n",
      "\u001b[35m4%|▍         | 12/270 [06:54<2:27:57, 34.41s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.2539, 'learning_rate': 0.00019923076923076925, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[35m4%|▍         | 12/270 [06:54<2:27:57, 34.41s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 12/270 [06:53<2:27:56, 34.41s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.2539, 'learning_rate': 0.00019923076923076925, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m4%|▍         | 12/270 [06:53<2:27:56, 34.41s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 13/270 [07:27<2:27:21, 34.40s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.209, 'learning_rate': 0.00019846153846153847, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m5%|▍         | 13/270 [07:27<2:27:21, 34.40s/it]\u001b[0m\n",
      "\u001b[35m5%|▍         | 13/270 [07:28<2:27:22, 34.41s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.209, 'learning_rate': 0.00019846153846153847, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[35m5%|▍         | 13/270 [07:28<2:27:22, 34.41s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 14/270 [08:02<2:26:46, 34.40s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1931, 'learning_rate': 0.0001976923076923077, 'epoch': 0.16}\u001b[0m\n",
      "\u001b[34m5%|▌         | 14/270 [08:02<2:26:46, 34.40s/it]\u001b[0m\n",
      "\u001b[35m5%|▌         | 14/270 [08:02<2:26:46, 34.40s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.1931, 'learning_rate': 0.0001976923076923077, 'epoch': 0.16}\u001b[0m\n",
      "\u001b[35m5%|▌         | 14/270 [08:02<2:26:46, 34.40s/it]\u001b[0m\n",
      "\u001b[35m6%|▌         | 15/270 [08:37<2:26:11, 34.40s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.1538, 'learning_rate': 0.00019692307692307696, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[35m6%|▌         | 15/270 [08:37<2:26:11, 34.40s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 15/270 [08:36<2:26:10, 34.40s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1538, 'learning_rate': 0.00019692307692307696, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m6%|▌         | 15/270 [08:36<2:26:10, 34.40s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 16/270 [09:10<2:25:36, 34.39s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1313, 'learning_rate': 0.00019615384615384615, 'epoch': 0.18}\u001b[0m\n",
      "\u001b[34m6%|▌         | 16/270 [09:10<2:25:36, 34.39s/it]\u001b[0m\n",
      "\u001b[35m6%|▌         | 16/270 [09:11<2:25:36, 34.40s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.1313, 'learning_rate': 0.00019615384615384615, 'epoch': 0.18}\u001b[0m\n",
      "\u001b[35m6%|▌         | 16/270 [09:11<2:25:36, 34.40s/it]\u001b[0m\n",
      "\u001b[35m6%|▋         | 17/270 [09:46<2:25:01, 34.39s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.1341, 'learning_rate': 0.0001953846153846154, 'epoch': 0.19}\u001b[0m\n",
      "\u001b[35m6%|▋         | 17/270 [09:46<2:25:01, 34.39s/it]\u001b[0m\n",
      "\u001b[34m6%|▋         | 17/270 [09:45<2:25:01, 34.39s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1341, 'learning_rate': 0.0001953846153846154, 'epoch': 0.19}\u001b[0m\n",
      "\u001b[34m6%|▋         | 17/270 [09:45<2:25:01, 34.39s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 18/270 [10:19<2:24:27, 34.40s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1226, 'learning_rate': 0.00019461538461538463, 'epoch': 0.2}\u001b[0m\n",
      "\u001b[34m7%|▋         | 18/270 [10:19<2:24:27, 34.40s/it]\u001b[0m\n",
      "\u001b[35m7%|▋         | 18/270 [10:20<2:24:27, 34.40s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.1226, 'learning_rate': 0.00019461538461538463, 'epoch': 0.2}\u001b[0m\n",
      "\u001b[35m7%|▋         | 18/270 [10:20<2:24:27, 34.40s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 19/270 [10:54<2:23:54, 34.40s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1135, 'learning_rate': 0.00019384615384615385, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[34m7%|▋         | 19/270 [10:54<2:23:54, 34.40s/it]\u001b[0m\n",
      "\u001b[35m7%|▋         | 19/270 [10:54<2:23:54, 34.40s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.1135, 'learning_rate': 0.00019384615384615385, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[35m7%|▋         | 19/270 [10:54<2:23:54, 34.40s/it]\u001b[0m\n",
      "\u001b[35m7%|▋         | 20/270 [11:29<2:23:19, 34.40s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.1118, 'learning_rate': 0.0001930769230769231, 'epoch': 0.22}\u001b[0m\n",
      "\u001b[35m7%|▋         | 20/270 [11:29<2:23:19, 34.40s/it]\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.10/site-packages/transformers/trainer_pt_utils.py:296: FutureWarning: SequentialDistributedSampler is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[35m[2023-10-27 21:51:43,443] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:196] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 21:51:43,451] [INFO] [axolotl.utils.dataloader.generate_batches:181] [PID:196] [RANK:0] generating packed batches#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 21:51:43,451] [INFO] [axolotl.utils.dataloader.generate_batches:187] [PID:196] [RANK:0] b68612dda09b7da3bd0173ed59f621c8c8f01237e9f3dcbc6d7b7c116d4e5e0c#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 21:51:43,453] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:196] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 21:51:45,948] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:196] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 21:51:45,948] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:196] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[35m0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 20/270 [11:28<2:23:19, 34.40s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1118, 'learning_rate': 0.0001930769230769231, 'epoch': 0.22}\u001b[0m\n",
      "\u001b[34m7%|▋         | 20/270 [11:28<2:23:19, 34.40s/it]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/transformers/trainer_pt_utils.py:296: FutureWarning: SequentialDistributedSampler is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m[2023-10-27 21:51:43,443] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:197] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 21:51:43,451] [INFO] [axolotl.utils.dataloader.generate_batches:181] [PID:197] [RANK:0] generating packed batches#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 21:51:43,451] [INFO] [axolotl.utils.dataloader.generate_batches:187] [PID:197] [RANK:0] a3efec085e0760812a351b73f7cdffc9a801ed4832469f9f56e2c45c91279c6e#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 21:51:43,453] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:197] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 21:51:45,949] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:197] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 21:51:45,949] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:197] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[34m0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[2023-10-27 21:51:48,512] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:196] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[35m100%|██████████| 2/2 [00:02<00:00,  1.28s/it]\u001b[0m\n",
      "\u001b[34m[2023-10-27 21:51:48,511] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:197] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[34m100%|██████████| 2/2 [00:02<00:00,  1.28s/it]\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.09194658696651459, 'eval_runtime': 5.1671, 'eval_samples_per_second': 152.115, 'eval_steps_per_second': 38.126, 'epoch': 0.22}\u001b[0m\n",
      "\u001b[34m7%|▋         | 20/270 [11:33<2:23:19, 34.40s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 2/2 [00:02<00:00,  1.28s/it]\u001b[0m\n",
      "\u001b[35m{'eval_loss': 0.09194658696651459, 'eval_runtime': 5.1673, 'eval_samples_per_second': 152.111, 'eval_steps_per_second': 38.125, 'epoch': 0.22}\u001b[0m\n",
      "\u001b[35m7%|▋         | 20/270 [11:34<2:23:19, 34.40s/it]\u001b[0m\n",
      "\u001b[35m100%|██████████| 2/2 [00:02<00:00,  1.28s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 21/270 [12:08<2:29:12, 35.95s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.111, 'learning_rate': 0.00019230769230769233, 'epoch': 0.23}\u001b[0m\n",
      "\u001b[34m8%|▊         | 21/270 [12:08<2:29:12, 35.95s/it]\u001b[0m\n",
      "\u001b[35m8%|▊         | 21/270 [12:08<2:29:12, 35.95s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.111, 'learning_rate': 0.00019230769230769233, 'epoch': 0.23}\u001b[0m\n",
      "\u001b[35m8%|▊         | 21/270 [12:08<2:29:12, 35.95s/it]\u001b[0m\n",
      "\u001b[35m8%|▊         | 22/270 [12:43<2:26:40, 35.49s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.1116, 'learning_rate': 0.00019153846153846155, 'epoch': 0.24}\u001b[0m\n",
      "\u001b[35m8%|▊         | 22/270 [12:43<2:26:40, 35.49s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 22/270 [12:42<2:26:40, 35.49s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1116, 'learning_rate': 0.00019153846153846155, 'epoch': 0.24}\u001b[0m\n",
      "\u001b[34m8%|▊         | 22/270 [12:42<2:26:40, 35.49s/it]\u001b[0m\n",
      "\u001b[34m9%|▊         | 23/270 [13:16<2:24:45, 35.16s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.1084, 'learning_rate': 0.0001907692307692308, 'epoch': 0.26}\u001b[0m\n",
      "\u001b[34m9%|▊         | 23/270 [13:16<2:24:45, 35.16s/it]\u001b[0m\n",
      "\u001b[35m9%|▊         | 23/270 [13:17<2:24:45, 35.16s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.1084, 'learning_rate': 0.0001907692307692308, 'epoch': 0.26}\u001b[0m\n",
      "\u001b[35m9%|▊         | 23/270 [13:17<2:24:45, 35.16s/it]\u001b[0m\n",
      "\u001b[35m9%|▉         | 24/270 [13:52<2:23:14, 34.94s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0973, 'learning_rate': 0.00019, 'epoch': 0.27}\u001b[0m\n",
      "\u001b[35m9%|▉         | 24/270 [13:52<2:23:14, 34.94s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 24/270 [13:51<2:23:14, 34.94s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0973, 'learning_rate': 0.00019, 'epoch': 0.27}\u001b[0m\n",
      "\u001b[34m9%|▉         | 24/270 [13:51<2:23:14, 34.94s/it]\u001b[0m\n",
      "\u001b[35m9%|▉         | 25/270 [14:26<2:21:59, 34.77s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0952, 'learning_rate': 0.00018923076923076923, 'epoch': 0.28}\u001b[0m\n",
      "\u001b[35m9%|▉         | 25/270 [14:26<2:21:59, 34.77s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 25/270 [14:25<2:21:59, 34.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0952, 'learning_rate': 0.00018923076923076923, 'epoch': 0.28}\u001b[0m\n",
      "\u001b[34m9%|▉         | 25/270 [14:25<2:21:59, 34.77s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 26/270 [15:00<2:20:56, 34.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0925, 'learning_rate': 0.00018846153846153847, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[34m10%|▉         | 26/270 [15:00<2:20:56, 34.66s/it]\u001b[0m\n",
      "\u001b[35m10%|▉         | 26/270 [15:00<2:20:56, 34.66s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0925, 'learning_rate': 0.00018846153846153847, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[35m10%|▉         | 26/270 [15:00<2:20:56, 34.66s/it]\u001b[0m\n",
      "\u001b[35m10%|█         | 27/270 [15:35<2:20:17, 34.64s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0937, 'learning_rate': 0.0001876923076923077, 'epoch': 0.3}\u001b[0m\n",
      "\u001b[35m10%|█         | 27/270 [15:35<2:20:17, 34.64s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 27/270 [15:34<2:20:17, 34.64s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0937, 'learning_rate': 0.0001876923076923077, 'epoch': 0.3}\u001b[0m\n",
      "\u001b[34m10%|█         | 27/270 [15:34<2:20:17, 34.64s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 28/270 [16:09<2:19:25, 34.57s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0854, 'learning_rate': 0.00018692307692307693, 'epoch': 0.31}\u001b[0m\n",
      "\u001b[34m10%|█         | 28/270 [16:09<2:19:25, 34.57s/it]\u001b[0m\n",
      "\u001b[35m10%|█         | 28/270 [16:09<2:19:25, 34.57s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0854, 'learning_rate': 0.00018692307692307693, 'epoch': 0.31}\u001b[0m\n",
      "\u001b[35m10%|█         | 28/270 [16:09<2:19:25, 34.57s/it]\u001b[0m\n",
      "\u001b[35m11%|█         | 29/270 [16:44<2:18:38, 34.52s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0872, 'learning_rate': 0.00018615384615384617, 'epoch': 0.32}\u001b[0m\n",
      "\u001b[35m11%|█         | 29/270 [16:44<2:18:38, 34.52s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 29/270 [16:43<2:18:38, 34.52s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0872, 'learning_rate': 0.00018615384615384617, 'epoch': 0.32}\u001b[0m\n",
      "\u001b[34m11%|█         | 29/270 [16:43<2:18:38, 34.52s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 30/270 [17:17<2:17:55, 34.48s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0873, 'learning_rate': 0.0001853846153846154, 'epoch': 0.33}\u001b[0m\n",
      "\u001b[34m11%|█         | 30/270 [17:17<2:17:55, 34.48s/it]\u001b[0m\n",
      "\u001b[35m11%|█         | 30/270 [17:18<2:17:55, 34.48s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0873, 'learning_rate': 0.0001853846153846154, 'epoch': 0.33}\u001b[0m\n",
      "\u001b[35m11%|█         | 30/270 [17:18<2:17:55, 34.48s/it]\u001b[0m\n",
      "\u001b[34m11%|█▏        | 31/270 [17:52<2:17:14, 34.45s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.087, 'learning_rate': 0.00018461538461538463, 'epoch': 0.34}\u001b[0m\n",
      "\u001b[34m11%|█▏        | 31/270 [17:52<2:17:14, 34.45s/it]\u001b[0m\n",
      "\u001b[35m11%|█▏        | 31/270 [17:53<2:17:14, 34.45s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.087, 'learning_rate': 0.00018461538461538463, 'epoch': 0.34}\u001b[0m\n",
      "\u001b[35m11%|█▏        | 31/270 [17:53<2:17:14, 34.45s/it]\u001b[0m\n",
      "\u001b[35m12%|█▏        | 32/270 [18:27<2:16:36, 34.44s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0915, 'learning_rate': 0.00018384615384615385, 'epoch': 0.36}\u001b[0m\n",
      "\u001b[35m12%|█▏        | 32/270 [18:27<2:16:36, 34.44s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 32/270 [18:26<2:16:35, 34.44s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0915, 'learning_rate': 0.00018384615384615385, 'epoch': 0.36}\u001b[0m\n",
      "\u001b[34m12%|█▏        | 32/270 [18:26<2:16:35, 34.44s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 33/270 [19:00<2:15:58, 34.42s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0814, 'learning_rate': 0.0001830769230769231, 'epoch': 0.37}\u001b[0m\n",
      "\u001b[34m12%|█▏        | 33/270 [19:01<2:15:58, 34.42s/it]\u001b[0m\n",
      "\u001b[35m12%|█▏        | 33/270 [19:01<2:15:58, 34.42s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0814, 'learning_rate': 0.0001830769230769231, 'epoch': 0.37}\u001b[0m\n",
      "\u001b[35m12%|█▏        | 33/270 [19:01<2:15:58, 34.42s/it]\u001b[0m\n",
      "\u001b[35m13%|█▎        | 34/270 [19:36<2:15:22, 34.42s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.076, 'learning_rate': 0.0001823076923076923, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[35m13%|█▎        | 34/270 [19:36<2:15:22, 34.42s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 34/270 [19:35<2:15:22, 34.42s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.076, 'learning_rate': 0.0001823076923076923, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[34m13%|█▎        | 34/270 [19:35<2:15:22, 34.42s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 35/270 [20:09<2:14:46, 34.41s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0772, 'learning_rate': 0.00018153846153846155, 'epoch': 0.39}\u001b[0m\n",
      "\u001b[34m13%|█▎        | 35/270 [20:09<2:14:46, 34.41s/it]\u001b[0m\n",
      "\u001b[35m13%|█▎        | 35/270 [20:10<2:14:46, 34.41s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0772, 'learning_rate': 0.00018153846153846155, 'epoch': 0.39}\u001b[0m\n",
      "\u001b[35m13%|█▎        | 35/270 [20:10<2:14:46, 34.41s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 36/270 [20:44<2:14:11, 34.41s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0653, 'learning_rate': 0.00018076923076923077, 'epoch': 0.4}\u001b[0m\n",
      "\u001b[34m13%|█▎        | 36/270 [20:44<2:14:11, 34.41s/it]\u001b[0m\n",
      "\u001b[35m13%|█▎        | 36/270 [20:45<2:14:11, 34.41s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0653, 'learning_rate': 0.00018076923076923077, 'epoch': 0.4}\u001b[0m\n",
      "\u001b[35m13%|█▎        | 36/270 [20:45<2:14:11, 34.41s/it]\u001b[0m\n",
      "\u001b[35m14%|█▎        | 37/270 [21:19<2:13:36, 34.40s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0781, 'learning_rate': 0.00018, 'epoch': 0.41}\u001b[0m\n",
      "\u001b[35m14%|█▎        | 37/270 [21:19<2:13:36, 34.40s/it]\u001b[0m\n",
      "\u001b[34m14%|█▎        | 37/270 [21:18<2:13:36, 34.40s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0781, 'learning_rate': 0.00018, 'epoch': 0.41}\u001b[0m\n",
      "\u001b[34m14%|█▎        | 37/270 [21:18<2:13:36, 34.40s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 38/270 [21:52<2:13:01, 34.40s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0674, 'learning_rate': 0.00017923076923076925, 'epoch': 0.42}\u001b[0m\n",
      "\u001b[34m14%|█▍        | 38/270 [21:52<2:13:01, 34.40s/it]\u001b[0m\n",
      "\u001b[35m14%|█▍        | 38/270 [21:53<2:13:01, 34.40s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0674, 'learning_rate': 0.00017923076923076925, 'epoch': 0.42}\u001b[0m\n",
      "\u001b[35m14%|█▍        | 38/270 [21:53<2:13:01, 34.40s/it]\u001b[0m\n",
      "\u001b[35m14%|█▍        | 39/270 [22:28<2:12:26, 34.40s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0605, 'learning_rate': 0.00017846153846153847, 'epoch': 0.43}\u001b[0m\n",
      "\u001b[35m14%|█▍        | 39/270 [22:28<2:12:26, 34.40s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 39/270 [22:27<2:12:26, 34.40s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0605, 'learning_rate': 0.00017846153846153847, 'epoch': 0.43}\u001b[0m\n",
      "\u001b[34m14%|█▍        | 39/270 [22:27<2:12:26, 34.40s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 40/270 [23:01<2:11:52, 34.40s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0636, 'learning_rate': 0.0001776923076923077, 'epoch': 0.44}\u001b[0m\n",
      "\u001b[34m15%|█▍        | 40/270 [23:01<2:11:52, 34.40s/it]\u001b[0m\n",
      "\u001b[34m[2023-10-27 22:03:16,779] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:197] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 22:03:16,787] [INFO] [axolotl.utils.dataloader.generate_batches:181] [PID:197] [RANK:0] generating packed batches#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 22:03:16,787] [INFO] [axolotl.utils.dataloader.generate_batches:187] [PID:197] [RANK:0] a3efec085e0760812a351b73f7cdffc9a801ed4832469f9f56e2c45c91279c6e#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 22:03:16,789] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:197] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[35m15%|█▍        | 40/270 [23:02<2:11:51, 34.40s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0636, 'learning_rate': 0.0001776923076923077, 'epoch': 0.44}\u001b[0m\n",
      "\u001b[35m15%|█▍        | 40/270 [23:02<2:11:51, 34.40s/it]\u001b[0m\n",
      "\u001b[35m[2023-10-27 22:03:16,778] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:196] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 22:03:16,786] [INFO] [axolotl.utils.dataloader.generate_batches:181] [PID:196] [RANK:0] generating packed batches#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 22:03:16,786] [INFO] [axolotl.utils.dataloader.generate_batches:187] [PID:196] [RANK:0] b68612dda09b7da3bd0173ed59f621c8c8f01237e9f3dcbc6d7b7c116d4e5e0c#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 22:03:16,788] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:196] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 22:03:19,285] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:197] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 22:03:19,285] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:197] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[34m0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[2023-10-27 22:03:19,283] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:196] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 22:03:19,283] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:196] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[35m0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[2023-10-27 22:03:21,847] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:197] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[34m100%|██████████| 2/2 [00:02<00:00,  1.28s/it]\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.05594940111041069, 'eval_runtime': 5.1673, 'eval_samples_per_second': 152.111, 'eval_steps_per_second': 38.124, 'epoch': 0.44}\u001b[0m\n",
      "\u001b[34m15%|█▍        | 40/270 [23:06<2:11:52, 34.40s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 2/2 [00:02<00:00,  1.28s/it]\u001b[0m\n",
      "\u001b[35m[2023-10-27 22:03:21,848] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:196] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[35m100%|██████████| 2/2 [00:02<00:00,  1.28s/it]\u001b[0m\n",
      "\u001b[35m{'eval_loss': 0.05594940111041069, 'eval_runtime': 5.1679, 'eval_samples_per_second': 152.094, 'eval_steps_per_second': 38.12, 'epoch': 0.44}\u001b[0m\n",
      "\u001b[35m15%|█▍        | 40/270 [23:07<2:11:51, 34.40s/it]\u001b[0m\n",
      "\u001b[35m100%|██████████| 2/2 [00:02<00:00,  1.28s/it]\u001b[0m\n",
      "\u001b[35m15%|█▌        | 41/270 [23:42<2:17:13, 35.96s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0593, 'learning_rate': 0.00017692307692307693, 'epoch': 0.46}\u001b[0m\n",
      "\u001b[35m15%|█▌        | 41/270 [23:42<2:17:13, 35.96s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 41/270 [23:41<2:17:13, 35.96s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0593, 'learning_rate': 0.00017692307692307693, 'epoch': 0.46}\u001b[0m\n",
      "\u001b[34m15%|█▌        | 41/270 [23:41<2:17:13, 35.96s/it]\u001b[0m\n",
      "\u001b[35m16%|█▌        | 42/270 [24:16<2:14:51, 35.49s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0612, 'learning_rate': 0.00017615384615384615, 'epoch': 0.47}\u001b[0m\n",
      "\u001b[35m16%|█▌        | 42/270 [24:16<2:14:51, 35.49s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 42/270 [24:15<2:14:51, 35.49s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0612, 'learning_rate': 0.00017615384615384615, 'epoch': 0.47}\u001b[0m\n",
      "\u001b[34m16%|█▌        | 42/270 [24:15<2:14:51, 35.49s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 43/270 [24:50<2:13:01, 35.16s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0653, 'learning_rate': 0.0001753846153846154, 'epoch': 0.48}\u001b[0m\n",
      "\u001b[34m16%|█▌        | 43/270 [24:50<2:13:01, 35.16s/it]\u001b[0m\n",
      "\u001b[35m16%|█▌        | 43/270 [24:50<2:13:01, 35.16s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0653, 'learning_rate': 0.0001753846153846154, 'epoch': 0.48}\u001b[0m\n",
      "\u001b[35m16%|█▌        | 43/270 [24:50<2:13:01, 35.16s/it]\u001b[0m\n",
      "\u001b[35m16%|█▋        | 44/270 [25:25<2:11:34, 34.93s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0577, 'learning_rate': 0.00017461538461538463, 'epoch': 0.49}\u001b[0m\n",
      "\u001b[35m16%|█▋        | 44/270 [25:25<2:11:34, 34.93s/it]\u001b[0m\n",
      "\u001b[34m16%|█▋        | 44/270 [25:24<2:11:34, 34.93s/it]\u001b[0m\n",
      "\u001b[34m16%|█▋        | 44/270 [25:24<2:11:34, 34.93s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0577, 'learning_rate': 0.00017461538461538463, 'epoch': 0.49}\u001b[0m\n",
      "\u001b[34m17%|█▋        | 45/270 [25:58<2:10:24, 34.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0629, 'learning_rate': 0.00017384615384615385, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34m17%|█▋        | 45/270 [25:58<2:10:24, 34.78s/it]\u001b[0m\n",
      "\u001b[35m17%|█▋        | 45/270 [25:59<2:10:24, 34.77s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0629, 'learning_rate': 0.00017384615384615385, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[35m17%|█▋        | 45/270 [25:59<2:10:24, 34.77s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 46/270 [26:33<2:09:24, 34.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0511, 'learning_rate': 0.0001730769230769231, 'epoch': 0.51}\u001b[0m\n",
      "\u001b[34m17%|█▋        | 46/270 [26:33<2:09:24, 34.66s/it]\u001b[0m\n",
      "\u001b[35m17%|█▋        | 46/270 [26:34<2:09:24, 34.67s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0511, 'learning_rate': 0.0001730769230769231, 'epoch': 0.51}\u001b[0m\n",
      "\u001b[35m17%|█▋        | 46/270 [26:34<2:09:24, 34.67s/it]\u001b[0m\n",
      "\u001b[35m17%|█▋        | 47/270 [27:08<2:08:32, 34.59s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0416, 'learning_rate': 0.00017230769230769234, 'epoch': 0.52}\u001b[0m\n",
      "\u001b[35m17%|█▋        | 47/270 [27:08<2:08:32, 34.59s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 47/270 [27:07<2:08:32, 34.59s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0416, 'learning_rate': 0.00017230769230769234, 'epoch': 0.52}\u001b[0m\n",
      "\u001b[34m17%|█▋        | 47/270 [27:07<2:08:32, 34.59s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 48/270 [27:42<2:07:47, 34.54s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0482, 'learning_rate': 0.00017153846153846153, 'epoch': 0.53}\u001b[0m\n",
      "\u001b[34m18%|█▊        | 48/270 [27:42<2:07:47, 34.54s/it]\u001b[0m\n",
      "\u001b[35m18%|█▊        | 48/270 [27:43<2:07:46, 34.54s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0482, 'learning_rate': 0.00017153846153846153, 'epoch': 0.53}\u001b[0m\n",
      "\u001b[35m18%|█▊        | 48/270 [27:43<2:07:46, 34.54s/it]\u001b[0m\n",
      "\u001b[35m18%|█▊        | 49/270 [28:17<2:07:02, 34.49s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0521, 'learning_rate': 0.00017076923076923077, 'epoch': 0.54}\u001b[0m\n",
      "\u001b[35m18%|█▊        | 49/270 [28:17<2:07:02, 34.49s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 49/270 [28:16<2:07:03, 34.49s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0521, 'learning_rate': 0.00017076923076923077, 'epoch': 0.54}\u001b[0m\n",
      "\u001b[34m18%|█▊        | 49/270 [28:16<2:07:03, 34.49s/it]\u001b[0m\n",
      "\u001b[34m19%|█▊        | 50/270 [28:50<2:06:22, 34.47s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0439, 'learning_rate': 0.00017, 'epoch': 0.56}\u001b[0m\n",
      "\u001b[34m19%|█▊        | 50/270 [28:51<2:06:22, 34.47s/it]\u001b[0m\n",
      "\u001b[35m19%|█▊        | 50/270 [28:51<2:06:22, 34.47s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0439, 'learning_rate': 0.00017, 'epoch': 0.56}\u001b[0m\n",
      "\u001b[35m19%|█▊        | 50/270 [28:51<2:06:22, 34.47s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 51/270 [29:25<2:05:43, 34.44s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0481, 'learning_rate': 0.00016923076923076923, 'epoch': 0.57}\u001b[0m\n",
      "\u001b[34m19%|█▉        | 51/270 [29:25<2:05:43, 34.44s/it]\u001b[0m\n",
      "\u001b[35m19%|█▉        | 51/270 [29:26<2:05:43, 34.44s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0481, 'learning_rate': 0.00016923076923076923, 'epoch': 0.57}\u001b[0m\n",
      "\u001b[35m19%|█▉        | 51/270 [29:26<2:05:43, 34.44s/it]\u001b[0m\n",
      "\u001b[35m19%|█▉        | 52/270 [30:00<2:05:06, 34.43s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0485, 'learning_rate': 0.00016846153846153847, 'epoch': 0.58}\u001b[0m\n",
      "\u001b[35m19%|█▉        | 52/270 [30:00<2:05:06, 34.43s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 52/270 [29:59<2:05:06, 34.43s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0485, 'learning_rate': 0.00016846153846153847, 'epoch': 0.58}\u001b[0m\n",
      "\u001b[34m19%|█▉        | 52/270 [29:59<2:05:06, 34.43s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 53/270 [30:34<2:04:30, 34.42s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0517, 'learning_rate': 0.00016769230769230772, 'epoch': 0.59}\u001b[0m\n",
      "\u001b[34m20%|█▉        | 53/270 [30:34<2:04:30, 34.42s/it]\u001b[0m\n",
      "\u001b[35m20%|█▉        | 53/270 [30:35<2:04:30, 34.42s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0517, 'learning_rate': 0.00016769230769230772, 'epoch': 0.59}\u001b[0m\n",
      "\u001b[35m20%|█▉        | 53/270 [30:35<2:04:30, 34.42s/it]\u001b[0m\n",
      "\u001b[35m20%|██        | 54/270 [31:09<2:03:54, 34.42s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0383, 'learning_rate': 0.00016692307692307693, 'epoch': 0.6}\u001b[0m\n",
      "\u001b[35m20%|██        | 54/270 [31:09<2:03:54, 34.42s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 54/270 [31:08<2:03:54, 34.42s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0383, 'learning_rate': 0.00016692307692307693, 'epoch': 0.6}\u001b[0m\n",
      "\u001b[34m20%|██        | 54/270 [31:08<2:03:54, 34.42s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 55/270 [31:43<2:03:18, 34.41s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0479, 'learning_rate': 0.00016615384615384617, 'epoch': 0.61}\u001b[0m\n",
      "\u001b[34m20%|██        | 55/270 [31:43<2:03:18, 34.41s/it]\u001b[0m\n",
      "\u001b[35m20%|██        | 55/270 [31:43<2:03:18, 34.41s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0479, 'learning_rate': 0.00016615384615384617, 'epoch': 0.61}\u001b[0m\n",
      "\u001b[35m20%|██        | 55/270 [31:43<2:03:18, 34.41s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 56/270 [32:17<2:02:43, 34.41s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0484, 'learning_rate': 0.0001653846153846154, 'epoch': 0.62}\u001b[0m\n",
      "\u001b[34m21%|██        | 56/270 [32:17<2:02:43, 34.41s/it]\u001b[0m\n",
      "\u001b[35m21%|██        | 56/270 [32:18<2:02:43, 34.41s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0484, 'learning_rate': 0.0001653846153846154, 'epoch': 0.62}\u001b[0m\n",
      "\u001b[35m21%|██        | 56/270 [32:18<2:02:43, 34.41s/it]\u001b[0m\n",
      "\u001b[35m21%|██        | 57/270 [32:52<2:02:08, 34.40s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0437, 'learning_rate': 0.0001646153846153846, 'epoch': 0.63}\u001b[0m\n",
      "\u001b[35m21%|██        | 57/270 [32:52<2:02:08, 34.40s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 57/270 [32:51<2:02:08, 34.40s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0437, 'learning_rate': 0.0001646153846153846, 'epoch': 0.63}\u001b[0m\n",
      "\u001b[34m21%|██        | 57/270 [32:51<2:02:08, 34.40s/it]\u001b[0m\n",
      "\u001b[34m21%|██▏       | 58/270 [33:26<2:01:34, 34.41s/it]\u001b[0m\n",
      "\u001b[34m21%|██▏       | 58/270 [33:26<2:01:34, 34.41s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0441, 'learning_rate': 0.00016384615384615385, 'epoch': 0.64}\u001b[0m\n",
      "\u001b[35m21%|██▏       | 58/270 [33:27<2:01:34, 34.41s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0441, 'learning_rate': 0.00016384615384615385, 'epoch': 0.64}\u001b[0m\n",
      "\u001b[35m21%|██▏       | 58/270 [33:27<2:01:34, 34.41s/it]\u001b[0m\n",
      "\u001b[35m22%|██▏       | 59/270 [34:01<2:00:59, 34.40s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0465, 'learning_rate': 0.0001630769230769231, 'epoch': 0.66}\u001b[0m\n",
      "\u001b[35m22%|██▏       | 59/270 [34:01<2:00:59, 34.40s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 59/270 [34:00<2:00:59, 34.40s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0465, 'learning_rate': 0.0001630769230769231, 'epoch': 0.66}\u001b[0m\n",
      "\u001b[34m22%|██▏       | 59/270 [34:00<2:00:59, 34.40s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 60/270 [34:35<2:00:24, 34.40s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0498, 'learning_rate': 0.0001623076923076923, 'epoch': 0.67}\u001b[0m\n",
      "\u001b[34m22%|██▏       | 60/270 [34:35<2:00:24, 34.40s/it]\u001b[0m\n",
      "\u001b[34m[2023-10-27 22:14:50,006] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:197] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 22:14:50,016] [INFO] [axolotl.utils.dataloader.generate_batches:181] [PID:197] [RANK:0] generating packed batches#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 22:14:50,016] [INFO] [axolotl.utils.dataloader.generate_batches:187] [PID:197] [RANK:0] a3efec085e0760812a351b73f7cdffc9a801ed4832469f9f56e2c45c91279c6e#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 22:14:50,018] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:197] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[35m22%|██▏       | 60/270 [34:35<2:00:25, 34.41s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0498, 'learning_rate': 0.0001623076923076923, 'epoch': 0.67}\u001b[0m\n",
      "\u001b[35m22%|██▏       | 60/270 [34:35<2:00:25, 34.41s/it]\u001b[0m\n",
      "\u001b[35m[2023-10-27 22:14:50,005] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:196] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 22:14:50,012] [INFO] [axolotl.utils.dataloader.generate_batches:181] [PID:196] [RANK:0] generating packed batches#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 22:14:50,013] [INFO] [axolotl.utils.dataloader.generate_batches:187] [PID:196] [RANK:0] b68612dda09b7da3bd0173ed59f621c8c8f01237e9f3dcbc6d7b7c116d4e5e0c#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 22:14:50,015] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:196] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 22:14:52,510] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:196] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 22:14:52,510] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:196] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[35m0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[2023-10-27 22:14:52,519] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:197] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 22:14:52,519] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:197] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[34m0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[2023-10-27 22:14:55,081] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:197] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[34m100%|██████████| 2/2 [00:02<00:00,  1.28s/it]\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.037978317588567734, 'eval_runtime': 5.1736, 'eval_samples_per_second': 151.926, 'eval_steps_per_second': 38.078, 'epoch': 0.67}\u001b[0m\n",
      "\u001b[34m22%|██▏       | 60/270 [34:40<2:00:24, 34.40s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 2/2 [00:02<00:00,  1.28s/it]\u001b[0m\n",
      "\u001b[35m[2023-10-27 22:14:55,081] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:196] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[35m100%|██████████| 2/2 [00:02<00:00,  1.29s/it]\u001b[0m\n",
      "\u001b[35m{'eval_loss': 0.037978317588567734, 'eval_runtime': 5.1745, 'eval_samples_per_second': 151.899, 'eval_steps_per_second': 38.071, 'epoch': 0.67}\u001b[0m\n",
      "\u001b[35m22%|██▏       | 60/270 [34:41<2:00:25, 34.41s/it]\u001b[0m\n",
      "\u001b[35m100%|██████████| 2/2 [00:02<00:00,  1.29s/it]\u001b[0m\n",
      "\u001b[35m23%|██▎       | 61/270 [35:15<2:05:15, 35.96s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0418, 'learning_rate': 0.00016153846153846155, 'epoch': 0.68}\u001b[0m\n",
      "\u001b[35m23%|██▎       | 61/270 [35:15<2:05:15, 35.96s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 61/270 [35:14<2:05:15, 35.96s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0418, 'learning_rate': 0.00016153846153846155, 'epoch': 0.68}\u001b[0m\n",
      "\u001b[34m23%|██▎       | 61/270 [35:14<2:05:15, 35.96s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 62/270 [35:48<2:03:01, 35.49s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0414, 'learning_rate': 0.00016076923076923077, 'epoch': 0.69}\u001b[0m\n",
      "\u001b[34m23%|██▎       | 62/270 [35:48<2:03:01, 35.49s/it]\u001b[0m\n",
      "\u001b[35m23%|██▎       | 62/270 [35:49<2:03:01, 35.49s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0414, 'learning_rate': 0.00016076923076923077, 'epoch': 0.69}\u001b[0m\n",
      "\u001b[35m23%|██▎       | 62/270 [35:49<2:03:01, 35.49s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 63/270 [36:23<2:01:18, 35.16s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0411, 'learning_rate': 0.00016, 'epoch': 0.7}\u001b[0m\n",
      "\u001b[34m23%|██▎       | 63/270 [36:23<2:01:18, 35.16s/it]\u001b[0m\n",
      "\u001b[35m23%|██▎       | 63/270 [36:24<2:01:17, 35.16s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0411, 'learning_rate': 0.00016, 'epoch': 0.7}\u001b[0m\n",
      "\u001b[35m23%|██▎       | 63/270 [36:24<2:01:17, 35.16s/it]\u001b[0m\n",
      "\u001b[35m24%|██▎       | 64/270 [36:58<1:59:56, 34.93s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.04, 'learning_rate': 0.00015923076923076923, 'epoch': 0.71}\u001b[0m\n",
      "\u001b[35m24%|██▎       | 64/270 [36:58<1:59:56, 34.93s/it]\u001b[0m\n",
      "\u001b[34m24%|██▎       | 64/270 [36:57<1:59:56, 34.93s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.04, 'learning_rate': 0.00015923076923076923, 'epoch': 0.71}\u001b[0m\n",
      "\u001b[34m24%|██▎       | 64/270 [36:57<1:59:56, 34.93s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 65/270 [37:32<1:58:49, 34.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0372, 'learning_rate': 0.00015846153846153847, 'epoch': 0.72}\u001b[0m\n",
      "\u001b[34m24%|██▍       | 65/270 [37:32<1:58:49, 34.78s/it]\u001b[0m\n",
      "\u001b[35m24%|██▍       | 65/270 [37:33<1:58:49, 34.78s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0372, 'learning_rate': 0.00015846153846153847, 'epoch': 0.72}\u001b[0m\n",
      "\u001b[35m24%|██▍       | 65/270 [37:33<1:58:49, 34.78s/it]\u001b[0m\n",
      "\u001b[35m24%|██▍       | 66/270 [38:07<1:57:51, 34.66s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0401, 'learning_rate': 0.0001576923076923077, 'epoch': 0.73}\u001b[0m\n",
      "\u001b[35m24%|██▍       | 66/270 [38:07<1:57:51, 34.66s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 66/270 [38:06<1:57:51, 34.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0401, 'learning_rate': 0.0001576923076923077, 'epoch': 0.73}\u001b[0m\n",
      "\u001b[34m24%|██▍       | 66/270 [38:06<1:57:51, 34.66s/it]\u001b[0m\n",
      "\u001b[35m25%|██▍       | 67/270 [38:41<1:57:00, 34.58s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0361, 'learning_rate': 0.00015692307692307693, 'epoch': 0.74}\u001b[0m\n",
      "\u001b[35m25%|██▍       | 67/270 [38:41<1:57:00, 34.58s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 67/270 [38:40<1:57:00, 34.58s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0361, 'learning_rate': 0.00015692307692307693, 'epoch': 0.74}\u001b[0m\n",
      "\u001b[34m25%|██▍       | 67/270 [38:40<1:57:00, 34.58s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 68/270 [39:15<1:56:14, 34.52s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0424, 'learning_rate': 0.00015615384615384615, 'epoch': 0.76}\u001b[0m\n",
      "\u001b[34m25%|██▌       | 68/270 [39:15<1:56:14, 34.52s/it]\u001b[0m\n",
      "\u001b[35m25%|██▌       | 68/270 [39:16<1:56:14, 34.52s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0424, 'learning_rate': 0.00015615384615384615, 'epoch': 0.76}\u001b[0m\n",
      "\u001b[35m25%|██▌       | 68/270 [39:16<1:56:14, 34.52s/it]\u001b[0m\n",
      "\u001b[35m26%|██▌       | 69/270 [39:50<1:55:31, 34.49s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0378, 'learning_rate': 0.0001553846153846154, 'epoch': 0.77}\u001b[0m\n",
      "\u001b[35m26%|██▌       | 69/270 [39:50<1:55:31, 34.49s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 69/270 [39:49<1:55:31, 34.49s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0378, 'learning_rate': 0.0001553846153846154, 'epoch': 0.77}\u001b[0m\n",
      "\u001b[34m26%|██▌       | 69/270 [39:49<1:55:31, 34.49s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 70/270 [40:24<1:54:52, 34.46s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0437, 'learning_rate': 0.00015461538461538464, 'epoch': 0.78}\u001b[0m\n",
      "\u001b[34m26%|██▌       | 70/270 [40:24<1:54:52, 34.46s/it]\u001b[0m\n",
      "\u001b[35m26%|██▌       | 70/270 [40:25<1:54:52, 34.46s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0437, 'learning_rate': 0.00015461538461538464, 'epoch': 0.78}\u001b[0m\n",
      "\u001b[35m26%|██▌       | 70/270 [40:25<1:54:52, 34.46s/it]\u001b[0m\n",
      "\u001b[34m26%|██▋       | 71/270 [40:58<1:54:14, 34.45s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0334, 'learning_rate': 0.00015384615384615385, 'epoch': 0.79}\u001b[0m\n",
      "\u001b[34m26%|██▋       | 71/270 [40:58<1:54:14, 34.45s/it]\u001b[0m\n",
      "\u001b[35m26%|██▋       | 71/270 [40:59<1:54:14, 34.44s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0334, 'learning_rate': 0.00015384615384615385, 'epoch': 0.79}\u001b[0m\n",
      "\u001b[35m26%|██▋       | 71/270 [40:59<1:54:14, 34.44s/it]\u001b[0m\n",
      "\u001b[35m27%|██▋       | 72/270 [41:33<1:53:38, 34.43s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0365, 'learning_rate': 0.00015307692307692307, 'epoch': 0.8}\u001b[0m\n",
      "\u001b[35m27%|██▋       | 72/270 [41:33<1:53:38, 34.43s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 72/270 [41:32<1:53:37, 34.43s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0365, 'learning_rate': 0.00015307692307692307, 'epoch': 0.8}\u001b[0m\n",
      "\u001b[34m27%|██▋       | 72/270 [41:33<1:53:37, 34.43s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 73/270 [42:07<1:53:01, 34.42s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0381, 'learning_rate': 0.0001523076923076923, 'epoch': 0.81}\u001b[0m\n",
      "\u001b[34m27%|██▋       | 73/270 [42:07<1:53:01, 34.42s/it]\u001b[0m\n",
      "\u001b[35m27%|██▋       | 73/270 [42:08<1:53:01, 34.42s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0381, 'learning_rate': 0.0001523076923076923, 'epoch': 0.81}\u001b[0m\n",
      "\u001b[35m27%|██▋       | 73/270 [42:08<1:53:01, 34.42s/it]\u001b[0m\n",
      "\u001b[35m27%|██▋       | 74/270 [42:42<1:52:25, 34.41s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0312, 'learning_rate': 0.00015153846153846153, 'epoch': 0.82}\u001b[0m\n",
      "\u001b[35m27%|██▋       | 74/270 [42:42<1:52:25, 34.41s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 74/270 [42:41<1:52:25, 34.41s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0312, 'learning_rate': 0.00015153846153846153, 'epoch': 0.82}\u001b[0m\n",
      "\u001b[34m27%|██▋       | 74/270 [42:41<1:52:25, 34.41s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 75/270 [43:16<1:51:49, 34.41s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0352, 'learning_rate': 0.00015076923076923077, 'epoch': 0.83}\u001b[0m\n",
      "\u001b[34m28%|██▊       | 75/270 [43:16<1:51:49, 34.41s/it]\u001b[0m\n",
      "\u001b[35m28%|██▊       | 75/270 [43:17<1:51:49, 34.41s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0352, 'learning_rate': 0.00015076923076923077, 'epoch': 0.83}\u001b[0m\n",
      "\u001b[35m28%|██▊       | 75/270 [43:17<1:51:49, 34.41s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 76/270 [43:50<1:51:14, 34.41s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0416, 'learning_rate': 0.00015000000000000001, 'epoch': 0.84}\u001b[0m\n",
      "\u001b[34m28%|██▊       | 76/270 [43:50<1:51:14, 34.41s/it]\u001b[0m\n",
      "\u001b[35m28%|██▊       | 76/270 [43:51<1:51:14, 34.41s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0416, 'learning_rate': 0.00015000000000000001, 'epoch': 0.84}\u001b[0m\n",
      "\u001b[35m28%|██▊       | 76/270 [43:51<1:51:14, 34.41s/it]\u001b[0m\n",
      "\u001b[35m29%|██▊       | 77/270 [44:25<1:50:40, 34.41s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0422, 'learning_rate': 0.00014923076923076923, 'epoch': 0.86}\u001b[0m\n",
      "\u001b[35m29%|██▊       | 77/270 [44:25<1:50:40, 34.41s/it]\u001b[0m\n",
      "\u001b[34m29%|██▊       | 77/270 [44:24<1:50:40, 34.41s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0422, 'learning_rate': 0.00014923076923076923, 'epoch': 0.86}\u001b[0m\n",
      "\u001b[34m29%|██▊       | 77/270 [44:24<1:50:40, 34.41s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 78/270 [44:59<1:50:05, 34.40s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0322, 'learning_rate': 0.00014846153846153847, 'epoch': 0.87}\u001b[0m\n",
      "\u001b[34m29%|██▉       | 78/270 [44:59<1:50:05, 34.40s/it]\u001b[0m\n",
      "\u001b[35m29%|██▉       | 78/270 [45:00<1:50:05, 34.40s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0322, 'learning_rate': 0.00014846153846153847, 'epoch': 0.87}\u001b[0m\n",
      "\u001b[35m29%|██▉       | 78/270 [45:00<1:50:05, 34.40s/it]\u001b[0m\n",
      "\u001b[35m29%|██▉       | 79/270 [45:34<1:49:30, 34.40s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0341, 'learning_rate': 0.00014769230769230772, 'epoch': 0.88}\u001b[0m\n",
      "\u001b[35m29%|██▉       | 79/270 [45:34<1:49:30, 34.40s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 79/270 [45:33<1:49:30, 34.40s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0341, 'learning_rate': 0.00014769230769230772, 'epoch': 0.88}\u001b[0m\n",
      "\u001b[34m29%|██▉       | 79/270 [45:33<1:49:30, 34.40s/it]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 80/270 [46:08<1:48:56, 34.40s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0382, 'learning_rate': 0.00014692307692307693, 'epoch': 0.89}\u001b[0m\n",
      "\u001b[34m30%|██▉       | 80/270 [46:08<1:48:56, 34.40s/it]\u001b[0m\n",
      "\u001b[34m[2023-10-27 22:26:23,174] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:197] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 22:26:23,182] [INFO] [axolotl.utils.dataloader.generate_batches:181] [PID:197] [RANK:0] generating packed batches#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 22:26:23,182] [INFO] [axolotl.utils.dataloader.generate_batches:187] [PID:197] [RANK:0] a3efec085e0760812a351b73f7cdffc9a801ed4832469f9f56e2c45c91279c6e#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 22:26:23,184] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:197] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[35m30%|██▉       | 80/270 [46:09<1:48:56, 34.40s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0382, 'learning_rate': 0.00014692307692307693, 'epoch': 0.89}\u001b[0m\n",
      "\u001b[35m30%|██▉       | 80/270 [46:09<1:48:56, 34.40s/it]\u001b[0m\n",
      "\u001b[35m[2023-10-27 22:26:23,173] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:196] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 22:26:23,181] [INFO] [axolotl.utils.dataloader.generate_batches:181] [PID:196] [RANK:0] generating packed batches#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 22:26:23,181] [INFO] [axolotl.utils.dataloader.generate_batches:187] [PID:196] [RANK:0] b68612dda09b7da3bd0173ed59f621c8c8f01237e9f3dcbc6d7b7c116d4e5e0c#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 22:26:23,183] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:196] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 22:26:25,678] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:196] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 22:26:25,678] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:196] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[35m0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[2023-10-27 22:26:25,679] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:197] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 22:26:25,679] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:197] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[34m0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[2023-10-27 22:26:28,241] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:197] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[34m100%|██████████| 2/2 [00:02<00:00,  1.28s/it]\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.030958540737628937, 'eval_runtime': 5.1661, 'eval_samples_per_second': 152.147, 'eval_steps_per_second': 38.133, 'epoch': 0.89}\u001b[0m\n",
      "\u001b[34m30%|██▉       | 80/270 [46:13<1:48:56, 34.40s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 2/2 [00:02<00:00,  1.28s/it]\u001b[0m\n",
      "\u001b[35m[2023-10-27 22:26:28,242] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:196] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[35m100%|██████████| 2/2 [00:02<00:00,  1.28s/it]\u001b[0m\n",
      "\u001b[35m{'eval_loss': 0.030958540737628937, 'eval_runtime': 5.1665, 'eval_samples_per_second': 152.134, 'eval_steps_per_second': 38.13, 'epoch': 0.89}\u001b[0m\n",
      "\u001b[35m30%|██▉       | 80/270 [46:14<1:48:56, 34.40s/it]\u001b[0m\n",
      "\u001b[35m100%|██████████| 2/2 [00:02<00:00,  1.28s/it]\u001b[0m\n",
      "\u001b[35m30%|███       | 81/270 [46:48<1:53:15, 35.96s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0325, 'learning_rate': 0.00014615384615384615, 'epoch': 0.9}\u001b[0m\n",
      "\u001b[35m30%|███       | 81/270 [46:48<1:53:15, 35.96s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 81/270 [46:47<1:53:15, 35.96s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0325, 'learning_rate': 0.00014615384615384615, 'epoch': 0.9}\u001b[0m\n",
      "\u001b[34m30%|███       | 81/270 [46:47<1:53:15, 35.96s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 82/270 [47:22<1:51:12, 35.49s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0352, 'learning_rate': 0.0001453846153846154, 'epoch': 0.91}\u001b[0m\n",
      "\u001b[34m30%|███       | 82/270 [47:22<1:51:12, 35.49s/it]\u001b[0m\n",
      "\u001b[35m30%|███       | 82/270 [47:23<1:51:12, 35.49s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0352, 'learning_rate': 0.0001453846153846154, 'epoch': 0.91}\u001b[0m\n",
      "\u001b[35m30%|███       | 82/270 [47:23<1:51:12, 35.49s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 83/270 [47:56<1:49:36, 35.17s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0389, 'learning_rate': 0.0001446153846153846, 'epoch': 0.92}\u001b[0m\n",
      "\u001b[34m31%|███       | 83/270 [47:56<1:49:36, 35.17s/it]\u001b[0m\n",
      "\u001b[35m31%|███       | 83/270 [47:57<1:49:35, 35.16s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0389, 'learning_rate': 0.0001446153846153846, 'epoch': 0.92}\u001b[0m\n",
      "\u001b[35m31%|███       | 83/270 [47:57<1:49:35, 35.16s/it]\u001b[0m\n",
      "\u001b[35m31%|███       | 84/270 [48:31<1:48:17, 34.94s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0337, 'learning_rate': 0.00014384615384615385, 'epoch': 0.93}\u001b[0m\n",
      "\u001b[35m31%|███       | 84/270 [48:31<1:48:17, 34.94s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 84/270 [48:30<1:48:17, 34.94s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0337, 'learning_rate': 0.00014384615384615385, 'epoch': 0.93}\u001b[0m\n",
      "\u001b[34m31%|███       | 84/270 [48:30<1:48:17, 34.94s/it]\u001b[0m\n",
      "\u001b[34m31%|███▏      | 85/270 [49:05<1:47:13, 34.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0378, 'learning_rate': 0.0001430769230769231, 'epoch': 0.94}\u001b[0m\n",
      "\u001b[34m31%|███▏      | 85/270 [49:05<1:47:13, 34.77s/it]\u001b[0m\n",
      "\u001b[35m31%|███▏      | 85/270 [49:06<1:47:12, 34.77s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0378, 'learning_rate': 0.0001430769230769231, 'epoch': 0.94}\u001b[0m\n",
      "\u001b[35m31%|███▏      | 85/270 [49:06<1:47:12, 34.77s/it]\u001b[0m\n",
      "\u001b[35m32%|███▏      | 86/270 [49:40<1:46:17, 34.66s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0332, 'learning_rate': 0.0001423076923076923, 'epoch': 0.96}\u001b[0m\n",
      "\u001b[35m32%|███▏      | 86/270 [49:40<1:46:17, 34.66s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 86/270 [49:39<1:46:17, 34.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0332, 'learning_rate': 0.0001423076923076923, 'epoch': 0.96}\u001b[0m\n",
      "\u001b[34m32%|███▏      | 86/270 [49:39<1:46:17, 34.66s/it]\u001b[0m\n",
      "\u001b[35m32%|███▏      | 87/270 [50:14<1:45:28, 34.58s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0343, 'learning_rate': 0.00014153846153846156, 'epoch': 0.97}\u001b[0m\n",
      "\u001b[35m32%|███▏      | 87/270 [50:14<1:45:28, 34.58s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 87/270 [50:14<1:45:28, 34.58s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0343, 'learning_rate': 0.00014153846153846156, 'epoch': 0.97}\u001b[0m\n",
      "\u001b[34m32%|███▏      | 87/270 [50:14<1:45:28, 34.58s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 88/270 [50:48<1:44:43, 34.53s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0365, 'learning_rate': 0.0001407692307692308, 'epoch': 0.98}\u001b[0m\n",
      "\u001b[34m33%|███▎      | 88/270 [50:48<1:44:43, 34.53s/it]\u001b[0m\n",
      "\u001b[35m33%|███▎      | 88/270 [50:49<1:44:43, 34.53s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0365, 'learning_rate': 0.0001407692307692308, 'epoch': 0.98}\u001b[0m\n",
      "\u001b[35m33%|███▎      | 88/270 [50:49<1:44:43, 34.53s/it]\u001b[0m\n",
      "\u001b[35m33%|███▎      | 89/270 [51:23<1:44:02, 34.49s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0338, 'learning_rate': 0.00014, 'epoch': 0.99}\u001b[0m\n",
      "\u001b[35m33%|███▎      | 89/270 [51:23<1:44:02, 34.49s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 89/270 [51:22<1:44:02, 34.49s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0338, 'learning_rate': 0.00014, 'epoch': 0.99}\u001b[0m\n",
      "\u001b[34m33%|███▎      | 89/270 [51:22<1:44:02, 34.49s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 90/270 [51:57<1:43:23, 34.46s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0295, 'learning_rate': 0.00013923076923076923, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34m33%|███▎      | 90/270 [51:57<1:43:23, 34.46s/it]\u001b[0m\n",
      "\u001b[35m33%|███▎      | 90/270 [51:58<1:43:23, 34.46s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0295, 'learning_rate': 0.00013923076923076923, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[35m33%|███▎      | 90/270 [51:58<1:43:23, 34.46s/it]\u001b[0m\n",
      "\u001b[35m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1802: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[35m[2023-10-27 22:32:46,795] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:196] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 2990518#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 22:32:46,795] [INFO] [axolotl.utils.dataloader.__iter__:213] [PID:196] [RANK:0] calling sampler.set_epoch(2)#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 22:32:46,795] [INFO] [axolotl.utils.dataloader.generate_batches:181] [PID:196] [RANK:0] generating packed batches#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 22:32:46,805] [INFO] [axolotl.utils.dataloader.generate_batches:187] [PID:196] [RANK:0] 45997ed5e4175d5bc78a4fb7243090e057bd7ea5b959437ab877923274973d59#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 22:32:46,983] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:196] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 2990518#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 22:32:46,797] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:197] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 2990518#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 22:32:46,798] [INFO] [axolotl.utils.dataloader.__iter__:213] [PID:197] [RANK:0] calling sampler.set_epoch(2)#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 22:32:46,798] [INFO] [axolotl.utils.dataloader.generate_batches:181] [PID:197] [RANK:0] generating packed batches#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 22:32:46,807] [INFO] [axolotl.utils.dataloader.generate_batches:187] [PID:197] [RANK:0] c3ccb74cae8fea5dbe15b2ec23f85564c4261f70a57ccdc7ef6a8ff10628ab13#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 22:32:46,985] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:197] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 2990518#033[39m\u001b[0m\n",
      "\u001b[34m34%|███▎      | 91/270 [53:06<2:13:51, 44.87s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0293, 'learning_rate': 0.00013846153846153847, 'epoch': 1.01}\u001b[0m\n",
      "\u001b[34m34%|███▎      | 91/270 [53:06<2:13:51, 44.87s/it]\u001b[0m\n",
      "\u001b[35m34%|███▎      | 91/270 [53:07<2:13:51, 44.87s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0293, 'learning_rate': 0.00013846153846153847, 'epoch': 1.01}\u001b[0m\n",
      "\u001b[35m34%|███▎      | 91/270 [53:07<2:13:51, 44.87s/it]\u001b[0m\n",
      "\u001b[35m34%|███▍      | 92/270 [53:41<2:03:45, 41.71s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0302, 'learning_rate': 0.0001376923076923077, 'epoch': 1.02}\u001b[0m\n",
      "\u001b[35m34%|███▍      | 92/270 [53:41<2:03:45, 41.71s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 92/270 [53:40<2:03:45, 41.71s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0302, 'learning_rate': 0.0001376923076923077, 'epoch': 1.02}\u001b[0m\n",
      "\u001b[34m34%|███▍      | 92/270 [53:40<2:03:45, 41.71s/it]\u001b[0m\n",
      "\u001b[35m34%|███▍      | 93/270 [54:16<1:56:33, 39.51s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0312, 'learning_rate': 0.00013692307692307693, 'epoch': 1.03}\u001b[0m\n",
      "\u001b[35m34%|███▍      | 93/270 [54:16<1:56:33, 39.51s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 93/270 [54:15<1:56:33, 39.51s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0312, 'learning_rate': 0.00013692307692307693, 'epoch': 1.03}\u001b[0m\n",
      "\u001b[34m34%|███▍      | 93/270 [54:15<1:56:33, 39.51s/it]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 94/270 [54:49<1:51:23, 37.98s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0305, 'learning_rate': 0.00013615384615384618, 'epoch': 1.04}\u001b[0m\n",
      "\u001b[34m35%|███▍      | 94/270 [54:49<1:51:23, 37.98s/it]\u001b[0m\n",
      "\u001b[35m35%|███▍      | 94/270 [54:50<1:51:23, 37.98s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0305, 'learning_rate': 0.00013615384615384618, 'epoch': 1.04}\u001b[0m\n",
      "\u001b[35m35%|███▍      | 94/270 [54:50<1:51:23, 37.98s/it]\u001b[0m\n",
      "\u001b[35m35%|███▌      | 95/270 [55:24<1:47:37, 36.90s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.03, 'learning_rate': 0.0001353846153846154, 'epoch': 1.06}\u001b[0m\n",
      "\u001b[35m35%|███▌      | 95/270 [55:24<1:47:37, 36.90s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 95/270 [55:24<1:47:37, 36.90s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.03, 'learning_rate': 0.0001353846153846154, 'epoch': 1.06}\u001b[0m\n",
      "\u001b[34m35%|███▌      | 95/270 [55:24<1:47:37, 36.90s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 96/270 [55:58<1:44:50, 36.15s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0293, 'learning_rate': 0.00013461538461538464, 'epoch': 1.07}\u001b[0m\n",
      "\u001b[34m36%|███▌      | 96/270 [55:58<1:44:50, 36.15s/it]\u001b[0m\n",
      "\u001b[35m36%|███▌      | 96/270 [55:59<1:44:50, 36.15s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0293, 'learning_rate': 0.00013461538461538464, 'epoch': 1.07}\u001b[0m\n",
      "\u001b[35m36%|███▌      | 96/270 [55:59<1:44:50, 36.15s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 97/270 [56:32<1:42:43, 35.62s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0282, 'learning_rate': 0.00013384615384615385, 'epoch': 1.08}\u001b[0m\n",
      "\u001b[34m36%|███▌      | 97/270 [56:32<1:42:43, 35.62s/it]\u001b[0m\n",
      "\u001b[35m36%|███▌      | 97/270 [56:33<1:42:43, 35.63s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0282, 'learning_rate': 0.00013384615384615385, 'epoch': 1.08}\u001b[0m\n",
      "\u001b[35m36%|███▌      | 97/270 [56:33<1:42:43, 35.63s/it]\u001b[0m\n",
      "\u001b[35m36%|███▋      | 98/270 [57:08<1:41:03, 35.25s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0327, 'learning_rate': 0.00013307692307692307, 'epoch': 1.09}\u001b[0m\n",
      "\u001b[35m36%|███▋      | 98/270 [57:08<1:41:03, 35.25s/it]\u001b[0m\n",
      "\u001b[34m36%|███▋      | 98/270 [57:07<1:41:03, 35.25s/it]\u001b[0m\n",
      "\u001b[34m36%|███▋      | 98/270 [57:07<1:41:03, 35.25s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0327, 'learning_rate': 0.00013307692307692307, 'epoch': 1.09}\u001b[0m\n",
      "\u001b[34m37%|███▋      | 99/270 [57:41<1:39:44, 34.99s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0331, 'learning_rate': 0.0001323076923076923, 'epoch': 1.1}\u001b[0m\n",
      "\u001b[34m37%|███▋      | 99/270 [57:41<1:39:44, 34.99s/it]\u001b[0m\n",
      "\u001b[35m37%|███▋      | 99/270 [57:42<1:39:44, 34.99s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0331, 'learning_rate': 0.0001323076923076923, 'epoch': 1.1}\u001b[0m\n",
      "\u001b[35m37%|███▋      | 99/270 [57:42<1:39:44, 34.99s/it]\u001b[0m\n",
      "\u001b[35m37%|███▋      | 100/270 [58:16<1:38:38, 34.81s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0271, 'learning_rate': 0.00013153846153846156, 'epoch': 1.11}\u001b[0m\n",
      "\u001b[35m37%|███▋      | 100/270 [58:16<1:38:38, 34.81s/it]\u001b[0m\n",
      "\u001b[35m[2023-10-27 22:38:30,990] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:196] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 22:38:30,998] [INFO] [axolotl.utils.dataloader.generate_batches:181] [PID:196] [RANK:0] generating packed batches#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 22:38:30,998] [INFO] [axolotl.utils.dataloader.generate_batches:187] [PID:196] [RANK:0] b68612dda09b7da3bd0173ed59f621c8c8f01237e9f3dcbc6d7b7c116d4e5e0c#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 22:38:31,000] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:196] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[34m37%|███▋      | 100/270 [58:15<1:38:38, 34.81s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0271, 'learning_rate': 0.00013153846153846156, 'epoch': 1.11}\u001b[0m\n",
      "\u001b[34m37%|███▋      | 100/270 [58:16<1:38:38, 34.81s/it]\u001b[0m\n",
      "\u001b[34m[2023-10-27 22:38:30,991] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:197] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 22:38:30,999] [INFO] [axolotl.utils.dataloader.generate_batches:181] [PID:197] [RANK:0] generating packed batches#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 22:38:30,999] [INFO] [axolotl.utils.dataloader.generate_batches:187] [PID:197] [RANK:0] a3efec085e0760812a351b73f7cdffc9a801ed4832469f9f56e2c45c91279c6e#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 22:38:31,001] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:197] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 22:38:33,496] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:197] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 22:38:33,496] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:197] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[34m0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[2023-10-27 22:38:33,496] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:196] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 22:38:33,496] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:196] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[35m0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[2023-10-27 22:38:36,059] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:196] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[35m100%|██████████| 2/2 [00:02<00:00,  1.28s/it]\u001b[0m\n",
      "\u001b[35m{'eval_loss': 0.02634860947728157, 'eval_runtime': 5.1668, 'eval_samples_per_second': 152.126, 'eval_steps_per_second': 38.128, 'epoch': 1.11}\u001b[0m\n",
      "\u001b[35m37%|███▋      | 100/270 [58:21<1:38:38, 34.81s/it]\u001b[0m\n",
      "\u001b[35m100%|██████████| 2/2 [00:02<00:00,  1.28s/it]\u001b[0m\n",
      "\u001b[34m[2023-10-27 22:38:36,058] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:197] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[34m100%|██████████| 2/2 [00:02<00:00,  1.28s/it]\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.02634860947728157, 'eval_runtime': 5.1665, 'eval_samples_per_second': 152.134, 'eval_steps_per_second': 38.13, 'epoch': 1.11}\u001b[0m\n",
      "\u001b[34m37%|███▋      | 100/270 [58:21<1:38:38, 34.81s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 2/2 [00:02<00:00,  1.28s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 101/270 [58:55<1:42:05, 36.24s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0368, 'learning_rate': 0.00013076923076923077, 'epoch': 1.12}\u001b[0m\n",
      "\u001b[34m37%|███▋      | 101/270 [58:55<1:42:05, 36.24s/it]\u001b[0m\n",
      "\u001b[35m37%|███▋      | 101/270 [58:56<1:42:05, 36.24s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0368, 'learning_rate': 0.00013076923076923077, 'epoch': 1.12}\u001b[0m\n",
      "\u001b[35m37%|███▋      | 101/270 [58:56<1:42:05, 36.24s/it]\u001b[0m\n",
      "\u001b[35m38%|███▊      | 102/270 [59:30<1:39:55, 35.69s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0307, 'learning_rate': 0.00013000000000000002, 'epoch': 1.13}\u001b[0m\n",
      "\u001b[35m38%|███▊      | 102/270 [59:30<1:39:55, 35.69s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 102/270 [59:29<1:39:55, 35.69s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0307, 'learning_rate': 0.00013000000000000002, 'epoch': 1.13}\u001b[0m\n",
      "\u001b[34m38%|███▊      | 102/270 [59:29<1:39:55, 35.69s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 103/270 [1:00:04<1:38:15, 35.30s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0269, 'learning_rate': 0.00012923076923076923, 'epoch': 1.14}\u001b[0m\n",
      "\u001b[34m38%|███▊      | 103/270 [1:00:04<1:38:15, 35.30s/it]\u001b[0m\n",
      "\u001b[35m38%|███▊      | 103/270 [1:00:05<1:38:15, 35.30s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0269, 'learning_rate': 0.00012923076923076923, 'epoch': 1.14}\u001b[0m\n",
      "\u001b[35m38%|███▊      | 103/270 [1:00:05<1:38:15, 35.30s/it]\u001b[0m\n",
      "\u001b[34m39%|███▊      | 104/270 [1:00:38<1:36:55, 35.03s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0262, 'learning_rate': 0.00012846153846153848, 'epoch': 1.16}\u001b[0m\n",
      "\u001b[34m39%|███▊      | 104/270 [1:00:38<1:36:55, 35.03s/it]\u001b[0m\n",
      "\u001b[35m39%|███▊      | 104/270 [1:00:39<1:36:55, 35.03s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0262, 'learning_rate': 0.00012846153846153848, 'epoch': 1.16}\u001b[0m\n",
      "\u001b[35m39%|███▊      | 104/270 [1:00:39<1:36:55, 35.03s/it]\u001b[0m\n",
      "\u001b[35m39%|███▉      | 105/270 [1:01:14<1:35:57, 34.89s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0265, 'learning_rate': 0.0001276923076923077, 'epoch': 1.17}\u001b[0m\n",
      "\u001b[35m39%|███▉      | 105/270 [1:01:14<1:35:57, 34.89s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 105/270 [1:01:13<1:35:57, 34.89s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0265, 'learning_rate': 0.0001276923076923077, 'epoch': 1.17}\u001b[0m\n",
      "\u001b[34m39%|███▉      | 105/270 [1:01:13<1:35:57, 34.89s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 106/270 [1:01:47<1:34:58, 34.75s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0328, 'learning_rate': 0.00012692307692307693, 'epoch': 1.18}\u001b[0m\n",
      "\u001b[34m39%|███▉      | 106/270 [1:01:47<1:34:58, 34.75s/it]\u001b[0m\n",
      "\u001b[35m39%|███▉      | 106/270 [1:01:48<1:34:58, 34.75s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0328, 'learning_rate': 0.00012692307692307693, 'epoch': 1.18}\u001b[0m\n",
      "\u001b[35m39%|███▉      | 106/270 [1:01:48<1:34:58, 34.75s/it]\u001b[0m\n",
      "\u001b[35m40%|███▉      | 107/270 [1:02:22<1:34:07, 34.65s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0321, 'learning_rate': 0.00012615384615384615, 'epoch': 1.19}\u001b[0m\n",
      "\u001b[35m40%|███▉      | 107/270 [1:02:22<1:34:07, 34.65s/it]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 107/270 [1:02:22<1:34:07, 34.65s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0321, 'learning_rate': 0.00012615384615384615, 'epoch': 1.19}\u001b[0m\n",
      "\u001b[34m40%|███▉      | 107/270 [1:02:22<1:34:07, 34.65s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 108/270 [1:02:56<1:33:29, 34.62s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0311, 'learning_rate': 0.0001253846153846154, 'epoch': 1.2}\u001b[0m\n",
      "\u001b[34m40%|████      | 108/270 [1:02:56<1:33:29, 34.62s/it]\u001b[0m\n",
      "\u001b[35m40%|████      | 108/270 [1:02:57<1:33:29, 34.62s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0311, 'learning_rate': 0.0001253846153846154, 'epoch': 1.2}\u001b[0m\n",
      "\u001b[35m40%|████      | 108/270 [1:02:57<1:33:29, 34.62s/it]\u001b[0m\n",
      "\u001b[35m40%|████      | 109/270 [1:03:31<1:32:43, 34.56s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0274, 'learning_rate': 0.0001246153846153846, 'epoch': 1.21}\u001b[0m\n",
      "\u001b[35m40%|████      | 109/270 [1:03:31<1:32:43, 34.56s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 109/270 [1:03:31<1:32:43, 34.56s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0274, 'learning_rate': 0.0001246153846153846, 'epoch': 1.21}\u001b[0m\n",
      "\u001b[34m40%|████      | 109/270 [1:03:31<1:32:43, 34.56s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 110/270 [1:04:05<1:32:01, 34.51s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0278, 'learning_rate': 0.00012384615384615385, 'epoch': 1.22}\u001b[0m\n",
      "\u001b[34m41%|████      | 110/270 [1:04:05<1:32:01, 34.51s/it]\u001b[0m\n",
      "\u001b[35m41%|████      | 110/270 [1:04:06<1:32:01, 34.51s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0278, 'learning_rate': 0.00012384615384615385, 'epoch': 1.22}\u001b[0m\n",
      "\u001b[35m41%|████      | 110/270 [1:04:06<1:32:01, 34.51s/it]\u001b[0m\n",
      "\u001b[35m41%|████      | 111/270 [1:04:40<1:31:21, 34.48s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.027, 'learning_rate': 0.0001230769230769231, 'epoch': 1.23}\u001b[0m\n",
      "\u001b[35m41%|████      | 111/270 [1:04:40<1:31:21, 34.48s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 111/270 [1:04:39<1:31:21, 34.47s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.027, 'learning_rate': 0.0001230769230769231, 'epoch': 1.23}\u001b[0m\n",
      "\u001b[34m41%|████      | 111/270 [1:04:39<1:31:21, 34.47s/it]\u001b[0m\n",
      "\u001b[35m41%|████▏     | 112/270 [1:05:15<1:30:43, 34.45s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0306, 'learning_rate': 0.00012230769230769231, 'epoch': 1.24}\u001b[0m\n",
      "\u001b[35m41%|████▏     | 112/270 [1:05:15<1:30:43, 34.45s/it]\u001b[0m\n",
      "\u001b[34m41%|████▏     | 112/270 [1:05:14<1:30:43, 34.45s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0306, 'learning_rate': 0.00012230769230769231, 'epoch': 1.24}\u001b[0m\n",
      "\u001b[34m41%|████▏     | 112/270 [1:05:14<1:30:43, 34.45s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 113/270 [1:05:48<1:30:06, 34.44s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0281, 'learning_rate': 0.00012153846153846153, 'epoch': 1.26}\u001b[0m\n",
      "\u001b[34m42%|████▏     | 113/270 [1:05:48<1:30:06, 34.44s/it]\u001b[0m\n",
      "\u001b[35m42%|████▏     | 113/270 [1:05:49<1:30:06, 34.44s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0281, 'learning_rate': 0.00012153846153846153, 'epoch': 1.26}\u001b[0m\n",
      "\u001b[35m42%|████▏     | 113/270 [1:05:49<1:30:06, 34.44s/it]\u001b[0m\n",
      "\u001b[35m42%|████▏     | 114/270 [1:06:23<1:29:30, 34.43s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0317, 'learning_rate': 0.00012076923076923077, 'epoch': 1.27}\u001b[0m\n",
      "\u001b[35m42%|████▏     | 114/270 [1:06:23<1:29:30, 34.43s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 114/270 [1:06:23<1:29:30, 34.43s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0317, 'learning_rate': 0.00012076923076923077, 'epoch': 1.27}\u001b[0m\n",
      "\u001b[34m42%|████▏     | 114/270 [1:06:23<1:29:30, 34.43s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 115/270 [1:06:57<1:28:54, 34.42s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 115/270 [1:06:57<1:28:54, 34.42s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0248, 'learning_rate': 0.00012, 'epoch': 1.28}\u001b[0m\n",
      "\u001b[35m43%|████▎     | 115/270 [1:06:58<1:28:54, 34.42s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0248, 'learning_rate': 0.00012, 'epoch': 1.28}\u001b[0m\n",
      "\u001b[35m43%|████▎     | 115/270 [1:06:58<1:28:54, 34.42s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 116/270 [1:07:31<1:28:19, 34.41s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0329, 'learning_rate': 0.00011923076923076923, 'epoch': 1.29}\u001b[0m\n",
      "\u001b[34m43%|████▎     | 116/270 [1:07:31<1:28:19, 34.41s/it]\u001b[0m\n",
      "\u001b[35m43%|████▎     | 116/270 [1:07:32<1:28:19, 34.41s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0329, 'learning_rate': 0.00011923076923076923, 'epoch': 1.29}\u001b[0m\n",
      "\u001b[35m43%|████▎     | 116/270 [1:07:32<1:28:19, 34.41s/it]\u001b[0m\n",
      "\u001b[35m43%|████▎     | 117/270 [1:08:07<1:27:44, 34.41s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0285, 'learning_rate': 0.00011846153846153846, 'epoch': 1.3}\u001b[0m\n",
      "\u001b[35m43%|████▎     | 117/270 [1:08:07<1:27:44, 34.41s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 117/270 [1:08:06<1:27:44, 34.41s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0285, 'learning_rate': 0.00011846153846153846, 'epoch': 1.3}\u001b[0m\n",
      "\u001b[34m43%|████▎     | 117/270 [1:08:06<1:27:44, 34.41s/it]\u001b[0m\n",
      "\u001b[34m44%|████▎     | 118/270 [1:08:40<1:27:09, 34.40s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0303, 'learning_rate': 0.0001176923076923077, 'epoch': 1.31}\u001b[0m\n",
      "\u001b[34m44%|████▎     | 118/270 [1:08:40<1:27:09, 34.40s/it]\u001b[0m\n",
      "\u001b[35m44%|████▎     | 118/270 [1:08:41<1:27:09, 34.40s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0303, 'learning_rate': 0.0001176923076923077, 'epoch': 1.31}\u001b[0m\n",
      "\u001b[35m44%|████▎     | 118/270 [1:08:41<1:27:09, 34.40s/it]\u001b[0m\n",
      "\u001b[35m44%|████▍     | 119/270 [1:09:15<1:26:35, 34.41s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0244, 'learning_rate': 0.00011692307692307694, 'epoch': 1.32}\u001b[0m\n",
      "\u001b[35m44%|████▍     | 119/270 [1:09:15<1:26:35, 34.41s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 119/270 [1:09:15<1:26:35, 34.41s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0244, 'learning_rate': 0.00011692307692307694, 'epoch': 1.32}\u001b[0m\n",
      "\u001b[34m44%|████▍     | 119/270 [1:09:15<1:26:35, 34.41s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 120/270 [1:09:49<1:26:00, 34.41s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0321, 'learning_rate': 0.00011615384615384617, 'epoch': 1.33}\u001b[0m\n",
      "\u001b[34m44%|████▍     | 120/270 [1:09:49<1:26:00, 34.41s/it]\u001b[0m\n",
      "\u001b[34m[2023-10-27 22:50:04,540] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:197] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 22:50:04,548] [INFO] [axolotl.utils.dataloader.generate_batches:181] [PID:197] [RANK:0] generating packed batches#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 22:50:04,548] [INFO] [axolotl.utils.dataloader.generate_batches:187] [PID:197] [RANK:0] a3efec085e0760812a351b73f7cdffc9a801ed4832469f9f56e2c45c91279c6e#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 22:50:04,550] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:197] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[35m44%|████▍     | 120/270 [1:09:50<1:26:00, 34.41s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0321, 'learning_rate': 0.00011615384615384617, 'epoch': 1.33}\u001b[0m\n",
      "\u001b[35m44%|████▍     | 120/270 [1:09:50<1:26:00, 34.41s/it]\u001b[0m\n",
      "\u001b[35m[2023-10-27 22:50:04,539] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:196] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 22:50:04,547] [INFO] [axolotl.utils.dataloader.generate_batches:181] [PID:196] [RANK:0] generating packed batches#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 22:50:04,547] [INFO] [axolotl.utils.dataloader.generate_batches:187] [PID:196] [RANK:0] b68612dda09b7da3bd0173ed59f621c8c8f01237e9f3dcbc6d7b7c116d4e5e0c#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 22:50:04,549] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:196] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 22:50:07,043] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:196] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 22:50:07,043] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:196] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[35m0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[2023-10-27 22:50:07,046] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:197] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 22:50:07,046] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:197] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[34m0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[2023-10-27 22:50:09,609] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:197] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[34m100%|██████████| 2/2 [00:02<00:00,  1.28s/it]\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.024548904970288277, 'eval_runtime': 5.1679, 'eval_samples_per_second': 152.094, 'eval_steps_per_second': 38.12, 'epoch': 1.33}\u001b[0m\n",
      "\u001b[34m44%|████▍     | 120/270 [1:09:54<1:26:00, 34.41s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 2/2 [00:02<00:00,  1.28s/it]\u001b[0m\n",
      "\u001b[35m[2023-10-27 22:50:09,609] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:196] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[35m100%|██████████| 2/2 [00:02<00:00,  1.28s/it]\u001b[0m\n",
      "\u001b[35m{'eval_loss': 0.024548904970288277, 'eval_runtime': 5.1683, 'eval_samples_per_second': 152.082, 'eval_steps_per_second': 38.117, 'epoch': 1.33}\u001b[0m\n",
      "\u001b[35m44%|████▍     | 120/270 [1:09:55<1:26:00, 34.41s/it]\u001b[0m\n",
      "\u001b[35m100%|██████████| 2/2 [00:02<00:00,  1.28s/it]\u001b[0m\n",
      "\u001b[35m45%|████▍     | 121/270 [1:10:29<1:29:17, 35.96s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0292, 'learning_rate': 0.00011538461538461538, 'epoch': 1.34}\u001b[0m\n",
      "\u001b[35m45%|████▍     | 121/270 [1:10:29<1:29:17, 35.96s/it]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 121/270 [1:10:29<1:29:17, 35.96s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0292, 'learning_rate': 0.00011538461538461538, 'epoch': 1.34}\u001b[0m\n",
      "\u001b[34m45%|████▍     | 121/270 [1:10:29<1:29:17, 35.96s/it]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 122/270 [1:11:03<1:27:32, 35.49s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0297, 'learning_rate': 0.00011461538461538461, 'epoch': 1.36}\u001b[0m\n",
      "\u001b[34m45%|████▌     | 122/270 [1:11:03<1:27:32, 35.49s/it]\u001b[0m\n",
      "\u001b[35m45%|████▌     | 122/270 [1:11:04<1:27:32, 35.49s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0297, 'learning_rate': 0.00011461538461538461, 'epoch': 1.36}\u001b[0m\n",
      "\u001b[35m45%|████▌     | 122/270 [1:11:04<1:27:32, 35.49s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 123/270 [1:11:37<1:26:08, 35.16s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0293, 'learning_rate': 0.00011384615384615384, 'epoch': 1.37}\u001b[0m\n",
      "\u001b[34m46%|████▌     | 123/270 [1:11:37<1:26:08, 35.16s/it]\u001b[0m\n",
      "\u001b[35m46%|████▌     | 123/270 [1:11:38<1:26:09, 35.16s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0293, 'learning_rate': 0.00011384615384615384, 'epoch': 1.37}\u001b[0m\n",
      "\u001b[35m46%|████▌     | 123/270 [1:11:38<1:26:09, 35.16s/it]\u001b[0m\n",
      "\u001b[35m46%|████▌     | 124/270 [1:12:13<1:25:00, 34.93s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0283, 'learning_rate': 0.00011307692307692308, 'epoch': 1.38}\u001b[0m\n",
      "\u001b[35m46%|████▌     | 124/270 [1:12:13<1:25:00, 34.93s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 124/270 [1:12:12<1:25:00, 34.94s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0283, 'learning_rate': 0.00011307692307692308, 'epoch': 1.38}\u001b[0m\n",
      "\u001b[34m46%|████▌     | 124/270 [1:12:12<1:25:00, 34.94s/it]\u001b[0m\n",
      "\u001b[34m46%|████▋     | 125/270 [1:12:46<1:24:02, 34.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0301, 'learning_rate': 0.00011230769230769231, 'epoch': 1.39}\u001b[0m\n",
      "\u001b[34m46%|████▋     | 125/270 [1:12:46<1:24:02, 34.78s/it]\u001b[0m\n",
      "\u001b[35m46%|████▋     | 125/270 [1:12:47<1:24:02, 34.78s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0301, 'learning_rate': 0.00011230769230769231, 'epoch': 1.39}\u001b[0m\n",
      "\u001b[35m46%|████▋     | 125/270 [1:12:47<1:24:02, 34.78s/it]\u001b[0m\n",
      "\u001b[35m47%|████▋     | 126/270 [1:13:21<1:23:11, 34.66s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.027, 'learning_rate': 0.00011153846153846154, 'epoch': 1.4}\u001b[0m\n",
      "\u001b[35m47%|████▋     | 126/270 [1:13:21<1:23:11, 34.66s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 126/270 [1:13:21<1:23:11, 34.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.027, 'learning_rate': 0.00011153846153846154, 'epoch': 1.4}\u001b[0m\n",
      "\u001b[34m47%|████▋     | 126/270 [1:13:21<1:23:11, 34.66s/it]\u001b[0m\n",
      "\u001b[35m47%|████▋     | 127/270 [1:13:56<1:22:25, 34.58s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0289, 'learning_rate': 0.00011076923076923077, 'epoch': 1.41}\u001b[0m\n",
      "\u001b[35m47%|████▋     | 127/270 [1:13:56<1:22:25, 34.58s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 127/270 [1:13:55<1:22:25, 34.58s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0289, 'learning_rate': 0.00011076923076923077, 'epoch': 1.41}\u001b[0m\n",
      "\u001b[34m47%|████▋     | 127/270 [1:13:55<1:22:25, 34.58s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 128/270 [1:14:29<1:21:42, 34.53s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0249, 'learning_rate': 0.00011000000000000002, 'epoch': 1.42}\u001b[0m\n",
      "\u001b[34m47%|████▋     | 128/270 [1:14:29<1:21:42, 34.53s/it]\u001b[0m\n",
      "\u001b[35m47%|████▋     | 128/270 [1:14:30<1:21:43, 34.53s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0249, 'learning_rate': 0.00011000000000000002, 'epoch': 1.42}\u001b[0m\n",
      "\u001b[35m47%|████▋     | 128/270 [1:14:30<1:21:43, 34.53s/it]\u001b[0m\n",
      "\u001b[35m48%|████▊     | 129/270 [1:15:05<1:21:03, 34.49s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0273, 'learning_rate': 0.00010923076923076922, 'epoch': 1.43}\u001b[0m\n",
      "\u001b[35m48%|████▊     | 129/270 [1:15:05<1:21:03, 34.49s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 129/270 [1:15:04<1:21:03, 34.49s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0273, 'learning_rate': 0.00010923076923076922, 'epoch': 1.43}\u001b[0m\n",
      "\u001b[34m48%|████▊     | 129/270 [1:15:04<1:21:03, 34.49s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 130/270 [1:15:38<1:20:25, 34.47s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0281, 'learning_rate': 0.00010846153846153846, 'epoch': 1.44}\u001b[0m\n",
      "\u001b[34m48%|████▊     | 130/270 [1:15:38<1:20:25, 34.47s/it]\u001b[0m\n",
      "\u001b[35m48%|████▊     | 130/270 [1:15:39<1:20:25, 34.47s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0281, 'learning_rate': 0.00010846153846153846, 'epoch': 1.44}\u001b[0m\n",
      "\u001b[35m48%|████▊     | 130/270 [1:15:39<1:20:25, 34.47s/it]\u001b[0m\n",
      "\u001b[34m49%|████▊     | 131/270 [1:16:13<1:19:48, 34.45s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0221, 'learning_rate': 0.0001076923076923077, 'epoch': 1.46}\u001b[0m\n",
      "\u001b[34m49%|████▊     | 131/270 [1:16:13<1:19:48, 34.45s/it]\u001b[0m\n",
      "\u001b[35m49%|████▊     | 131/270 [1:16:13<1:19:48, 34.45s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0221, 'learning_rate': 0.0001076923076923077, 'epoch': 1.46}\u001b[0m\n",
      "\u001b[35m49%|████▊     | 131/270 [1:16:13<1:19:48, 34.45s/it]\u001b[0m\n",
      "\u001b[35m49%|████▉     | 132/270 [1:16:48<1:19:12, 34.44s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0275, 'learning_rate': 0.00010692307692307692, 'epoch': 1.47}\u001b[0m\n",
      "\u001b[35m49%|████▉     | 132/270 [1:16:48<1:19:12, 34.44s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 132/270 [1:16:47<1:19:12, 34.44s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0275, 'learning_rate': 0.00010692307692307692, 'epoch': 1.47}\u001b[0m\n",
      "\u001b[34m49%|████▉     | 132/270 [1:16:47<1:19:12, 34.44s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 133/270 [1:17:21<1:18:36, 34.43s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0271, 'learning_rate': 0.00010615384615384615, 'epoch': 1.48}\u001b[0m\n",
      "\u001b[34m49%|████▉     | 133/270 [1:17:21<1:18:36, 34.43s/it]\u001b[0m\n",
      "\u001b[35m49%|████▉     | 133/270 [1:17:22<1:18:36, 34.43s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0271, 'learning_rate': 0.00010615384615384615, 'epoch': 1.48}\u001b[0m\n",
      "\u001b[35m49%|████▉     | 133/270 [1:17:22<1:18:36, 34.43s/it]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 134/270 [1:17:56<1:18:00, 34.42s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0281, 'learning_rate': 0.0001053846153846154, 'epoch': 1.49}\u001b[0m\n",
      "\u001b[34m50%|████▉     | 134/270 [1:17:56<1:18:00, 34.42s/it]\u001b[0m\n",
      "\u001b[35m50%|████▉     | 134/270 [1:17:57<1:18:00, 34.42s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0281, 'learning_rate': 0.0001053846153846154, 'epoch': 1.49}\u001b[0m\n",
      "\u001b[35m50%|████▉     | 134/270 [1:17:57<1:18:00, 34.42s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 135/270 [1:18:30<1:17:24, 34.41s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0231, 'learning_rate': 0.00010461538461538463, 'epoch': 1.5}\u001b[0m\n",
      "\u001b[34m50%|█████     | 135/270 [1:18:30<1:17:24, 34.41s/it]\u001b[0m\n",
      "\u001b[35m50%|█████     | 135/270 [1:18:31<1:17:24, 34.41s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0231, 'learning_rate': 0.00010461538461538463, 'epoch': 1.5}\u001b[0m\n",
      "\u001b[35m50%|█████     | 135/270 [1:18:31<1:17:24, 34.41s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 136/270 [1:19:05<1:16:50, 34.40s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0291, 'learning_rate': 0.00010384615384615386, 'epoch': 1.51}\u001b[0m\n",
      "\u001b[34m50%|█████     | 136/270 [1:19:05<1:16:50, 34.40s/it]\u001b[0m\n",
      "\u001b[35m50%|█████     | 136/270 [1:19:05<1:16:50, 34.41s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0291, 'learning_rate': 0.00010384615384615386, 'epoch': 1.51}\u001b[0m\n",
      "\u001b[35m50%|█████     | 136/270 [1:19:05<1:16:50, 34.41s/it]\u001b[0m\n",
      "\u001b[35m51%|█████     | 137/270 [1:19:40<1:16:15, 34.40s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0282, 'learning_rate': 0.00010307692307692307, 'epoch': 1.52}\u001b[0m\n",
      "\u001b[35m51%|█████     | 137/270 [1:19:40<1:16:15, 34.40s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 137/270 [1:19:39<1:16:15, 34.40s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0282, 'learning_rate': 0.00010307692307692307, 'epoch': 1.52}\u001b[0m\n",
      "\u001b[34m51%|█████     | 137/270 [1:19:39<1:16:15, 34.40s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 138/270 [1:20:13<1:15:40, 34.40s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0332, 'learning_rate': 0.0001023076923076923, 'epoch': 1.53}\u001b[0m\n",
      "\u001b[34m51%|█████     | 138/270 [1:20:13<1:15:40, 34.40s/it]\u001b[0m\n",
      "\u001b[35m51%|█████     | 138/270 [1:20:14<1:15:41, 34.40s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0332, 'learning_rate': 0.0001023076923076923, 'epoch': 1.53}\u001b[0m\n",
      "\u001b[35m51%|█████     | 138/270 [1:20:14<1:15:41, 34.40s/it]\u001b[0m\n",
      "\u001b[35m51%|█████▏    | 139/270 [1:20:49<1:15:06, 34.40s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0289, 'learning_rate': 0.00010153846153846153, 'epoch': 1.54}\u001b[0m\n",
      "\u001b[35m51%|█████▏    | 139/270 [1:20:49<1:15:06, 34.40s/it]\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 139/270 [1:20:48<1:15:06, 34.40s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0289, 'learning_rate': 0.00010153846153846153, 'epoch': 1.54}\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 139/270 [1:20:48<1:15:06, 34.40s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 140/270 [1:21:22<1:14:40, 34.47s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.026, 'learning_rate': 0.00010076923076923077, 'epoch': 1.56}\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 140/270 [1:21:22<1:14:40, 34.47s/it]\u001b[0m\n",
      "\u001b[34m[2023-10-27 23:01:37,951] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:197] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 23:01:37,958] [INFO] [axolotl.utils.dataloader.generate_batches:181] [PID:197] [RANK:0] generating packed batches#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 23:01:37,959] [INFO] [axolotl.utils.dataloader.generate_batches:187] [PID:197] [RANK:0] a3efec085e0760812a351b73f7cdffc9a801ed4832469f9f56e2c45c91279c6e#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 23:01:37,961] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:197] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[35m52%|█████▏    | 140/270 [1:21:23<1:14:40, 34.47s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.026, 'learning_rate': 0.00010076923076923077, 'epoch': 1.56}\u001b[0m\n",
      "\u001b[35m52%|█████▏    | 140/270 [1:21:23<1:14:40, 34.47s/it]\u001b[0m\n",
      "\u001b[35m[2023-10-27 23:01:37,950] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:196] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 23:01:37,958] [INFO] [axolotl.utils.dataloader.generate_batches:181] [PID:196] [RANK:0] generating packed batches#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 23:01:37,958] [INFO] [axolotl.utils.dataloader.generate_batches:187] [PID:196] [RANK:0] b68612dda09b7da3bd0173ed59f621c8c8f01237e9f3dcbc6d7b7c116d4e5e0c#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 23:01:37,960] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:196] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 23:01:40,454] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:196] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 23:01:40,455] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:196] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[35m0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[2023-10-27 23:01:40,456] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:197] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 23:01:40,456] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:197] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[34m0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[2023-10-27 23:01:43,018] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:197] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[34m100%|██████████| 2/2 [00:02<00:00,  1.28s/it]\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.022682638838887215, 'eval_runtime': 5.1658, 'eval_samples_per_second': 152.153, 'eval_steps_per_second': 38.135, 'epoch': 1.56}\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 140/270 [1:21:28<1:14:40, 34.47s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 2/2 [00:02<00:00,  1.28s/it]\u001b[0m\n",
      "\u001b[35m[2023-10-27 23:01:43,019] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:196] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[35m100%|██████████| 2/2 [00:02<00:00,  1.28s/it]\u001b[0m\n",
      "\u001b[35m{'eval_loss': 0.022682638838887215, 'eval_runtime': 5.1664, 'eval_samples_per_second': 152.138, 'eval_steps_per_second': 38.131, 'epoch': 1.56}\u001b[0m\n",
      "\u001b[35m52%|█████▏    | 140/270 [1:21:28<1:14:40, 34.47s/it]\u001b[0m\n",
      "\u001b[35m100%|██████████| 2/2 [00:02<00:00,  1.28s/it]\u001b[0m\n",
      "\u001b[35m52%|█████▏    | 141/270 [1:22:03<1:17:23, 36.00s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0269, 'learning_rate': 0.0001, 'epoch': 1.57}\u001b[0m\n",
      "\u001b[35m52%|█████▏    | 141/270 [1:22:03<1:17:23, 36.00s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 141/270 [1:22:02<1:17:23, 36.00s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0269, 'learning_rate': 0.0001, 'epoch': 1.57}\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 141/270 [1:22:02<1:17:23, 36.00s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 142/270 [1:22:36<1:15:45, 35.51s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0273, 'learning_rate': 9.923076923076923e-05, 'epoch': 1.58}\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 142/270 [1:22:36<1:15:45, 35.51s/it]\u001b[0m\n",
      "\u001b[35m53%|█████▎    | 142/270 [1:22:37<1:15:45, 35.51s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0273, 'learning_rate': 9.923076923076923e-05, 'epoch': 1.58}\u001b[0m\n",
      "\u001b[35m53%|█████▎    | 142/270 [1:22:37<1:15:45, 35.51s/it]\u001b[0m\n",
      "\u001b[35m53%|█████▎    | 143/270 [1:23:12<1:14:27, 35.18s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0258, 'learning_rate': 9.846153846153848e-05, 'epoch': 1.59}\u001b[0m\n",
      "\u001b[35m53%|█████▎    | 143/270 [1:23:12<1:14:27, 35.18s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 143/270 [1:23:11<1:14:27, 35.18s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0258, 'learning_rate': 9.846153846153848e-05, 'epoch': 1.59}\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 143/270 [1:23:11<1:14:27, 35.18s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 144/270 [1:23:45<1:13:22, 34.94s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0265, 'learning_rate': 9.76923076923077e-05, 'epoch': 1.6}\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 144/270 [1:23:45<1:13:22, 34.94s/it]\u001b[0m\n",
      "\u001b[35m53%|█████▎    | 144/270 [1:23:46<1:13:22, 34.94s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0265, 'learning_rate': 9.76923076923077e-05, 'epoch': 1.6}\u001b[0m\n",
      "\u001b[35m53%|█████▎    | 144/270 [1:23:46<1:13:22, 34.94s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 145/270 [1:24:20<1:12:27, 34.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.026, 'learning_rate': 9.692307692307692e-05, 'epoch': 1.61}\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 145/270 [1:24:20<1:12:27, 34.78s/it]\u001b[0m\n",
      "\u001b[35m54%|█████▎    | 145/270 [1:24:20<1:12:27, 34.78s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.026, 'learning_rate': 9.692307692307692e-05, 'epoch': 1.61}\u001b[0m\n",
      "\u001b[35m54%|█████▎    | 145/270 [1:24:20<1:12:27, 34.78s/it]\u001b[0m\n",
      "\u001b[35m54%|█████▍    | 146/270 [1:24:55<1:11:38, 34.66s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0286, 'learning_rate': 9.615384615384617e-05, 'epoch': 1.62}\u001b[0m\n",
      "\u001b[35m54%|█████▍    | 146/270 [1:24:55<1:11:38, 34.66s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 146/270 [1:24:54<1:11:38, 34.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0286, 'learning_rate': 9.615384615384617e-05, 'epoch': 1.62}\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 146/270 [1:24:54<1:11:38, 34.66s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 147/270 [1:25:28<1:10:53, 34.58s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0241, 'learning_rate': 9.53846153846154e-05, 'epoch': 1.63}\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 147/270 [1:25:28<1:10:53, 34.58s/it]\u001b[0m\n",
      "\u001b[35m54%|█████▍    | 147/270 [1:25:29<1:10:53, 34.58s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0241, 'learning_rate': 9.53846153846154e-05, 'epoch': 1.63}\u001b[0m\n",
      "\u001b[35m54%|█████▍    | 147/270 [1:25:29<1:10:53, 34.58s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 148/270 [1:26:03<1:10:12, 34.53s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0301, 'learning_rate': 9.461538461538461e-05, 'epoch': 1.64}\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 148/270 [1:26:03<1:10:12, 34.53s/it]\u001b[0m\n",
      "\u001b[35m55%|█████▍    | 148/270 [1:26:04<1:10:12, 34.53s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0301, 'learning_rate': 9.461538461538461e-05, 'epoch': 1.64}\u001b[0m\n",
      "\u001b[35m55%|█████▍    | 148/270 [1:26:04<1:10:12, 34.53s/it]\u001b[0m\n",
      "\u001b[35m55%|█████▌    | 149/270 [1:26:38<1:09:33, 34.49s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0265, 'learning_rate': 9.384615384615386e-05, 'epoch': 1.66}\u001b[0m\n",
      "\u001b[35m55%|█████▌    | 149/270 [1:26:38<1:09:33, 34.49s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 149/270 [1:26:37<1:09:33, 34.49s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0265, 'learning_rate': 9.384615384615386e-05, 'epoch': 1.66}\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 149/270 [1:26:37<1:09:33, 34.49s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 150/270 [1:27:12<1:08:55, 34.46s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0278, 'learning_rate': 9.307692307692309e-05, 'epoch': 1.67}\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 150/270 [1:27:12<1:08:55, 34.46s/it]\u001b[0m\n",
      "\u001b[35m56%|█████▌    | 150/270 [1:27:12<1:08:55, 34.46s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0278, 'learning_rate': 9.307692307692309e-05, 'epoch': 1.67}\u001b[0m\n",
      "\u001b[35m56%|█████▌    | 150/270 [1:27:12<1:08:55, 34.46s/it]\u001b[0m\n",
      "\u001b[35m56%|█████▌    | 151/270 [1:27:47<1:08:18, 34.44s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0252, 'learning_rate': 9.230769230769232e-05, 'epoch': 1.68}\u001b[0m\n",
      "\u001b[35m56%|█████▌    | 151/270 [1:27:47<1:08:18, 34.44s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 151/270 [1:27:46<1:08:18, 34.44s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0252, 'learning_rate': 9.230769230769232e-05, 'epoch': 1.68}\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 151/270 [1:27:46<1:08:18, 34.44s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 152/270 [1:28:21<1:07:50, 34.49s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0266, 'learning_rate': 9.153846153846155e-05, 'epoch': 1.69}\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 152/270 [1:28:21<1:07:50, 34.49s/it]\u001b[0m\n",
      "\u001b[35m56%|█████▋    | 152/270 [1:28:21<1:07:50, 34.49s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0266, 'learning_rate': 9.153846153846155e-05, 'epoch': 1.69}\u001b[0m\n",
      "\u001b[35m56%|█████▋    | 152/270 [1:28:21<1:07:50, 34.49s/it]\u001b[0m\n",
      "\u001b[35m57%|█████▋    | 153/270 [1:28:56<1:07:12, 34.46s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0263, 'learning_rate': 9.076923076923078e-05, 'epoch': 1.7}\u001b[0m\n",
      "\u001b[35m57%|█████▋    | 153/270 [1:28:56<1:07:12, 34.46s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 153/270 [1:28:55<1:07:12, 34.46s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0263, 'learning_rate': 9.076923076923078e-05, 'epoch': 1.7}\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 153/270 [1:28:55<1:07:12, 34.46s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 154/270 [1:29:29<1:06:35, 34.44s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0256, 'learning_rate': 9e-05, 'epoch': 1.71}\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 154/270 [1:29:29<1:06:35, 34.44s/it]\u001b[0m\n",
      "\u001b[35m57%|█████▋    | 154/270 [1:29:30<1:06:35, 34.44s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0256, 'learning_rate': 9e-05, 'epoch': 1.71}\u001b[0m\n",
      "\u001b[35m57%|█████▋    | 154/270 [1:29:30<1:06:35, 34.44s/it]\u001b[0m\n",
      "\u001b[35m57%|█████▋    | 155/270 [1:30:05<1:06:05, 34.48s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0268, 'learning_rate': 8.923076923076924e-05, 'epoch': 1.72}\u001b[0m\n",
      "\u001b[35m57%|█████▋    | 155/270 [1:30:05<1:06:05, 34.48s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 155/270 [1:30:04<1:06:05, 34.48s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0268, 'learning_rate': 8.923076923076924e-05, 'epoch': 1.72}\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 155/270 [1:30:04<1:06:05, 34.48s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 156/270 [1:30:38<1:05:28, 34.46s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0327, 'learning_rate': 8.846153846153847e-05, 'epoch': 1.73}\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 156/270 [1:30:38<1:05:28, 34.46s/it]\u001b[0m\n",
      "\u001b[35m58%|█████▊    | 156/270 [1:30:39<1:05:28, 34.46s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0327, 'learning_rate': 8.846153846153847e-05, 'epoch': 1.73}\u001b[0m\n",
      "\u001b[35m58%|█████▊    | 156/270 [1:30:39<1:05:28, 34.46s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 157/270 [1:31:13<1:04:51, 34.44s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0275, 'learning_rate': 8.76923076923077e-05, 'epoch': 1.74}\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 157/270 [1:31:13<1:04:51, 34.44s/it]\u001b[0m\n",
      "\u001b[35m58%|█████▊    | 157/270 [1:31:14<1:04:51, 34.44s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0275, 'learning_rate': 8.76923076923077e-05, 'epoch': 1.74}\u001b[0m\n",
      "\u001b[35m58%|█████▊    | 157/270 [1:31:14<1:04:51, 34.44s/it]\u001b[0m\n",
      "\u001b[35m59%|█████▊    | 158/270 [1:31:48<1:04:15, 34.43s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0262, 'learning_rate': 8.692307692307692e-05, 'epoch': 1.76}\u001b[0m\n",
      "\u001b[35m59%|█████▊    | 158/270 [1:31:48<1:04:15, 34.43s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▊    | 158/270 [1:31:47<1:04:15, 34.43s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0262, 'learning_rate': 8.692307692307692e-05, 'epoch': 1.76}\u001b[0m\n",
      "\u001b[34m59%|█████▊    | 158/270 [1:31:47<1:04:15, 34.43s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 159/270 [1:32:22<1:03:47, 34.48s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0255, 'learning_rate': 8.615384615384617e-05, 'epoch': 1.77}\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 159/270 [1:32:22<1:03:47, 34.48s/it]\u001b[0m\n",
      "\u001b[35m59%|█████▉    | 159/270 [1:32:23<1:03:47, 34.48s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0255, 'learning_rate': 8.615384615384617e-05, 'epoch': 1.77}\u001b[0m\n",
      "\u001b[35m59%|█████▉    | 159/270 [1:32:23<1:03:47, 34.48s/it]\u001b[0m\n",
      "\u001b[35m59%|█████▉    | 160/270 [1:32:57<1:03:10, 34.46s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0261, 'learning_rate': 8.538461538461538e-05, 'epoch': 1.78}\u001b[0m\n",
      "\u001b[35m59%|█████▉    | 160/270 [1:32:57<1:03:10, 34.46s/it]\u001b[0m\n",
      "\u001b[35m[2023-10-27 23:13:11,645] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:196] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 23:13:11,653] [INFO] [axolotl.utils.dataloader.generate_batches:181] [PID:196] [RANK:0] generating packed batches#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 23:13:11,653] [INFO] [axolotl.utils.dataloader.generate_batches:187] [PID:196] [RANK:0] b68612dda09b7da3bd0173ed59f621c8c8f01237e9f3dcbc6d7b7c116d4e5e0c#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 23:13:11,655] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:196] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 160/270 [1:32:56<1:03:10, 34.46s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0261, 'learning_rate': 8.538461538461538e-05, 'epoch': 1.78}\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 160/270 [1:32:56<1:03:10, 34.46s/it]\u001b[0m\n",
      "\u001b[34m[2023-10-27 23:13:11,647] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:197] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 23:13:11,656] [INFO] [axolotl.utils.dataloader.generate_batches:181] [PID:197] [RANK:0] generating packed batches#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 23:13:11,657] [INFO] [axolotl.utils.dataloader.generate_batches:187] [PID:197] [RANK:0] a3efec085e0760812a351b73f7cdffc9a801ed4832469f9f56e2c45c91279c6e#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 23:13:11,659] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:197] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 23:13:14,162] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:197] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 23:13:14,162] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:197] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[34m0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[2023-10-27 23:13:14,150] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:196] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 23:13:14,150] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:196] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[35m0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[2023-10-27 23:13:16,725] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:196] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[35m100%|██████████| 2/2 [00:02<00:00,  1.29s/it]\u001b[0m\n",
      "\u001b[35m{'eval_loss': 0.022325271740555763, 'eval_runtime': 5.1781, 'eval_samples_per_second': 151.794, 'eval_steps_per_second': 38.045, 'epoch': 1.78}\u001b[0m\n",
      "\u001b[35m59%|█████▉    | 160/270 [1:33:02<1:03:10, 34.46s/it]\u001b[0m\n",
      "\u001b[35m100%|██████████| 2/2 [00:02<00:00,  1.29s/it]\u001b[0m\n",
      "\u001b[34m[2023-10-27 23:13:16,725] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:197] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[34m100%|██████████| 2/2 [00:02<00:00,  1.28s/it]\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.022325271740555763, 'eval_runtime': 5.1766, 'eval_samples_per_second': 151.836, 'eval_steps_per_second': 38.056, 'epoch': 1.78}\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 160/270 [1:33:01<1:03:10, 34.46s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 2/2 [00:02<00:00,  1.28s/it]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 161/270 [1:33:36<1:05:23, 35.99s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0273, 'learning_rate': 8.461538461538461e-05, 'epoch': 1.79}\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 161/270 [1:33:36<1:05:23, 35.99s/it]\u001b[0m\n",
      "\u001b[35m60%|█████▉    | 161/270 [1:33:37<1:05:23, 35.99s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0273, 'learning_rate': 8.461538461538461e-05, 'epoch': 1.79}\u001b[0m\n",
      "\u001b[35m60%|█████▉    | 161/270 [1:33:37<1:05:23, 35.99s/it]\u001b[0m\n",
      "\u001b[35m60%|██████    | 162/270 [1:34:11<1:03:55, 35.51s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0251, 'learning_rate': 8.384615384615386e-05, 'epoch': 1.8}\u001b[0m\n",
      "\u001b[35m60%|██████    | 162/270 [1:34:11<1:03:55, 35.51s/it]\u001b[0m\n",
      "\u001b[34m60%|██████    | 162/270 [1:34:10<1:03:55, 35.51s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0251, 'learning_rate': 8.384615384615386e-05, 'epoch': 1.8}\u001b[0m\n",
      "\u001b[34m60%|██████    | 162/270 [1:34:10<1:03:55, 35.51s/it]\u001b[0m\n",
      "\u001b[34m60%|██████    | 163/270 [1:34:45<1:02:43, 35.18s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0232, 'learning_rate': 8.307692307692309e-05, 'epoch': 1.81}\u001b[0m\n",
      "\u001b[34m60%|██████    | 163/270 [1:34:45<1:02:43, 35.18s/it]\u001b[0m\n",
      "\u001b[35m60%|██████    | 163/270 [1:34:45<1:02:43, 35.18s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0232, 'learning_rate': 8.307692307692309e-05, 'epoch': 1.81}\u001b[0m\n",
      "\u001b[35m60%|██████    | 163/270 [1:34:45<1:02:43, 35.18s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 164/270 [1:35:19<1:01:43, 34.94s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0257, 'learning_rate': 8.23076923076923e-05, 'epoch': 1.82}\u001b[0m\n",
      "\u001b[34m61%|██████    | 164/270 [1:35:19<1:01:43, 34.94s/it]\u001b[0m\n",
      "\u001b[35m61%|██████    | 164/270 [1:35:20<1:01:43, 34.94s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0257, 'learning_rate': 8.23076923076923e-05, 'epoch': 1.82}\u001b[0m\n",
      "\u001b[35m61%|██████    | 164/270 [1:35:20<1:01:43, 34.94s/it]\u001b[0m\n",
      "\u001b[35m61%|██████    | 165/270 [1:35:54<1:00:51, 34.78s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0254, 'learning_rate': 8.153846153846155e-05, 'epoch': 1.83}\u001b[0m\n",
      "\u001b[35m61%|██████    | 165/270 [1:35:54<1:00:51, 34.78s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 165/270 [1:35:53<1:00:51, 34.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0254, 'learning_rate': 8.153846153846155e-05, 'epoch': 1.83}\u001b[0m\n",
      "\u001b[34m61%|██████    | 165/270 [1:35:53<1:00:51, 34.78s/it]\u001b[0m\n",
      "\u001b[34m61%|██████▏   | 166/270 [1:36:28<1:00:04, 34.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0245, 'learning_rate': 8.076923076923078e-05, 'epoch': 1.84}\u001b[0m\n",
      "\u001b[34m61%|██████▏   | 166/270 [1:36:28<1:00:04, 34.66s/it]\u001b[0m\n",
      "\u001b[35m61%|██████▏   | 166/270 [1:36:29<1:00:04, 34.66s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0245, 'learning_rate': 8.076923076923078e-05, 'epoch': 1.84}\u001b[0m\n",
      "\u001b[35m61%|██████▏   | 166/270 [1:36:29<1:00:04, 34.66s/it]\u001b[0m\n",
      "\u001b[35m62%|██████▏   | 167/270 [1:37:03<59:22, 34.58s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0214, 'learning_rate': 8e-05, 'epoch': 1.86}\u001b[0m\n",
      "\u001b[35m62%|██████▏   | 167/270 [1:37:03<59:22, 34.58s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 167/270 [1:37:02<59:22, 34.58s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0214, 'learning_rate': 8e-05, 'epoch': 1.86}\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 167/270 [1:37:02<59:22, 34.58s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 168/270 [1:37:36<58:41, 34.53s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0271, 'learning_rate': 7.923076923076924e-05, 'epoch': 1.87}\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 168/270 [1:37:36<58:41, 34.53s/it]\u001b[0m\n",
      "\u001b[35m62%|██████▏   | 168/270 [1:37:37<58:42, 34.53s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0271, 'learning_rate': 7.923076923076924e-05, 'epoch': 1.87}\u001b[0m\n",
      "\u001b[35m62%|██████▏   | 168/270 [1:37:37<58:42, 34.53s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 169/270 [1:38:11<58:03, 34.49s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0286, 'learning_rate': 7.846153846153847e-05, 'epoch': 1.88}\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 169/270 [1:38:11<58:03, 34.49s/it]\u001b[0m\n",
      "\u001b[35m63%|██████▎   | 169/270 [1:38:12<58:03, 34.49s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0286, 'learning_rate': 7.846153846153847e-05, 'epoch': 1.88}\u001b[0m\n",
      "\u001b[35m63%|██████▎   | 169/270 [1:38:12<58:03, 34.49s/it]\u001b[0m\n",
      "\u001b[35m63%|██████▎   | 170/270 [1:38:46<57:26, 34.47s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0242, 'learning_rate': 7.76923076923077e-05, 'epoch': 1.89}\u001b[0m\n",
      "\u001b[35m63%|██████▎   | 170/270 [1:38:46<57:26, 34.47s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 170/270 [1:38:45<57:26, 34.47s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0242, 'learning_rate': 7.76923076923077e-05, 'epoch': 1.89}\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 170/270 [1:38:45<57:26, 34.47s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 171/270 [1:39:20<56:49, 34.44s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0209, 'learning_rate': 7.692307692307693e-05, 'epoch': 1.9}\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 171/270 [1:39:20<56:49, 34.44s/it]\u001b[0m\n",
      "\u001b[35m63%|██████▎   | 171/270 [1:39:21<56:49, 34.44s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0209, 'learning_rate': 7.692307692307693e-05, 'epoch': 1.9}\u001b[0m\n",
      "\u001b[35m63%|██████▎   | 171/270 [1:39:21<56:49, 34.44s/it]\u001b[0m\n",
      "\u001b[35m64%|██████▎   | 172/270 [1:39:55<56:19, 34.49s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0273, 'learning_rate': 7.615384615384616e-05, 'epoch': 1.91}\u001b[0m\n",
      "\u001b[35m64%|██████▎   | 172/270 [1:39:55<56:19, 34.49s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▎   | 172/270 [1:39:54<56:19, 34.49s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0273, 'learning_rate': 7.615384615384616e-05, 'epoch': 1.91}\u001b[0m\n",
      "\u001b[34m64%|██████▎   | 172/270 [1:39:54<56:19, 34.49s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 173/270 [1:40:29<55:42, 34.46s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0243, 'learning_rate': 7.538461538461539e-05, 'epoch': 1.92}\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 173/270 [1:40:29<55:42, 34.46s/it]\u001b[0m\n",
      "\u001b[35m64%|██████▍   | 173/270 [1:40:30<55:42, 34.46s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0243, 'learning_rate': 7.538461538461539e-05, 'epoch': 1.92}\u001b[0m\n",
      "\u001b[35m64%|██████▍   | 173/270 [1:40:30<55:42, 34.46s/it]\u001b[0m\n",
      "\u001b[35m64%|██████▍   | 174/270 [1:41:04<55:05, 34.44s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0267, 'learning_rate': 7.461538461538462e-05, 'epoch': 1.93}\u001b[0m\n",
      "\u001b[35m64%|██████▍   | 174/270 [1:41:04<55:05, 34.44s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 174/270 [1:41:03<55:05, 34.44s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0267, 'learning_rate': 7.461538461538462e-05, 'epoch': 1.93}\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 174/270 [1:41:03<55:05, 34.44s/it]\u001b[0m\n",
      "\u001b[35m65%|██████▍   | 175/270 [1:41:38<54:30, 34.42s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0198, 'learning_rate': 7.384615384615386e-05, 'epoch': 1.94}\u001b[0m\n",
      "\u001b[35m65%|██████▍   | 175/270 [1:41:38<54:30, 34.42s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 175/270 [1:41:37<54:30, 34.42s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0198, 'learning_rate': 7.384615384615386e-05, 'epoch': 1.94}\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 175/270 [1:41:37<54:30, 34.42s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 176/270 [1:42:12<53:54, 34.41s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0229, 'learning_rate': 7.307692307692307e-05, 'epoch': 1.96}\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 176/270 [1:42:12<53:54, 34.41s/it]\u001b[0m\n",
      "\u001b[35m65%|██████▌   | 176/270 [1:42:13<53:54, 34.41s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0229, 'learning_rate': 7.307692307692307e-05, 'epoch': 1.96}\u001b[0m\n",
      "\u001b[35m65%|██████▌   | 176/270 [1:42:13<53:54, 34.41s/it]\u001b[0m\n",
      "\u001b[35m66%|██████▌   | 177/270 [1:42:47<53:19, 34.41s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0239, 'learning_rate': 7.23076923076923e-05, 'epoch': 1.97}\u001b[0m\n",
      "\u001b[35m66%|██████▌   | 177/270 [1:42:47<53:19, 34.41s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 177/270 [1:42:46<53:19, 34.41s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0239, 'learning_rate': 7.23076923076923e-05, 'epoch': 1.97}\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 177/270 [1:42:46<53:19, 34.41s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 178/270 [1:43:21<52:45, 34.41s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0267, 'learning_rate': 7.153846153846155e-05, 'epoch': 1.98}\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 178/270 [1:43:21<52:45, 34.41s/it]\u001b[0m\n",
      "\u001b[35m66%|██████▌   | 178/270 [1:43:21<52:45, 34.41s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0267, 'learning_rate': 7.153846153846155e-05, 'epoch': 1.98}\u001b[0m\n",
      "\u001b[35m66%|██████▌   | 178/270 [1:43:21<52:45, 34.41s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▋   | 179/270 [1:43:55<52:10, 34.40s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0255, 'learning_rate': 7.076923076923078e-05, 'epoch': 1.99}\u001b[0m\n",
      "\u001b[34m66%|██████▋   | 179/270 [1:43:55<52:10, 34.40s/it]\u001b[0m\n",
      "\u001b[35m66%|██████▋   | 179/270 [1:43:56<52:10, 34.40s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0255, 'learning_rate': 7.076923076923078e-05, 'epoch': 1.99}\u001b[0m\n",
      "\u001b[35m66%|██████▋   | 179/270 [1:43:56<52:10, 34.40s/it]\u001b[0m\n",
      "\u001b[35m67%|██████▋   | 180/270 [1:44:30<51:35, 34.40s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0236, 'learning_rate': 7e-05, 'epoch': 2.0}\u001b[0m\n",
      "\u001b[35m67%|██████▋   | 180/270 [1:44:30<51:35, 34.40s/it]\u001b[0m\n",
      "\u001b[35m[2023-10-27 23:24:44,923] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:196] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 23:24:44,931] [INFO] [axolotl.utils.dataloader.generate_batches:181] [PID:196] [RANK:0] generating packed batches#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 23:24:44,931] [INFO] [axolotl.utils.dataloader.generate_batches:187] [PID:196] [RANK:0] b68612dda09b7da3bd0173ed59f621c8c8f01237e9f3dcbc6d7b7c116d4e5e0c#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 23:24:44,933] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:196] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 180/270 [1:44:29<51:35, 34.40s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0236, 'learning_rate': 7e-05, 'epoch': 2.0}\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 180/270 [1:44:29<51:35, 34.40s/it]\u001b[0m\n",
      "\u001b[34m[2023-10-27 23:24:44,924] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:197] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 23:24:44,934] [INFO] [axolotl.utils.dataloader.generate_batches:181] [PID:197] [RANK:0] generating packed batches#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 23:24:44,934] [INFO] [axolotl.utils.dataloader.generate_batches:187] [PID:197] [RANK:0] a3efec085e0760812a351b73f7cdffc9a801ed4832469f9f56e2c45c91279c6e#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 23:24:44,937] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:197] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 23:24:47,438] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:197] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 23:24:47,438] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:197] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[34m0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[2023-10-27 23:24:47,428] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:196] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 23:24:47,428] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:196] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[35m0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[2023-10-27 23:24:50,001] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:197] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[34m100%|██████████| 2/2 [00:02<00:00,  1.28s/it]\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.021645627915859222, 'eval_runtime': 5.1755, 'eval_samples_per_second': 151.87, 'eval_steps_per_second': 38.064, 'epoch': 2.0}\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 180/270 [1:44:35<51:35, 34.40s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 2/2 [00:02<00:00,  1.28s/it]\u001b[0m\n",
      "\u001b[35m[2023-10-27 23:24:50,001] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:196] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[35m100%|██████████| 2/2 [00:02<00:00,  1.29s/it]\u001b[0m\n",
      "\u001b[35m{'eval_loss': 0.021645627915859222, 'eval_runtime': 5.1764, 'eval_samples_per_second': 151.842, 'eval_steps_per_second': 38.057, 'epoch': 2.0}\u001b[0m\n",
      "\u001b[35m67%|██████▋   | 180/270 [1:44:35<51:35, 34.40s/it]\u001b[0m\n",
      "\u001b[35m100%|██████████| 2/2 [00:02<00:00,  1.29s/it]\u001b[0m\n",
      "\u001b[35m[2023-10-27 23:25:22,749] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:196] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 2990518#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 23:25:22,749] [INFO] [axolotl.utils.dataloader.__iter__:213] [PID:196] [RANK:0] calling sampler.set_epoch(3)#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 23:25:22,749] [INFO] [axolotl.utils.dataloader.generate_batches:181] [PID:196] [RANK:0] generating packed batches#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 23:25:22,759] [INFO] [axolotl.utils.dataloader.generate_batches:187] [PID:196] [RANK:0] 3604631016ab13df47f57468943a29def62632cb7b73b74ca5ad3f6a2a191e78#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 23:25:22,940] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:196] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 2990518#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 23:25:22,754] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:197] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 2990518#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 23:25:22,754] [INFO] [axolotl.utils.dataloader.__iter__:213] [PID:197] [RANK:0] calling sampler.set_epoch(3)#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 23:25:22,754] [INFO] [axolotl.utils.dataloader.generate_batches:181] [PID:197] [RANK:0] generating packed batches#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 23:25:22,766] [INFO] [axolotl.utils.dataloader.generate_batches:187] [PID:197] [RANK:0] 05da74cd2ebb09fc451a341ebb88666c847b2cdb14fc549fd3690542d44cd6c7#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 23:25:22,947] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:197] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 2990518#033[39m\u001b[0m\n",
      "\u001b[35m67%|██████▋   | 181/270 [1:45:43<1:07:58, 45.82s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0202, 'learning_rate': 6.923076923076924e-05, 'epoch': 2.01}\u001b[0m\n",
      "\u001b[35m67%|██████▋   | 181/270 [1:45:43<1:07:58, 45.82s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 181/270 [1:45:42<1:07:58, 45.82s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 181/270 [1:45:42<1:07:58, 45.82s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0202, 'learning_rate': 6.923076923076924e-05, 'epoch': 2.01}\u001b[0m\n",
      "\u001b[35m67%|██████▋   | 182/270 [1:46:17<1:02:09, 42.38s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.022, 'learning_rate': 6.846153846153847e-05, 'epoch': 2.02}\u001b[0m\n",
      "\u001b[35m67%|██████▋   | 182/270 [1:46:17<1:02:09, 42.38s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 182/270 [1:46:16<1:02:09, 42.38s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.022, 'learning_rate': 6.846153846153847e-05, 'epoch': 2.02}\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 182/270 [1:46:16<1:02:09, 42.38s/it]\u001b[0m\n",
      "\u001b[35m68%|██████▊   | 183/270 [1:46:51<57:57, 39.97s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0227, 'learning_rate': 6.76923076923077e-05, 'epoch': 2.03}\u001b[0m\n",
      "\u001b[35m68%|██████▊   | 183/270 [1:46:51<57:57, 39.97s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 183/270 [1:46:51<57:57, 39.97s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0227, 'learning_rate': 6.76923076923077e-05, 'epoch': 2.03}\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 183/270 [1:46:51<57:57, 39.97s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 184/270 [1:47:25<54:53, 38.30s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0214, 'learning_rate': 6.692307692307693e-05, 'epoch': 2.04}\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 184/270 [1:47:25<54:53, 38.30s/it]\u001b[0m\n",
      "\u001b[35m68%|██████▊   | 184/270 [1:47:26<54:53, 38.30s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0214, 'learning_rate': 6.692307692307693e-05, 'epoch': 2.04}\u001b[0m\n",
      "\u001b[35m68%|██████▊   | 184/270 [1:47:26<54:53, 38.30s/it]\u001b[0m\n",
      "\u001b[35m69%|██████▊   | 185/270 [1:48:00<52:35, 37.12s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0231, 'learning_rate': 6.615384615384616e-05, 'epoch': 2.06}\u001b[0m\n",
      "\u001b[35m69%|██████▊   | 185/270 [1:48:00<52:35, 37.12s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▊   | 185/270 [1:47:59<52:35, 37.12s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0231, 'learning_rate': 6.615384615384616e-05, 'epoch': 2.06}\u001b[0m\n",
      "\u001b[34m69%|██████▊   | 185/270 [1:47:59<52:35, 37.12s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 186/270 [1:48:34<50:49, 36.30s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0245, 'learning_rate': 6.538461538461539e-05, 'epoch': 2.07}\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 186/270 [1:48:34<50:49, 36.30s/it]\u001b[0m\n",
      "\u001b[35m69%|██████▉   | 186/270 [1:48:35<50:49, 36.30s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0245, 'learning_rate': 6.538461538461539e-05, 'epoch': 2.07}\u001b[0m\n",
      "\u001b[35m69%|██████▉   | 186/270 [1:48:35<50:49, 36.30s/it]\u001b[0m\n",
      "\u001b[35m69%|██████▉   | 187/270 [1:49:09<49:25, 35.73s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0245, 'learning_rate': 6.461538461538462e-05, 'epoch': 2.08}\u001b[0m\n",
      "\u001b[35m69%|██████▉   | 187/270 [1:49:09<49:25, 35.73s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 187/270 [1:49:08<49:25, 35.73s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0245, 'learning_rate': 6.461538461538462e-05, 'epoch': 2.08}\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 187/270 [1:49:08<49:25, 35.73s/it]\u001b[0m\n",
      "\u001b[35m70%|██████▉   | 188/270 [1:49:43<48:16, 35.33s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0234, 'learning_rate': 6.384615384615385e-05, 'epoch': 2.09}\u001b[0m\n",
      "\u001b[35m70%|██████▉   | 188/270 [1:49:43<48:16, 35.33s/it]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 188/270 [1:49:43<48:16, 35.33s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0234, 'learning_rate': 6.384615384615385e-05, 'epoch': 2.09}\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 188/270 [1:49:43<48:16, 35.33s/it]\u001b[0m\n",
      "\u001b[34m70%|███████   | 189/270 [1:50:17<47:18, 35.05s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0244, 'learning_rate': 6.307692307692308e-05, 'epoch': 2.1}\u001b[0m\n",
      "\u001b[34m70%|███████   | 189/270 [1:50:17<47:18, 35.05s/it]\u001b[0m\n",
      "\u001b[35m70%|███████   | 189/270 [1:50:18<47:18, 35.05s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0244, 'learning_rate': 6.307692307692308e-05, 'epoch': 2.1}\u001b[0m\n",
      "\u001b[35m70%|███████   | 189/270 [1:50:18<47:18, 35.05s/it]\u001b[0m\n",
      "\u001b[35m70%|███████   | 190/270 [1:50:52<46:28, 34.85s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0206, 'learning_rate': 6.23076923076923e-05, 'epoch': 2.11}\u001b[0m\n",
      "\u001b[35m70%|███████   | 190/270 [1:50:52<46:28, 34.85s/it]\u001b[0m\n",
      "\u001b[34m70%|███████   | 190/270 [1:50:51<46:28, 34.85s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0206, 'learning_rate': 6.23076923076923e-05, 'epoch': 2.11}\u001b[0m\n",
      "\u001b[34m70%|███████   | 190/270 [1:50:51<46:28, 34.85s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 191/270 [1:51:26<45:42, 34.72s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.023, 'learning_rate': 6.153846153846155e-05, 'epoch': 2.12}\u001b[0m\n",
      "\u001b[34m71%|███████   | 191/270 [1:51:26<45:42, 34.72s/it]\u001b[0m\n",
      "\u001b[35m71%|███████   | 191/270 [1:51:27<45:42, 34.72s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.023, 'learning_rate': 6.153846153846155e-05, 'epoch': 2.12}\u001b[0m\n",
      "\u001b[35m71%|███████   | 191/270 [1:51:27<45:42, 34.72s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 192/270 [1:52:00<45:00, 34.62s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0248, 'learning_rate': 6.0769230769230765e-05, 'epoch': 2.13}\u001b[0m\n",
      "\u001b[34m71%|███████   | 192/270 [1:52:00<45:00, 34.62s/it]\u001b[0m\n",
      "\u001b[35m71%|███████   | 192/270 [1:52:01<45:00, 34.62s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0248, 'learning_rate': 6.0769230769230765e-05, 'epoch': 2.13}\u001b[0m\n",
      "\u001b[35m71%|███████   | 192/270 [1:52:01<45:00, 34.62s/it]\u001b[0m\n",
      "\u001b[35m71%|███████▏  | 193/270 [1:52:35<44:20, 34.55s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0212, 'learning_rate': 6e-05, 'epoch': 2.14}\u001b[0m\n",
      "\u001b[35m71%|███████▏  | 193/270 [1:52:35<44:20, 34.55s/it]\u001b[0m\n",
      "\u001b[34m71%|███████▏  | 193/270 [1:52:35<44:20, 34.55s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0212, 'learning_rate': 6e-05, 'epoch': 2.14}\u001b[0m\n",
      "\u001b[34m71%|███████▏  | 193/270 [1:52:35<44:20, 34.55s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 194/270 [1:53:09<43:42, 34.50s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0221, 'learning_rate': 5.923076923076923e-05, 'epoch': 2.16}\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 194/270 [1:53:09<43:42, 34.50s/it]\u001b[0m\n",
      "\u001b[35m72%|███████▏  | 194/270 [1:53:10<43:42, 34.50s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0221, 'learning_rate': 5.923076923076923e-05, 'epoch': 2.16}\u001b[0m\n",
      "\u001b[35m72%|███████▏  | 194/270 [1:53:10<43:42, 34.50s/it]\u001b[0m\n",
      "\u001b[35m72%|███████▏  | 195/270 [1:53:44<43:05, 34.47s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0252, 'learning_rate': 5.846153846153847e-05, 'epoch': 2.17}\u001b[0m\n",
      "\u001b[35m72%|███████▏  | 195/270 [1:53:44<43:05, 34.47s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 195/270 [1:53:43<43:05, 34.47s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0252, 'learning_rate': 5.846153846153847e-05, 'epoch': 2.17}\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 195/270 [1:53:43<43:05, 34.47s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 196/270 [1:54:18<42:29, 34.45s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0219, 'learning_rate': 5.769230769230769e-05, 'epoch': 2.18}\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 196/270 [1:54:18<42:29, 34.45s/it]\u001b[0m\n",
      "\u001b[35m73%|███████▎  | 196/270 [1:54:19<42:29, 34.45s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0219, 'learning_rate': 5.769230769230769e-05, 'epoch': 2.18}\u001b[0m\n",
      "\u001b[35m73%|███████▎  | 196/270 [1:54:19<42:29, 34.45s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 197/270 [1:54:52<41:53, 34.43s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0236, 'learning_rate': 5.692307692307692e-05, 'epoch': 2.19}\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 197/270 [1:54:52<41:53, 34.43s/it]\u001b[0m\n",
      "\u001b[35m73%|███████▎  | 197/270 [1:54:53<41:53, 34.43s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0236, 'learning_rate': 5.692307692307692e-05, 'epoch': 2.19}\u001b[0m\n",
      "\u001b[35m73%|███████▎  | 197/270 [1:54:53<41:53, 34.43s/it]\u001b[0m\n",
      "\u001b[35m73%|███████▎  | 198/270 [1:55:27<41:18, 34.42s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0211, 'learning_rate': 5.615384615384616e-05, 'epoch': 2.2}\u001b[0m\n",
      "\u001b[35m73%|███████▎  | 198/270 [1:55:27<41:18, 34.42s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 198/270 [1:55:27<41:18, 34.42s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0211, 'learning_rate': 5.615384615384616e-05, 'epoch': 2.2}\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 198/270 [1:55:27<41:18, 34.42s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▎  | 199/270 [1:56:01<40:43, 34.41s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0221, 'learning_rate': 5.538461538461539e-05, 'epoch': 2.21}\u001b[0m\n",
      "\u001b[34m74%|███████▎  | 199/270 [1:56:01<40:43, 34.41s/it]\u001b[0m\n",
      "\u001b[35m74%|███████▎  | 199/270 [1:56:02<40:43, 34.41s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0221, 'learning_rate': 5.538461538461539e-05, 'epoch': 2.21}\u001b[0m\n",
      "\u001b[35m74%|███████▎  | 199/270 [1:56:02<40:43, 34.41s/it]\u001b[0m\n",
      "\u001b[35m74%|███████▍  | 200/270 [1:56:36<40:08, 34.40s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0211, 'learning_rate': 5.461538461538461e-05, 'epoch': 2.22}\u001b[0m\n",
      "\u001b[35m74%|███████▍  | 200/270 [1:56:36<40:08, 34.40s/it]\u001b[0m\n",
      "\u001b[35m[2023-10-27 23:36:50,774] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:196] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 23:36:50,781] [INFO] [axolotl.utils.dataloader.generate_batches:181] [PID:196] [RANK:0] generating packed batches#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 23:36:50,781] [INFO] [axolotl.utils.dataloader.generate_batches:187] [PID:196] [RANK:0] b68612dda09b7da3bd0173ed59f621c8c8f01237e9f3dcbc6d7b7c116d4e5e0c#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 23:36:50,784] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:196] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 200/270 [1:56:35<40:08, 34.40s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0211, 'learning_rate': 5.461538461538461e-05, 'epoch': 2.22}\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 200/270 [1:56:35<40:08, 34.40s/it]\u001b[0m\n",
      "\u001b[34m[2023-10-27 23:36:50,775] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:197] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 23:36:50,782] [INFO] [axolotl.utils.dataloader.generate_batches:181] [PID:197] [RANK:0] generating packed batches#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 23:36:50,783] [INFO] [axolotl.utils.dataloader.generate_batches:187] [PID:197] [RANK:0] a3efec085e0760812a351b73f7cdffc9a801ed4832469f9f56e2c45c91279c6e#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 23:36:50,785] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:197] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 23:36:53,279] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:197] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 23:36:53,280] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:197] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[34m0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[2023-10-27 23:36:53,278] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:196] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 23:36:53,279] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:196] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[35m0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[2023-10-27 23:36:55,842] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:196] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[35m100%|██████████| 2/2 [00:02<00:00,  1.28s/it]\u001b[0m\n",
      "\u001b[35m{'eval_loss': 0.021089177578687668, 'eval_runtime': 5.1666, 'eval_samples_per_second': 152.13, 'eval_steps_per_second': 38.129, 'epoch': 2.22}\u001b[0m\n",
      "\u001b[35m74%|███████▍  | 200/270 [1:56:41<40:08, 34.40s/it]\u001b[0m\n",
      "\u001b[35m100%|██████████| 2/2 [00:02<00:00,  1.28s/it]\u001b[0m\n",
      "\u001b[34m[2023-10-27 23:36:55,841] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:197] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[34m100%|██████████| 2/2 [00:02<00:00,  1.28s/it]\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.021089177578687668, 'eval_runtime': 5.1661, 'eval_samples_per_second': 152.146, 'eval_steps_per_second': 38.133, 'epoch': 2.22}\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 200/270 [1:56:40<40:08, 34.40s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 2/2 [00:02<00:00,  1.28s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 201/270 [1:57:15<41:20, 35.95s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0224, 'learning_rate': 5.384615384615385e-05, 'epoch': 2.23}\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 201/270 [1:57:15<41:20, 35.95s/it]\u001b[0m\n",
      "\u001b[35m74%|███████▍  | 201/270 [1:57:16<41:20, 35.95s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0224, 'learning_rate': 5.384615384615385e-05, 'epoch': 2.23}\u001b[0m\n",
      "\u001b[35m74%|███████▍  | 201/270 [1:57:16<41:20, 35.95s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 202/270 [1:57:49<40:13, 35.49s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0215, 'learning_rate': 5.3076923076923076e-05, 'epoch': 2.24}\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 202/270 [1:57:49<40:13, 35.49s/it]\u001b[0m\n",
      "\u001b[35m75%|███████▍  | 202/270 [1:57:50<40:13, 35.49s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0215, 'learning_rate': 5.3076923076923076e-05, 'epoch': 2.24}\u001b[0m\n",
      "\u001b[35m75%|███████▍  | 202/270 [1:57:50<40:13, 35.49s/it]\u001b[0m\n",
      "\u001b[35m75%|███████▌  | 203/270 [1:58:24<39:15, 35.16s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0245, 'learning_rate': 5.230769230769231e-05, 'epoch': 2.26}\u001b[0m\n",
      "\u001b[35m75%|███████▌  | 203/270 [1:58:24<39:15, 35.16s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 203/270 [1:58:24<39:15, 35.16s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0245, 'learning_rate': 5.230769230769231e-05, 'epoch': 2.26}\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 203/270 [1:58:24<39:15, 35.16s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 204/270 [1:58:58<38:25, 34.93s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0262, 'learning_rate': 5.1538461538461536e-05, 'epoch': 2.27}\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 204/270 [1:58:58<38:25, 34.93s/it]\u001b[0m\n",
      "\u001b[35m76%|███████▌  | 204/270 [1:58:59<38:25, 34.93s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0262, 'learning_rate': 5.1538461538461536e-05, 'epoch': 2.27}\u001b[0m\n",
      "\u001b[35m76%|███████▌  | 204/270 [1:58:59<38:25, 34.93s/it]\u001b[0m\n",
      "\u001b[35m76%|███████▌  | 205/270 [1:59:33<37:40, 34.77s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0221, 'learning_rate': 5.0769230769230766e-05, 'epoch': 2.28}\u001b[0m\n",
      "\u001b[35m76%|███████▌  | 205/270 [1:59:33<37:40, 34.77s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 205/270 [1:59:32<37:40, 34.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0221, 'learning_rate': 5.0769230769230766e-05, 'epoch': 2.28}\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 205/270 [1:59:32<37:40, 34.77s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▋  | 206/270 [2:00:07<36:58, 34.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0226, 'learning_rate': 5e-05, 'epoch': 2.29}\u001b[0m\n",
      "\u001b[34m76%|███████▋  | 206/270 [2:00:07<36:58, 34.66s/it]\u001b[0m\n",
      "\u001b[35m76%|███████▋  | 206/270 [2:00:08<36:58, 34.66s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0226, 'learning_rate': 5e-05, 'epoch': 2.29}\u001b[0m\n",
      "\u001b[35m76%|███████▋  | 206/270 [2:00:08<36:58, 34.66s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 207/270 [2:00:41<36:18, 34.58s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.024, 'learning_rate': 4.923076923076924e-05, 'epoch': 2.3}\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 207/270 [2:00:41<36:18, 34.58s/it]\u001b[0m\n",
      "\u001b[35m77%|███████▋  | 207/270 [2:00:42<36:18, 34.58s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.024, 'learning_rate': 4.923076923076924e-05, 'epoch': 2.3}\u001b[0m\n",
      "\u001b[35m77%|███████▋  | 207/270 [2:00:42<36:18, 34.58s/it]\u001b[0m\n",
      "\u001b[35m77%|███████▋  | 208/270 [2:01:16<35:40, 34.53s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0217, 'learning_rate': 4.846153846153846e-05, 'epoch': 2.31}\u001b[0m\n",
      "\u001b[35m77%|███████▋  | 208/270 [2:01:16<35:40, 34.53s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 208/270 [2:01:16<35:40, 34.53s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0217, 'learning_rate': 4.846153846153846e-05, 'epoch': 2.31}\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 208/270 [2:01:16<35:40, 34.53s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 209/270 [2:01:50<35:03, 34.49s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0172, 'learning_rate': 4.76923076923077e-05, 'epoch': 2.32}\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 209/270 [2:01:50<35:03, 34.49s/it]\u001b[0m\n",
      "\u001b[35m77%|███████▋  | 209/270 [2:01:51<35:03, 34.49s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0172, 'learning_rate': 4.76923076923077e-05, 'epoch': 2.32}\u001b[0m\n",
      "\u001b[35m77%|███████▋  | 209/270 [2:01:51<35:03, 34.49s/it]\u001b[0m\n",
      "\u001b[35m78%|███████▊  | 210/270 [2:02:25<34:27, 34.46s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0238, 'learning_rate': 4.692307692307693e-05, 'epoch': 2.33}\u001b[0m\n",
      "\u001b[35m78%|███████▊  | 210/270 [2:02:25<34:27, 34.46s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 210/270 [2:02:24<34:27, 34.46s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0238, 'learning_rate': 4.692307692307693e-05, 'epoch': 2.33}\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 210/270 [2:02:24<34:27, 34.46s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 211/270 [2:02:59<33:51, 34.44s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0233, 'learning_rate': 4.615384615384616e-05, 'epoch': 2.34}\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 211/270 [2:02:59<33:51, 34.44s/it]\u001b[0m\n",
      "\u001b[35m78%|███████▊  | 211/270 [2:03:00<33:51, 34.44s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0233, 'learning_rate': 4.615384615384616e-05, 'epoch': 2.34}\u001b[0m\n",
      "\u001b[35m78%|███████▊  | 211/270 [2:03:00<33:51, 34.44s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▊  | 212/270 [2:03:33<33:16, 34.42s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0223, 'learning_rate': 4.538461538461539e-05, 'epoch': 2.36}\u001b[0m\n",
      "\u001b[34m79%|███████▊  | 212/270 [2:03:33<33:16, 34.42s/it]\u001b[0m\n",
      "\u001b[35m79%|███████▊  | 212/270 [2:03:34<33:16, 34.42s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0223, 'learning_rate': 4.538461538461539e-05, 'epoch': 2.36}\u001b[0m\n",
      "\u001b[35m79%|███████▊  | 212/270 [2:03:34<33:16, 34.42s/it]\u001b[0m\n",
      "\u001b[35m79%|███████▉  | 213/270 [2:04:08<32:41, 34.42s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0214, 'learning_rate': 4.461538461538462e-05, 'epoch': 2.37}\u001b[0m\n",
      "\u001b[35m79%|███████▉  | 213/270 [2:04:08<32:41, 34.42s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 213/270 [2:04:08<32:41, 34.41s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0214, 'learning_rate': 4.461538461538462e-05, 'epoch': 2.37}\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 213/270 [2:04:08<32:41, 34.41s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 214/270 [2:04:42<32:06, 34.41s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0226, 'learning_rate': 4.384615384615385e-05, 'epoch': 2.38}\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 214/270 [2:04:42<32:06, 34.41s/it]\u001b[0m\n",
      "\u001b[35m79%|███████▉  | 214/270 [2:04:43<32:06, 34.41s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0226, 'learning_rate': 4.384615384615385e-05, 'epoch': 2.38}\u001b[0m\n",
      "\u001b[35m79%|███████▉  | 214/270 [2:04:43<32:06, 34.41s/it]\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 215/270 [2:05:16<31:32, 34.41s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0177, 'learning_rate': 4.3076923076923084e-05, 'epoch': 2.39}\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 215/270 [2:05:16<31:32, 34.41s/it]\u001b[0m\n",
      "\u001b[35m80%|███████▉  | 215/270 [2:05:17<31:32, 34.41s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0177, 'learning_rate': 4.3076923076923084e-05, 'epoch': 2.39}\u001b[0m\n",
      "\u001b[35m80%|███████▉  | 215/270 [2:05:17<31:32, 34.41s/it]\u001b[0m\n",
      "\u001b[35m80%|████████  | 216/270 [2:05:52<30:57, 34.41s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0214, 'learning_rate': 4.230769230769231e-05, 'epoch': 2.4}\u001b[0m\n",
      "\u001b[35m80%|████████  | 216/270 [2:05:52<30:57, 34.41s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 216/270 [2:05:51<30:57, 34.41s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0214, 'learning_rate': 4.230769230769231e-05, 'epoch': 2.4}\u001b[0m\n",
      "\u001b[34m80%|████████  | 216/270 [2:05:51<30:57, 34.41s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 217/270 [2:06:25<30:23, 34.40s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0212, 'learning_rate': 4.1538461538461544e-05, 'epoch': 2.41}\u001b[0m\n",
      "\u001b[34m80%|████████  | 217/270 [2:06:25<30:23, 34.40s/it]\u001b[0m\n",
      "\u001b[35m80%|████████  | 217/270 [2:06:26<30:23, 34.40s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0212, 'learning_rate': 4.1538461538461544e-05, 'epoch': 2.41}\u001b[0m\n",
      "\u001b[35m80%|████████  | 217/270 [2:06:26<30:23, 34.40s/it]\u001b[0m\n",
      "\u001b[35m81%|████████  | 218/270 [2:07:01<29:52, 34.46s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.023, 'learning_rate': 4.0769230769230773e-05, 'epoch': 2.42}\u001b[0m\n",
      "\u001b[35m81%|████████  | 218/270 [2:07:01<29:52, 34.46s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 218/270 [2:07:00<29:52, 34.46s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.023, 'learning_rate': 4.0769230769230773e-05, 'epoch': 2.42}\u001b[0m\n",
      "\u001b[34m81%|████████  | 218/270 [2:07:00<29:52, 34.46s/it]\u001b[0m\n",
      "\u001b[35m81%|████████  | 219/270 [2:07:35<29:16, 34.44s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0198, 'learning_rate': 4e-05, 'epoch': 2.43}\u001b[0m\n",
      "\u001b[35m81%|████████  | 219/270 [2:07:35<29:16, 34.44s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 219/270 [2:07:34<29:16, 34.44s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0198, 'learning_rate': 4e-05, 'epoch': 2.43}\u001b[0m\n",
      "\u001b[34m81%|████████  | 219/270 [2:07:34<29:16, 34.44s/it]\u001b[0m\n",
      "\u001b[35m81%|████████▏ | 220/270 [2:08:09<28:41, 34.43s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0228, 'learning_rate': 3.923076923076923e-05, 'epoch': 2.44}\u001b[0m\n",
      "\u001b[35m81%|████████▏ | 220/270 [2:08:09<28:41, 34.43s/it]\u001b[0m\n",
      "\u001b[35m[2023-10-27 23:48:24,074] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:196] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 23:48:24,083] [INFO] [axolotl.utils.dataloader.generate_batches:181] [PID:196] [RANK:0] generating packed batches#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 23:48:24,083] [INFO] [axolotl.utils.dataloader.generate_batches:187] [PID:196] [RANK:0] b68612dda09b7da3bd0173ed59f621c8c8f01237e9f3dcbc6d7b7c116d4e5e0c#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 23:48:24,086] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:196] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[34m81%|████████▏ | 220/270 [2:08:09<28:41, 34.43s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0228, 'learning_rate': 3.923076923076923e-05, 'epoch': 2.44}\u001b[0m\n",
      "\u001b[34m81%|████████▏ | 220/270 [2:08:09<28:41, 34.43s/it]\u001b[0m\n",
      "\u001b[34m[2023-10-27 23:48:24,074] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:197] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 23:48:24,082] [INFO] [axolotl.utils.dataloader.generate_batches:181] [PID:197] [RANK:0] generating packed batches#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 23:48:24,082] [INFO] [axolotl.utils.dataloader.generate_batches:187] [PID:197] [RANK:0] a3efec085e0760812a351b73f7cdffc9a801ed4832469f9f56e2c45c91279c6e#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 23:48:24,084] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:197] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 23:48:26,579] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:197] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 23:48:26,579] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:197] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[34m0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[2023-10-27 23:48:26,589] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:196] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 23:48:26,589] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:196] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[35m0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[2023-10-27 23:48:29,151] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:196] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[35m100%|██████████| 2/2 [00:02<00:00,  1.28s/it]\u001b[0m\n",
      "\u001b[35m{'eval_loss': 0.02067030407488346, 'eval_runtime': 5.1758, 'eval_samples_per_second': 151.861, 'eval_steps_per_second': 38.062, 'epoch': 2.44}\u001b[0m\n",
      "\u001b[35m81%|████████▏ | 220/270 [2:08:15<28:41, 34.43s/it]\u001b[0m\n",
      "\u001b[35m100%|██████████| 2/2 [00:02<00:00,  1.28s/it]\u001b[0m\n",
      "\u001b[34m[2023-10-27 23:48:29,151] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:197] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[34m100%|██████████| 2/2 [00:02<00:00,  1.29s/it]\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.02067030407488346, 'eval_runtime': 5.1754, 'eval_samples_per_second': 151.872, 'eval_steps_per_second': 38.064, 'epoch': 2.44}\u001b[0m\n",
      "\u001b[34m81%|████████▏ | 220/270 [2:08:14<28:41, 34.43s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 2/2 [00:02<00:00,  1.29s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 221/270 [2:08:48<29:22, 35.97s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0261, 'learning_rate': 3.846153846153846e-05, 'epoch': 2.46}\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 221/270 [2:08:48<29:22, 35.97s/it]\u001b[0m\n",
      "\u001b[35m82%|████████▏ | 221/270 [2:08:49<29:22, 35.97s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0261, 'learning_rate': 3.846153846153846e-05, 'epoch': 2.46}\u001b[0m\n",
      "\u001b[35m82%|████████▏ | 221/270 [2:08:49<29:22, 35.97s/it]\u001b[0m\n",
      "\u001b[35m82%|████████▏ | 222/270 [2:09:23<28:23, 35.50s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0248, 'learning_rate': 3.769230769230769e-05, 'epoch': 2.47}\u001b[0m\n",
      "\u001b[35m82%|████████▏ | 222/270 [2:09:23<28:23, 35.50s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 222/270 [2:09:23<28:24, 35.50s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0248, 'learning_rate': 3.769230769230769e-05, 'epoch': 2.47}\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 222/270 [2:09:23<28:24, 35.50s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 223/270 [2:09:57<27:33, 35.17s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.023, 'learning_rate': 3.692307692307693e-05, 'epoch': 2.48}\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 223/270 [2:09:57<27:33, 35.17s/it]\u001b[0m\n",
      "\u001b[35m83%|████████▎ | 223/270 [2:09:58<27:33, 35.17s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.023, 'learning_rate': 3.692307692307693e-05, 'epoch': 2.48}\u001b[0m\n",
      "\u001b[35m83%|████████▎ | 223/270 [2:09:58<27:33, 35.17s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 224/270 [2:10:31<26:47, 34.94s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.02, 'learning_rate': 3.615384615384615e-05, 'epoch': 2.49}\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 224/270 [2:10:31<26:47, 34.94s/it]\u001b[0m\n",
      "\u001b[35m83%|████████▎ | 224/270 [2:10:32<26:47, 34.94s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.02, 'learning_rate': 3.615384615384615e-05, 'epoch': 2.49}\u001b[0m\n",
      "\u001b[35m83%|████████▎ | 224/270 [2:10:32<26:47, 34.94s/it]\u001b[0m\n",
      "\u001b[35m83%|████████▎ | 225/270 [2:11:07<26:04, 34.78s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0231, 'learning_rate': 3.538461538461539e-05, 'epoch': 2.5}\u001b[0m\n",
      "\u001b[35m83%|████████▎ | 225/270 [2:11:07<26:04, 34.78s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 225/270 [2:11:06<26:04, 34.78s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0231, 'learning_rate': 3.538461538461539e-05, 'epoch': 2.5}\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 225/270 [2:11:06<26:04, 34.78s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▎ | 226/270 [2:11:40<25:25, 34.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0235, 'learning_rate': 3.461538461538462e-05, 'epoch': 2.51}\u001b[0m\n",
      "\u001b[34m84%|████████▎ | 226/270 [2:11:40<25:25, 34.66s/it]\u001b[0m\n",
      "\u001b[35m84%|████████▎ | 226/270 [2:11:41<25:25, 34.66s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0235, 'learning_rate': 3.461538461538462e-05, 'epoch': 2.51}\u001b[0m\n",
      "\u001b[35m84%|████████▎ | 226/270 [2:11:41<25:25, 34.66s/it]\u001b[0m\n",
      "\u001b[35m84%|████████▍ | 227/270 [2:12:15<24:47, 34.58s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0227, 'learning_rate': 3.384615384615385e-05, 'epoch': 2.52}\u001b[0m\n",
      "\u001b[35m84%|████████▍ | 227/270 [2:12:15<24:47, 34.58s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 227/270 [2:12:15<24:47, 34.59s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0227, 'learning_rate': 3.384615384615385e-05, 'epoch': 2.52}\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 227/270 [2:12:15<24:47, 34.59s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 228/270 [2:12:49<24:10, 34.53s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0216, 'learning_rate': 3.307692307692308e-05, 'epoch': 2.53}\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 228/270 [2:12:49<24:10, 34.53s/it]\u001b[0m\n",
      "\u001b[35m84%|████████▍ | 228/270 [2:12:50<24:10, 34.53s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0216, 'learning_rate': 3.307692307692308e-05, 'epoch': 2.53}\u001b[0m\n",
      "\u001b[35m84%|████████▍ | 228/270 [2:12:50<24:10, 34.53s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 229/270 [2:13:23<23:34, 34.49s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 229/270 [2:13:23<23:34, 34.49s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0213, 'learning_rate': 3.230769230769231e-05, 'epoch': 2.54}\u001b[0m\n",
      "\u001b[35m85%|████████▍ | 229/270 [2:13:24<23:34, 34.49s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0213, 'learning_rate': 3.230769230769231e-05, 'epoch': 2.54}\u001b[0m\n",
      "\u001b[35m85%|████████▍ | 229/270 [2:13:24<23:34, 34.49s/it]\u001b[0m\n",
      "\u001b[35m85%|████████▌ | 230/270 [2:13:59<22:58, 34.47s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0209, 'learning_rate': 3.153846153846154e-05, 'epoch': 2.56}\u001b[0m\n",
      "\u001b[35m85%|████████▌ | 230/270 [2:13:59<22:58, 34.47s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 230/270 [2:13:58<22:58, 34.47s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 230/270 [2:13:58<22:58, 34.47s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0209, 'learning_rate': 3.153846153846154e-05, 'epoch': 2.56}\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 231/270 [2:14:32<22:23, 34.45s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0242, 'learning_rate': 3.0769230769230774e-05, 'epoch': 2.57}\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 231/270 [2:14:32<22:23, 34.45s/it]\u001b[0m\n",
      "\u001b[35m86%|████████▌ | 231/270 [2:14:33<22:23, 34.45s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0242, 'learning_rate': 3.0769230769230774e-05, 'epoch': 2.57}\u001b[0m\n",
      "\u001b[35m86%|████████▌ | 231/270 [2:14:33<22:23, 34.45s/it]\u001b[0m\n",
      "\u001b[35m86%|████████▌ | 232/270 [2:15:07<21:48, 34.43s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0221, 'learning_rate': 3e-05, 'epoch': 2.58}\u001b[0m\n",
      "\u001b[35m86%|████████▌ | 232/270 [2:15:07<21:48, 34.43s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 232/270 [2:15:07<21:48, 34.43s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0221, 'learning_rate': 3e-05, 'epoch': 2.58}\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 232/270 [2:15:07<21:48, 34.43s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▋ | 233/270 [2:15:41<21:13, 34.42s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0183, 'learning_rate': 2.9230769230769234e-05, 'epoch': 2.59}\u001b[0m\n",
      "\u001b[34m86%|████████▋ | 233/270 [2:15:41<21:13, 34.42s/it]\u001b[0m\n",
      "\u001b[35m86%|████████▋ | 233/270 [2:15:42<21:13, 34.42s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0183, 'learning_rate': 2.9230769230769234e-05, 'epoch': 2.59}\u001b[0m\n",
      "\u001b[35m86%|████████▋ | 233/270 [2:15:42<21:13, 34.42s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 234/270 [2:16:15<20:38, 34.41s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0211, 'learning_rate': 2.846153846153846e-05, 'epoch': 2.6}\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 234/270 [2:16:15<20:38, 34.41s/it]\u001b[0m\n",
      "\u001b[35m87%|████████▋ | 234/270 [2:16:16<20:38, 34.42s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0211, 'learning_rate': 2.846153846153846e-05, 'epoch': 2.6}\u001b[0m\n",
      "\u001b[35m87%|████████▋ | 234/270 [2:16:16<20:38, 34.42s/it]\u001b[0m\n",
      "\u001b[35m87%|████████▋ | 235/270 [2:16:51<20:04, 34.41s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0265, 'learning_rate': 2.7692307692307694e-05, 'epoch': 2.61}\u001b[0m\n",
      "\u001b[35m87%|████████▋ | 235/270 [2:16:51<20:04, 34.41s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 235/270 [2:16:50<20:04, 34.41s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0265, 'learning_rate': 2.7692307692307694e-05, 'epoch': 2.61}\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 235/270 [2:16:50<20:04, 34.41s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 236/270 [2:17:24<19:30, 34.44s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0209, 'learning_rate': 2.6923076923076923e-05, 'epoch': 2.62}\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 236/270 [2:17:24<19:30, 34.44s/it]\u001b[0m\n",
      "\u001b[35m87%|████████▋ | 236/270 [2:17:25<19:30, 34.44s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0209, 'learning_rate': 2.6923076923076923e-05, 'epoch': 2.62}\u001b[0m\n",
      "\u001b[35m87%|████████▋ | 236/270 [2:17:25<19:30, 34.44s/it]\u001b[0m\n",
      "\u001b[35m88%|████████▊ | 237/270 [2:18:00<18:56, 34.43s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0258, 'learning_rate': 2.6153846153846157e-05, 'epoch': 2.63}\u001b[0m\n",
      "\u001b[35m88%|████████▊ | 237/270 [2:18:00<18:56, 34.43s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 237/270 [2:17:59<18:56, 34.43s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0258, 'learning_rate': 2.6153846153846157e-05, 'epoch': 2.63}\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 237/270 [2:17:59<18:56, 34.43s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 238/270 [2:18:33<18:21, 34.43s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0244, 'learning_rate': 2.5384615384615383e-05, 'epoch': 2.64}\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 238/270 [2:18:33<18:21, 34.43s/it]\u001b[0m\n",
      "\u001b[35m88%|████████▊ | 238/270 [2:18:34<18:21, 34.43s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0244, 'learning_rate': 2.5384615384615383e-05, 'epoch': 2.64}\u001b[0m\n",
      "\u001b[35m88%|████████▊ | 238/270 [2:18:34<18:21, 34.43s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▊ | 239/270 [2:19:08<17:47, 34.42s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0209, 'learning_rate': 2.461538461538462e-05, 'epoch': 2.66}\u001b[0m\n",
      "\u001b[34m89%|████████▊ | 239/270 [2:19:08<17:47, 34.42s/it]\u001b[0m\n",
      "\u001b[35m89%|████████▊ | 239/270 [2:19:08<17:47, 34.42s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0209, 'learning_rate': 2.461538461538462e-05, 'epoch': 2.66}\u001b[0m\n",
      "\u001b[35m89%|████████▊ | 239/270 [2:19:08<17:47, 34.42s/it]\u001b[0m\n",
      "\u001b[35m89%|████████▉ | 240/270 [2:19:43<17:12, 34.41s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0244, 'learning_rate': 2.384615384615385e-05, 'epoch': 2.67}\u001b[0m\n",
      "\u001b[35m89%|████████▉ | 240/270 [2:19:43<17:12, 34.41s/it]\u001b[0m\n",
      "\u001b[35m[2023-10-27 23:59:57,387] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:196] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 23:59:57,396] [INFO] [axolotl.utils.dataloader.generate_batches:181] [PID:196] [RANK:0] generating packed batches#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 23:59:57,396] [INFO] [axolotl.utils.dataloader.generate_batches:187] [PID:196] [RANK:0] b68612dda09b7da3bd0173ed59f621c8c8f01237e9f3dcbc6d7b7c116d4e5e0c#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 23:59:57,398] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:196] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 240/270 [2:19:42<17:12, 34.41s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0244, 'learning_rate': 2.384615384615385e-05, 'epoch': 2.67}\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 240/270 [2:19:42<17:12, 34.41s/it]\u001b[0m\n",
      "\u001b[34m[2023-10-27 23:59:57,387] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:197] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 23:59:57,395] [INFO] [axolotl.utils.dataloader.generate_batches:181] [PID:197] [RANK:0] generating packed batches#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 23:59:57,395] [INFO] [axolotl.utils.dataloader.generate_batches:187] [PID:197] [RANK:0] a3efec085e0760812a351b73f7cdffc9a801ed4832469f9f56e2c45c91279c6e#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 23:59:57,397] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:197] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 23:59:59,892] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:197] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-27 23:59:59,892] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:197] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[34m0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[2023-10-27 23:59:59,902] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:196] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-27 23:59:59,902] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:196] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[35m0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[2023-10-28 00:00:02,465] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:196] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[35m100%|██████████| 2/2 [00:02<00:00,  1.28s/it]\u001b[0m\n",
      "\u001b[34m[2023-10-28 00:00:02,465] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:197] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[34m100%|██████████| 2/2 [00:02<00:00,  1.29s/it]\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.020027311518788338, 'eval_runtime': 5.1769, 'eval_samples_per_second': 151.83, 'eval_steps_per_second': 38.054, 'epoch': 2.67}\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 240/270 [2:19:47<17:12, 34.41s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 2/2 [00:02<00:00,  1.29s/it]\u001b[0m\n",
      "\u001b[35m{'eval_loss': 0.020027311518788338, 'eval_runtime': 5.1766, 'eval_samples_per_second': 151.837, 'eval_steps_per_second': 38.056, 'epoch': 2.67}\u001b[0m\n",
      "\u001b[35m89%|████████▉ | 240/270 [2:19:48<17:12, 34.41s/it]\u001b[0m\n",
      "\u001b[35m100%|██████████| 2/2 [00:02<00:00,  1.28s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 241/270 [2:20:21<17:22, 35.96s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0212, 'learning_rate': 2.307692307692308e-05, 'epoch': 2.68}\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 241/270 [2:20:21<17:22, 35.96s/it]\u001b[0m\n",
      "\u001b[35m89%|████████▉ | 241/270 [2:20:22<17:22, 35.96s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0212, 'learning_rate': 2.307692307692308e-05, 'epoch': 2.68}\u001b[0m\n",
      "\u001b[35m89%|████████▉ | 241/270 [2:20:22<17:22, 35.96s/it]\u001b[0m\n",
      "\u001b[35m90%|████████▉ | 242/270 [2:20:57<16:33, 35.49s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0201, 'learning_rate': 2.230769230769231e-05, 'epoch': 2.69}\u001b[0m\n",
      "\u001b[35m90%|████████▉ | 242/270 [2:20:57<16:33, 35.49s/it]\u001b[0m\n",
      "\u001b[34m90%|████████▉ | 242/270 [2:20:56<16:33, 35.49s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0201, 'learning_rate': 2.230769230769231e-05, 'epoch': 2.69}\u001b[0m\n",
      "\u001b[34m90%|████████▉ | 242/270 [2:20:56<16:33, 35.49s/it]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 243/270 [2:21:30<15:49, 35.17s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0182, 'learning_rate': 2.1538461538461542e-05, 'epoch': 2.7}\u001b[0m\n",
      "\u001b[34m90%|█████████ | 243/270 [2:21:30<15:49, 35.17s/it]\u001b[0m\n",
      "\u001b[35m90%|█████████ | 243/270 [2:21:31<15:49, 35.17s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0182, 'learning_rate': 2.1538461538461542e-05, 'epoch': 2.7}\u001b[0m\n",
      "\u001b[35m90%|█████████ | 243/270 [2:21:31<15:49, 35.17s/it]\u001b[0m\n",
      "\u001b[35m90%|█████████ | 244/270 [2:22:05<15:08, 34.93s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0255, 'learning_rate': 2.0769230769230772e-05, 'epoch': 2.71}\u001b[0m\n",
      "\u001b[35m90%|█████████ | 244/270 [2:22:05<15:08, 34.93s/it]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 244/270 [2:22:05<15:08, 34.93s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0255, 'learning_rate': 2.0769230769230772e-05, 'epoch': 2.71}\u001b[0m\n",
      "\u001b[34m90%|█████████ | 244/270 [2:22:05<15:08, 34.93s/it]\u001b[0m\n",
      "\u001b[35m91%|█████████ | 245/270 [2:22:40<14:29, 34.77s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0182, 'learning_rate': 2e-05, 'epoch': 2.72}\u001b[0m\n",
      "\u001b[35m91%|█████████ | 245/270 [2:22:40<14:29, 34.77s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 245/270 [2:22:39<14:29, 34.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0182, 'learning_rate': 2e-05, 'epoch': 2.72}\u001b[0m\n",
      "\u001b[34m91%|█████████ | 245/270 [2:22:39<14:29, 34.77s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 246/270 [2:23:13<13:51, 34.67s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.021, 'learning_rate': 1.923076923076923e-05, 'epoch': 2.73}\u001b[0m\n",
      "\u001b[34m91%|█████████ | 246/270 [2:23:13<13:51, 34.67s/it]\u001b[0m\n",
      "\u001b[35m91%|█████████ | 246/270 [2:23:14<13:52, 34.67s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.021, 'learning_rate': 1.923076923076923e-05, 'epoch': 2.73}\u001b[0m\n",
      "\u001b[35m91%|█████████ | 246/270 [2:23:14<13:52, 34.67s/it]\u001b[0m\n",
      "\u001b[35m91%|█████████▏| 247/270 [2:23:49<13:15, 34.58s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0202, 'learning_rate': 1.8461538461538465e-05, 'epoch': 2.74}\u001b[0m\n",
      "\u001b[35m91%|█████████▏| 247/270 [2:23:49<13:15, 34.58s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████▏| 247/270 [2:23:48<13:15, 34.59s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0202, 'learning_rate': 1.8461538461538465e-05, 'epoch': 2.74}\u001b[0m\n",
      "\u001b[34m91%|█████████▏| 247/270 [2:23:48<13:15, 34.59s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 248/270 [2:24:22<12:39, 34.53s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0207, 'learning_rate': 1.7692307692307694e-05, 'epoch': 2.76}\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 248/270 [2:24:22<12:39, 34.53s/it]\u001b[0m\n",
      "\u001b[35m92%|█████████▏| 248/270 [2:24:23<12:39, 34.53s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0207, 'learning_rate': 1.7692307692307694e-05, 'epoch': 2.76}\u001b[0m\n",
      "\u001b[35m92%|█████████▏| 248/270 [2:24:23<12:39, 34.53s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 249/270 [2:24:57<12:04, 34.49s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0207, 'learning_rate': 1.6923076923076924e-05, 'epoch': 2.77}\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 249/270 [2:24:57<12:04, 34.49s/it]\u001b[0m\n",
      "\u001b[35m92%|█████████▏| 249/270 [2:24:57<12:04, 34.49s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0207, 'learning_rate': 1.6923076923076924e-05, 'epoch': 2.77}\u001b[0m\n",
      "\u001b[35m92%|█████████▏| 249/270 [2:24:57<12:04, 34.49s/it]\u001b[0m\n",
      "\u001b[35m93%|█████████▎| 250/270 [2:25:32<11:29, 34.46s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.02, 'learning_rate': 1.6153846153846154e-05, 'epoch': 2.78}\u001b[0m\n",
      "\u001b[35m93%|█████████▎| 250/270 [2:25:32<11:29, 34.46s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 250/270 [2:25:31<11:29, 34.46s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.02, 'learning_rate': 1.6153846153846154e-05, 'epoch': 2.78}\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 250/270 [2:25:31<11:29, 34.46s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 251/270 [2:26:05<10:54, 34.44s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0242, 'learning_rate': 1.5384615384615387e-05, 'epoch': 2.79}\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 251/270 [2:26:05<10:54, 34.44s/it]\u001b[0m\n",
      "\u001b[35m93%|█████████▎| 251/270 [2:26:06<10:54, 34.44s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0242, 'learning_rate': 1.5384615384615387e-05, 'epoch': 2.79}\u001b[0m\n",
      "\u001b[35m93%|█████████▎| 251/270 [2:26:06<10:54, 34.44s/it]\u001b[0m\n",
      "\u001b[35m93%|█████████▎| 252/270 [2:26:41<10:19, 34.43s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0186, 'learning_rate': 1.4615384615384617e-05, 'epoch': 2.8}\u001b[0m\n",
      "\u001b[35m93%|█████████▎| 252/270 [2:26:41<10:19, 34.43s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 252/270 [2:26:40<10:19, 34.43s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0186, 'learning_rate': 1.4615384615384617e-05, 'epoch': 2.8}\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 252/270 [2:26:40<10:19, 34.43s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▎| 253/270 [2:27:14<09:45, 34.42s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.021, 'learning_rate': 1.3846153846153847e-05, 'epoch': 2.81}\u001b[0m\n",
      "\u001b[34m94%|█████████▎| 253/270 [2:27:14<09:45, 34.42s/it]\u001b[0m\n",
      "\u001b[35m94%|█████████▎| 253/270 [2:27:15<09:45, 34.42s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.021, 'learning_rate': 1.3846153846153847e-05, 'epoch': 2.81}\u001b[0m\n",
      "\u001b[35m94%|█████████▎| 253/270 [2:27:15<09:45, 34.42s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 254/270 [2:27:49<09:10, 34.41s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0238, 'learning_rate': 1.3076923076923078e-05, 'epoch': 2.82}\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 254/270 [2:27:49<09:10, 34.41s/it]\u001b[0m\n",
      "\u001b[35m94%|█████████▍| 254/270 [2:27:49<09:10, 34.41s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0238, 'learning_rate': 1.3076923076923078e-05, 'epoch': 2.82}\u001b[0m\n",
      "\u001b[35m94%|█████████▍| 254/270 [2:27:49<09:10, 34.41s/it]\u001b[0m\n",
      "\u001b[35m94%|█████████▍| 255/270 [2:28:24<08:36, 34.41s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0262, 'learning_rate': 1.230769230769231e-05, 'epoch': 2.83}\u001b[0m\n",
      "\u001b[35m94%|█████████▍| 255/270 [2:28:24<08:36, 34.41s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 255/270 [2:28:23<08:36, 34.41s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 255/270 [2:28:23<08:36, 34.41s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0262, 'learning_rate': 1.230769230769231e-05, 'epoch': 2.83}\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 256/270 [2:28:57<08:01, 34.40s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0215, 'learning_rate': 1.153846153846154e-05, 'epoch': 2.84}\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 256/270 [2:28:57<08:01, 34.40s/it]\u001b[0m\n",
      "\u001b[35m95%|█████████▍| 256/270 [2:28:58<08:01, 34.40s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0215, 'learning_rate': 1.153846153846154e-05, 'epoch': 2.84}\u001b[0m\n",
      "\u001b[35m95%|█████████▍| 256/270 [2:28:58<08:01, 34.40s/it]\u001b[0m\n",
      "\u001b[35m95%|█████████▌| 257/270 [2:29:33<07:27, 34.40s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0235, 'learning_rate': 1.0769230769230771e-05, 'epoch': 2.86}\u001b[0m\n",
      "\u001b[35m95%|█████████▌| 257/270 [2:29:33<07:27, 34.40s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 257/270 [2:29:32<07:27, 34.40s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0235, 'learning_rate': 1.0769230769230771e-05, 'epoch': 2.86}\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 257/270 [2:29:32<07:27, 34.40s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 258/270 [2:30:06<06:52, 34.40s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0233, 'learning_rate': 1e-05, 'epoch': 2.87}\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 258/270 [2:30:06<06:52, 34.40s/it]\u001b[0m\n",
      "\u001b[35m96%|█████████▌| 258/270 [2:30:07<06:52, 34.40s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0233, 'learning_rate': 1e-05, 'epoch': 2.87}\u001b[0m\n",
      "\u001b[35m96%|█████████▌| 258/270 [2:30:07<06:52, 34.40s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 259/270 [2:30:41<06:18, 34.40s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0222, 'learning_rate': 9.230769230769232e-06, 'epoch': 2.88}\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 259/270 [2:30:41<06:18, 34.40s/it]\u001b[0m\n",
      "\u001b[35m96%|█████████▌| 259/270 [2:30:41<06:18, 34.40s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0222, 'learning_rate': 9.230769230769232e-06, 'epoch': 2.88}\u001b[0m\n",
      "\u001b[35m96%|█████████▌| 259/270 [2:30:41<06:18, 34.40s/it]\u001b[0m\n",
      "\u001b[35m96%|█████████▋| 260/270 [2:31:16<05:43, 34.39s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0215, 'learning_rate': 8.461538461538462e-06, 'epoch': 2.89}\u001b[0m\n",
      "\u001b[35m96%|█████████▋| 260/270 [2:31:16<05:43, 34.39s/it]\u001b[0m\n",
      "\u001b[35m[2023-10-28 00:11:30,501] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:196] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-28 00:11:30,508] [INFO] [axolotl.utils.dataloader.generate_batches:181] [PID:196] [RANK:0] generating packed batches#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-28 00:11:30,509] [INFO] [axolotl.utils.dataloader.generate_batches:187] [PID:196] [RANK:0] b68612dda09b7da3bd0173ed59f621c8c8f01237e9f3dcbc6d7b7c116d4e5e0c#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-28 00:11:30,511] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:196] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[34m96%|█████████▋| 260/270 [2:31:15<05:43, 34.39s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0215, 'learning_rate': 8.461538461538462e-06, 'epoch': 2.89}\u001b[0m\n",
      "\u001b[34m96%|█████████▋| 260/270 [2:31:15<05:43, 34.39s/it]\u001b[0m\n",
      "\u001b[34m[2023-10-28 00:11:30,502] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:197] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-28 00:11:30,512] [INFO] [axolotl.utils.dataloader.generate_batches:181] [PID:197] [RANK:0] generating packed batches#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-28 00:11:30,512] [INFO] [axolotl.utils.dataloader.generate_batches:187] [PID:197] [RANK:0] a3efec085e0760812a351b73f7cdffc9a801ed4832469f9f56e2c45c91279c6e#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-28 00:11:30,515] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:197] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-28 00:11:33,015] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:197] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[34m[2023-10-28 00:11:33,015] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:197] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[34m0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[2023-10-28 00:11:33,005] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:196] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[35m[2023-10-28 00:11:33,005] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:196] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[35m0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[2023-10-28 00:11:35,578] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:196] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[35m100%|██████████| 2/2 [00:02<00:00,  1.29s/it]\u001b[0m\n",
      "\u001b[35m{'eval_loss': 0.019884735345840454, 'eval_runtime': 5.1752, 'eval_samples_per_second': 151.877, 'eval_steps_per_second': 38.066, 'epoch': 2.89}\u001b[0m\n",
      "\u001b[35m96%|█████████▋| 260/270 [2:31:21<05:43, 34.39s/it]\u001b[0m\n",
      "\u001b[35m100%|██████████| 2/2 [00:02<00:00,  1.29s/it]\u001b[0m\n",
      "\u001b[34m[2023-10-28 00:11:35,578] [INFO] [axolotl.utils.dataloader._len_est:264] [PID:197] [RANK:0] packing_efficiency_estimate: 1.0 total_num_tokens per device: 28891#033[39m\u001b[0m\n",
      "\u001b[34m100%|██████████| 2/2 [00:02<00:00,  1.28s/it]\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.019884735345840454, 'eval_runtime': 5.1741, 'eval_samples_per_second': 151.912, 'eval_steps_per_second': 38.075, 'epoch': 2.89}\u001b[0m\n",
      "\u001b[34m96%|█████████▋| 260/270 [2:31:20<05:43, 34.39s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 2/2 [00:02<00:00,  1.28s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 261/270 [2:31:55<05:23, 35.95s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0216, 'learning_rate': 7.692307692307694e-06, 'epoch': 2.9}\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 261/270 [2:31:55<05:23, 35.95s/it]\u001b[0m\n",
      "\u001b[35m97%|█████████▋| 261/270 [2:31:55<05:23, 35.95s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0216, 'learning_rate': 7.692307692307694e-06, 'epoch': 2.9}\u001b[0m\n",
      "\u001b[35m97%|█████████▋| 261/270 [2:31:55<05:23, 35.95s/it]\u001b[0m\n",
      "\u001b[35m97%|█████████▋| 262/270 [2:32:30<04:43, 35.48s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0203, 'learning_rate': 6.923076923076923e-06, 'epoch': 2.91}\u001b[0m\n",
      "\u001b[35m97%|█████████▋| 262/270 [2:32:30<04:43, 35.48s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 262/270 [2:32:29<04:43, 35.48s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0203, 'learning_rate': 6.923076923076923e-06, 'epoch': 2.91}\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 262/270 [2:32:29<04:43, 35.48s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 263/270 [2:33:03<04:06, 35.15s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.022, 'learning_rate': 6.153846153846155e-06, 'epoch': 2.92}\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 263/270 [2:33:03<04:06, 35.15s/it]\u001b[0m\n",
      "\u001b[35m97%|█████████▋| 263/270 [2:33:04<04:06, 35.15s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.022, 'learning_rate': 6.153846153846155e-06, 'epoch': 2.92}\u001b[0m\n",
      "\u001b[35m97%|█████████▋| 263/270 [2:33:04<04:06, 35.15s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 264/270 [2:33:38<03:29, 34.93s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0228, 'learning_rate': 5.3846153846153855e-06, 'epoch': 2.93}\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 264/270 [2:33:38<03:29, 34.93s/it]\u001b[0m\n",
      "\u001b[35m98%|█████████▊| 264/270 [2:33:39<03:29, 34.93s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0228, 'learning_rate': 5.3846153846153855e-06, 'epoch': 2.93}\u001b[0m\n",
      "\u001b[35m98%|█████████▊| 264/270 [2:33:39<03:29, 34.93s/it]\u001b[0m\n",
      "\u001b[35m98%|█████████▊| 265/270 [2:34:13<02:53, 34.77s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0248, 'learning_rate': 4.615384615384616e-06, 'epoch': 2.94}\u001b[0m\n",
      "\u001b[35m98%|█████████▊| 265/270 [2:34:13<02:53, 34.77s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 265/270 [2:34:12<02:53, 34.77s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0248, 'learning_rate': 4.615384615384616e-06, 'epoch': 2.94}\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 265/270 [2:34:12<02:53, 34.77s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▊| 266/270 [2:34:47<02:18, 34.66s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0212, 'learning_rate': 3.846153846153847e-06, 'epoch': 2.96}\u001b[0m\n",
      "\u001b[34m99%|█████████▊| 266/270 [2:34:47<02:18, 34.66s/it]\u001b[0m\n",
      "\u001b[35m99%|█████████▊| 266/270 [2:34:47<02:18, 34.66s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0212, 'learning_rate': 3.846153846153847e-06, 'epoch': 2.96}\u001b[0m\n",
      "\u001b[35m99%|█████████▊| 266/270 [2:34:47<02:18, 34.66s/it]\u001b[0m\n",
      "\u001b[35m99%|█████████▉| 267/270 [2:35:22<01:43, 34.59s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0243, 'learning_rate': 3.0769230769230774e-06, 'epoch': 2.97}\u001b[0m\n",
      "\u001b[35m99%|█████████▉| 267/270 [2:35:22<01:43, 34.59s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 267/270 [2:35:21<01:43, 34.59s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0243, 'learning_rate': 3.0769230769230774e-06, 'epoch': 2.97}\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 267/270 [2:35:21<01:43, 34.59s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 268/270 [2:35:55<01:09, 34.53s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0214, 'learning_rate': 2.307692307692308e-06, 'epoch': 2.98}\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 268/270 [2:35:55<01:09, 34.53s/it]\u001b[0m\n",
      "\u001b[35m99%|█████████▉| 268/270 [2:35:56<01:09, 34.53s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0214, 'learning_rate': 2.307692307692308e-06, 'epoch': 2.98}\u001b[0m\n",
      "\u001b[35m99%|█████████▉| 268/270 [2:35:56<01:09, 34.53s/it]\u001b[0m\n",
      "\u001b[34m100%|█████████▉| 269/270 [2:36:30<00:34, 34.49s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0252, 'learning_rate': 1.5384615384615387e-06, 'epoch': 2.99}\u001b[0m\n",
      "\u001b[34m100%|█████████▉| 269/270 [2:36:30<00:34, 34.49s/it]\u001b[0m\n",
      "\u001b[35m100%|█████████▉| 269/270 [2:36:31<00:34, 34.49s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0252, 'learning_rate': 1.5384615384615387e-06, 'epoch': 2.99}\u001b[0m\n",
      "\u001b[35m100%|█████████▉| 269/270 [2:36:31<00:34, 34.49s/it]\u001b[0m\n",
      "\u001b[35m100%|██████████| 270/270 [2:37:05<00:00, 34.46s/it]\u001b[0m\n",
      "\u001b[35m{'loss': 0.0236, 'learning_rate': 7.692307692307694e-07, 'epoch': 3.0}\u001b[0m\n",
      "\u001b[35m100%|██████████| 270/270 [2:37:05<00:00, 34.46s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 270/270 [2:37:04<00:00, 34.46s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.0236, 'learning_rate': 7.692307692307694e-07, 'epoch': 3.0}\u001b[0m\n",
      "\u001b[34m100%|██████████| 270/270 [2:37:04<00:00, 34.46s/it]\u001b[0m\n",
      "\u001b[35m{'train_runtime': 9459.4156, 'train_samples_per_second': 24.671, 'train_steps_per_second': 0.029, 'train_loss': 0.06919827722702865, 'epoch': 3.0}\u001b[0m\n",
      "\u001b[35m100%|██████████| 270/270 [2:37:39<00:00, 34.46s/it]\u001b[0m\n",
      "\u001b[35m100%|██████████| 270/270 [2:37:39<00:00, 35.03s/it]\u001b[0m\n",
      "\u001b[35m[2023-10-28 00:17:53,580] [INFO] [axolotl.train.train:120] [PID:196] [RANK:0] Training Completed!!! Saving pre-trained model to /opt/ml/model#033[39m\u001b[0m\n",
      "\u001b[35m2023-10-28 00:17:57,180 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[35m2023-10-28 00:17:57,180 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[35m2023-10-28 00:17:57,180 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\u001b[34m{'train_runtime': 9458.6017, 'train_samples_per_second': 24.673, 'train_steps_per_second': 0.029, 'train_loss': 0.06919827722702865, 'epoch': 3.0}\u001b[0m\n",
      "\u001b[34m100%|██████████| 270/270 [2:37:38<00:00, 34.46s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 270/270 [2:37:38<00:00, 35.03s/it]\u001b[0m\n",
      "\u001b[34m[2023-10-28 00:17:53,708] [INFO] [axolotl.train.train:120] [PID:197] [RANK:0] Training Completed!!! Saving pre-trained model to /opt/ml/model#033[39m\u001b[0m\n",
      "\u001b[34m2023-10-28 00:17:57,186 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-10-28 00:17:57,186 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-10-28 00:17:57,186 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2023-10-28 00:18:27 Uploading - Uploading generated training model\n",
      "2023-10-28 00:18:49 Completed - Instances not retained as a result of warmpool resource limits being exceeded\n",
      "Training seconds: 20720\n",
      "Billable seconds: 20720\n"
     ]
    }
   ],
   "source": [
    "estimator.fit({\"model\": s3_model_location, \"train\": s3_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Tensorboard report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-west-2-376678947624/code-llama7b/tensorboard/2023-10-27-21-24-01'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"s3://{bucket}/{s3_prefix}/tensorboard/{str_time}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model performance before and after fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:48<00:00, 24.37s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    local_model_path,\n",
    "    load_in_8bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(local_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables.\n",
      "\n",
      "You must output the SQL query that answers the question.\n",
      "### Input:\n",
      "Which Class has a Frequency MHz larger than 91.5, and a City of license of hyannis, nebraska?\n",
      "\n",
      "### Context:\n",
      "CREATE TABLE table_name_12 (class VARCHAR, frequency_mhz VARCHAR, city_of_license VARCHAR)\n",
      "\n",
      "### Response:\n",
      "SELECT * FROM table_name_12 WHERE class > '91.5' AND city_of_license = 'hyannis'\n",
      "\n",
      "### Input:\n",
      "Which Class has a Frequency MHz larger than 91.5, and a City of license of hyannis, nebraska?\n",
      "\n",
      "### Context:\n",
      "CREATE TABLE table_name_12 (class VARCHAR, frequency_mhz VARCHAR, city_of_lic\n"
     ]
    }
   ],
   "source": [
    "eval_prompt = \"\"\"You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables.\n",
    "\n",
    "You must output the SQL query that answers the question.\n",
    "### Input:\n",
    "Which Class has a Frequency MHz larger than 91.5, and a City of license of hyannis, nebraska?\n",
    "\n",
    "### Context:\n",
    "CREATE TABLE table_name_12 (class VARCHAR, frequency_mhz VARCHAR, city_of_license VARCHAR)\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "# {'question': 'Name the comptroller for office of prohibition', 'context': 'CREATE TABLE table_22607062_1 (comptroller VARCHAR, ticket___office VARCHAR)', 'answer': 'SELECT comptroller FROM table_22607062_1 WHERE ticket___office = \"Prohibition\"'}\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=100)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# lora_path = estimator.model_data\n",
    "lora_path = \"s3://sagemaker-us-west-2-376678947624/pytorch-training-2023-10-27-21-24-37-446/output/model.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "download: s3://sagemaker-us-west-2-376678947624/pytorch-training-2023-10-27-21-24-37-446/output/model.tar.gz to ./model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp {lora_path} ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.creationtime'\n",
      "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.creationtime'\n",
      "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.creationtime'\n",
      "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.creationtime'\n",
      "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.creationtime'\n",
      "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.creationtime'\n",
      "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.creationtime'\n",
      "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.creationtime'\n",
      "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.creationtime'\n",
      "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.creationtime'\n",
      "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.creationtime'\n",
      "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.creationtime'\n",
      "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.creationtime'\n",
      "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.creationtime'\n"
     ]
    }
   ],
   "source": [
    "!tar -xzf model.tar.gz -C lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "model = PeftModel.from_pretrained(model, \"lora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables.\n",
      "\n",
      "You must output the SQL query that answers the question.\n",
      "### Input:\n",
      "Which Class has a Frequency MHz larger than 91.5, and a City of license of hyannis, nebraska?\n",
      "\n",
      "### Context:\n",
      "CREATE TABLE table_name_12 (class VARCHAR, frequency_mhz VARCHAR, city_of_license VARCHAR)\n",
      "\n",
      "### Response:\n",
      " SELECT class FROM table_name_12 WHERE frequency_mhz > 91.5 AND city_of_license = \"hyannis, nebraska\"\n"
     ]
    }
   ],
   "source": [
    "eval_prompt = \"\"\"You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables.\n",
    "\n",
    "You must output the SQL query that answers the question.\n",
    "### Input:\n",
    "Which Class has a Frequency MHz larger than 91.5, and a City of license of hyannis, nebraska?\n",
    "\n",
    "### Context:\n",
    "CREATE TABLE table_name_12 (class VARCHAR, frequency_mhz VARCHAR, city_of_license VARCHAR)\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=100)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 2.0.0 Python 3.10 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/pytorch-2.0.0-gpu-py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
