{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3529419c-859d-4d04-b87f-18ae4e36e532",
   "metadata": {},
   "source": [
    "## Model Management for LoRA Fine-tuned models using Llama2 & Amazon SageMaker (Separate Adapter and Base Models)\n",
    "\n",
    "In this example notebook, we will walk through an example using LoRA techniques to fine-tune a LLama2 7B model on Amazon SageMaker, and then add the proper model governance using SageMaker Model Registry. This notebook focus on saving and managing LoRA adapter and base models seperately. \n",
    "\n",
    "The example is tested on following kernel and instance types:\n",
    "\n",
    "**Kernel: PyTorch 2.0.0 Python 3.10 GPU Optimized, instance type: ml.g4dn.xlarge**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0940ae6c-06c9-4fcb-8890-fe47d2064b51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -Uq pip\n",
    "!pip install -Uq anytree==2.8.0\n",
    "!pip install -Uq graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb50f25a-b0dd-4ba4-9207-9a95237d03c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -Uq datasets\n",
    "!pip install -Uq transformers==4.31.0\n",
    "!pip install -Uq accelerate==0.21.0\n",
    "!pip install -Uq safetensors>=0.3.1\n",
    "!pip install -Uq botocore\n",
    "!pip install -Uq boto3\n",
    "!pip install -q sagemaker==2.177.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358dd2d0-cd4a-4c62-b2e7-6636034f0183",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import boto3\n",
    "import pprint\n",
    "from tqdm import tqdm\n",
    "import sagemaker\n",
    "from sagemaker.collection import Collection\n",
    "from sagemaker.utils import name_from_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf85bc0-8f47-42e8-820d-2fdbe7aceacf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sagemaker_session =  sagemaker.session.Session(boto3.session.Session(region_name=\"us-west-2\")) #sagemaker.session.Session()\n",
    "region = sagemaker_session.boto_region_name\n",
    "role = sagemaker.get_execution_role()\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "sm_client = boto3.client('sagemaker', region_name=region)\n",
    "model_collector = Collection(sagemaker_session=sagemaker_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95b8777-7e66-4ea6-b74b-1a55cea14fe1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Define Parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d76a118-1f5f-4c25-8e92-218e1cc5c84e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define base model name\n",
    "model_group_for_base = \"llama-2-7b\" # we'll group all llama-2 variants under this collection \n",
    "model_id = f\"Mikael110/{model_group_for_base}-guanaco-fp16\" \n",
    "# define a base dataset to finetune this base model\n",
    "dataset_name = \"databricks/databricks-dolly-15k\"\n",
    "# base model collection name\n",
    "model_registry_name_base = f\"{model_id.replace('/', '-')}-base\"\n",
    "# finetuned model collection name\n",
    "model_registry_name_finetuned = f\"{model_id.replace('/', '-')}-finetuned\"\n",
    "model_group_for_finetune = dataset_name.split('/')[-1] # we will group all dataset finetunes to this and attach it back to the parent model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d682d0-ed29-44a6-87f5-b13993b39aeb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd444849-0514-43f2-abee-c3b2a52b7b85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from random import randrange\n",
    "\n",
    "# Load dataset from the hub\n",
    "train_dataset = load_dataset(dataset_name, split=\"train[:05%]\")\n",
    "validation_dataset = load_dataset(dataset_name, split=\"train[95%:]\")\n",
    "\n",
    "print(f\"Training size: {len(train_dataset)} | Validation size: {len(validation_dataset)}\")\n",
    "print(\"\\nTraining sample:\\n\")\n",
    "print(train_dataset[randrange(len(train_dataset))])\n",
    "print(\"\\nValidation sample:\\n\")\n",
    "print(validation_dataset[randrange(len(validation_dataset))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc42da95-2c97-4ec6-87cd-0cdc7d66bcb6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def format_dolly(sample):\n",
    "    instruction = f\"### Instruction\\n{sample['instruction']}\"\n",
    "    context = f\"### Context\\n{sample['context']}\" if len(sample[\"context\"]) > 0 else None\n",
    "    response = f\"### Answer\\n{sample['response']}\"\n",
    "    # join all the parts together\n",
    "    prompt = \"\\n\\n\".join([i for i in [instruction, context, response] if i is not None])\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d201dd-b9b4-43a5-a333-01a0e067c6e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from random import randrange\n",
    "\n",
    "print(format_dolly(train_dataset[randrange(len(train_dataset))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cc07b9-0cb5-4103-a58b-c5a6d5044812",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d3bb03-3d65-4b03-b79e-90b28886627e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from itertools import chain\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "# template dataset to add prompt to each sample\n",
    "def template_dataset(sample):\n",
    "    sample[\"text\"] = f\"{format_dolly(sample)}{tokenizer.eos_token}\"\n",
    "    return sample\n",
    "\n",
    "\n",
    "# apply prompt template per sample\n",
    "# train\n",
    "train_dataset = train_dataset.map(template_dataset, remove_columns=list(train_dataset.features))\n",
    "# validation\n",
    "validation_dataset = validation_dataset.map(template_dataset, remove_columns=list(validation_dataset.features))\n",
    "# print random sample\n",
    "print(validation_dataset[randint(0, len(validation_dataset))][\"text\"])\n",
    "\n",
    "# empty list to save remainder from batches to use in next batch\n",
    "remainder = {\"input_ids\": [], \"attention_mask\": [], \"token_type_ids\": []}\n",
    "\n",
    "def chunk(sample, chunk_length=2048):\n",
    "    # define global remainder variable to save remainder from batches to use in next batch\n",
    "    global remainder\n",
    "    # Concatenate all texts and add remainder from previous batch\n",
    "    concatenated_examples = {k: list(chain(*sample[k])) for k in sample.keys()}\n",
    "    concatenated_examples = {k: remainder[k] + concatenated_examples[k] for k in concatenated_examples.keys()}\n",
    "    # get total number of tokens for batch\n",
    "    batch_total_length = len(concatenated_examples[list(sample.keys())[0]])\n",
    "\n",
    "    # get max number of chunks for batch\n",
    "    if batch_total_length >= chunk_length:\n",
    "        batch_chunk_length = (batch_total_length // chunk_length) * chunk_length\n",
    "\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + chunk_length] for i in range(0, batch_chunk_length, chunk_length)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # add remainder to global variable for next batch\n",
    "    remainder = {k: concatenated_examples[k][batch_chunk_length:] for k in concatenated_examples.keys()}\n",
    "    # prepare labels\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "\n",
    "# tokenize and chunk dataset\n",
    "\n",
    "# training\n",
    "lm_train_dataset = train_dataset.map(\n",
    "    lambda sample: tokenizer(sample[\"text\"]), batched=True, remove_columns=list(train_dataset.features)\n",
    ").map(\n",
    "    partial(chunk, chunk_length=2048),\n",
    "    batched=True,\n",
    ")\n",
    "\n",
    "# validation\n",
    "lm_valid_dataset = validation_dataset.map(\n",
    "    lambda sample: tokenizer(sample[\"text\"]), batched=True, remove_columns=list(validation_dataset.features)\n",
    ").map(\n",
    "    partial(chunk, chunk_length=2048),\n",
    "    batched=True,\n",
    ")\n",
    "\n",
    "# Print total number of samples\n",
    "print(f\"Total number of samples: {len(validation_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531a2b2b-7f56-46cf-bffe-2e79842242ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save train_dataset to s3\n",
    "training_input_path = f's3://{default_bucket}/largelanguagemodels/{model_id}/dataset/train'\n",
    "lm_train_dataset.save_to_disk(training_input_path)\n",
    "\n",
    "print(f\"saving training dataset to: {training_input_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc22fd93-c713-4e01-bb7e-b52bb53eefe6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save train_dataset to s3\n",
    "validation_input_path = f's3://{default_bucket}/largelanguagemodels/{model_id}/dataset/validation'\n",
    "lm_valid_dataset.save_to_disk(validation_input_path)\n",
    "\n",
    "print(f\"saving validation dataset to: {validation_input_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60f4eba-3daf-4487-b28e-f09e158b4335",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Save Base model into Model Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a823dbae-0315-4bbc-a21f-135f7a663597",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785e619b-c727-4713-acfb-2dd8186cf327",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_model_save_dir = f\"./base_model/{model_id}\"\n",
    "os.makedirs(base_model_save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02993ec-b309-4633-a188-f7f52f730b51",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id).save_pretrained(base_model_save_dir)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, \n",
    "    torch_dtype=torch.float16, \n",
    "    device_map=\"auto\"\n",
    ").save_pretrained(base_model_save_dir) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d64aaaa-e569-400b-9c14-48cca6836b62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del model\n",
    "import torch; torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18de1f5-544b-4299-8b2c-cc9ae831973d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_save_dir,\n",
    "    # use_cache=False if args.gradient_checkpointing else True,\n",
    "    torch_dtype=torch.float16, \n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e7041a-ef5f-44dc-886b-42b55a530a8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_tar_filename = f\"{model_id.replace('/', '-')}.tar.gz\"\n",
    "print(f\"Model tar file name: {model_tar_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f653d43-6bfc-4d7d-95f6-b7ab86b69d44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cd ./base_model && tar -cvf ./{model_tar_filename} ./{model_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da47f9fb-7c53-466c-bc7c-0f0c9f557ccf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_data_uri = sagemaker.s3.S3Uploader.upload(\n",
    "    local_path=f\"./base_model/{model_tar_filename}\",\n",
    "    desired_s3_uri=f's3://{default_bucket}/largelanguagemodels/{model_id}/models/base',\n",
    ")\n",
    "print(model_data_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcce7ef-f1b6-4164-a40a-768bb688b5d6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create a Model Package Group "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec0aef9-a947-4ae4-8cb0-64e4824f4a2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Model Package Group Vars\n",
    "base_package_group_name = name_from_base(model_id.replace('/', '-'))\n",
    "base_package_group_desc = \"Source: https://huggingface.co/Mikael110/llama-2-7b-guanaco-fp16\"\n",
    "base_tags = [\n",
    "    { \n",
    "        \"Key\": \"modelType\",\n",
    "        \"Value\": \"BaseModel\"\n",
    "    },\n",
    "    { \n",
    "        \"Key\": \"fineTuned\",\n",
    "        \"Value\": \"False\"\n",
    "    },\n",
    "    { \n",
    "        \"Key\": \"sourceDataset\",\n",
    "        \"Value\": \"None\"\n",
    "    }\n",
    "]\n",
    "\n",
    "model_package_group_input_dict = {\n",
    "    \"ModelPackageGroupName\" : base_package_group_name,\n",
    "    \"ModelPackageGroupDescription\" : base_package_group_desc,\n",
    "    \"Tags\": base_tags\n",
    "    \n",
    "}\n",
    "create_model_pacakge_group_response = sm_client.create_model_package_group(\n",
    "    **model_package_group_input_dict\n",
    ")\n",
    "print(f'Created ModelPackageGroup Arn : {create_model_pacakge_group_response[\"ModelPackageGroupArn\"]}')\n",
    "\n",
    "base_model_pkg_group_name = create_model_pacakge_group_response[\"ModelPackageGroupArn\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070619c0-5f20-4670-921b-5b39b4d3de86",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Register the Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416d004a-0bb7-4f0e-acf8-639d3ffa660b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFaceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb4bdfc-17df-4a8f-9dd6-4ed7ccf1c4e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "    transformers_version='4.28',\n",
    "    pytorch_version='2.0',  \n",
    "    py_version='py310',\n",
    "    model_data=model_data_uri,\n",
    "    role=role,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48572af-00bf-4b90-9fdd-8c467ccbedea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_response = huggingface_model.register(\n",
    "    content_types=[\"application/json\"],\n",
    "    response_types=[\"application/json\"],\n",
    "    inference_instances=[\n",
    "        \"ml.p2.16xlarge\", \n",
    "        \"ml.p3.16xlarge\", \n",
    "        \"ml.g4dn.4xlarge\", \n",
    "        \"ml.g4dn.8xlarge\", \n",
    "        \"ml.g4dn.12xlarge\", \n",
    "        \"ml.g4dn.16xlarge\"\n",
    "    ],\n",
    "    transform_instances=[\n",
    "        \"ml.p2.16xlarge\", \n",
    "        \"ml.p3.16xlarge\", \n",
    "        \"ml.g4dn.4xlarge\", \n",
    "        \"ml.g4dn.8xlarge\", \n",
    "        \"ml.g4dn.12xlarge\", \n",
    "        \"ml.g4dn.16xlarge\"\n",
    "    ],\n",
    "    model_package_group_name=base_model_pkg_group_name,\n",
    "    approval_status=\"Approved\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4f68d9-3f2d-40df-adb4-73db7248098f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Add Base Model to Model Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de687bd-2228-4a33-a80c-35661a2fed37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create model collection\n",
    "collection_name = name_from_base(model_group_for_base)\n",
    "base_collection = model_collector.create(\n",
    "    collection_name=collection_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75053dd6-1c4a-4ae7-b470-8a550435814e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_response = model_collector.add_model_groups(\n",
    "    collection_name=base_collection[\"Arn\"], \n",
    "    model_groups=[base_model_pkg_group_name]\n",
    ")\n",
    "\n",
    "print(f\"Model collection creation status: {_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11564158-f72f-442c-963e-71c56647fe23",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57df7c5-5cce-4d41-a2d6-dabf67db59d5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "from sagemaker.experiments.run import Run\n",
    "\n",
    "# define Training Job Name \n",
    "time_suffix = datetime.now().strftime('%y%m%d%H%M')\n",
    "job_name = f'huggingface-qlora-{time_suffix}'\n",
    "experiments_name = f\"exp-{model_id.replace('/', '-')}\"\n",
    "run_name = f\"qlora-finetune-run-{time_suffix}\"\n",
    "\n",
    "with Run(\n",
    "    experiment_name=experiments_name, \n",
    "    run_name=run_name, \n",
    "    sagemaker_session=sagemaker.Session()\n",
    ") as run:\n",
    "    # create the Estimator\n",
    "    huggingface_estimator = HuggingFace(\n",
    "        entry_point='finetune_llm.py',      \n",
    "        source_dir='code',         \n",
    "        instance_type='ml.g5.2xlarge',   \n",
    "        instance_count=1,       \n",
    "        role=role,              \n",
    "        volume_size=300,               \n",
    "        transformers_version='4.28',            \n",
    "        pytorch_version='2.0',             \n",
    "        py_version='py310',           \n",
    "        hyperparameters={\n",
    "            'base_model_group_name': base_package_group_name,\n",
    "            'model_id': model_id,                             \n",
    "            'dataset_path': '/opt/ml/input/data/training',    \n",
    "            'epochs': 1,                                      \n",
    "            'per_device_train_batch_size': 2,                 \n",
    "            'lr': 1e-4,\n",
    "            'region': region,\n",
    "        },\n",
    "        sagemaker_session=sagemaker_session\n",
    "    )\n",
    "\n",
    "    # starting the train job with our uploaded datasets as input\n",
    "    data = {\n",
    "        'training': training_input_path, \n",
    "        'validation': validation_input_path\n",
    "    }\n",
    "    huggingface_estimator.fit(\n",
    "        data, \n",
    "        wait=True,\n",
    "        job_name=job_name\n",
    "    )\n",
    "    \n",
    "    run.log_parameters(data)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d06f2d-9a55-4a3f-9cf9-b0895df2229b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "huggingface_estimator.attach(training_job_name=job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a525fe16-1eb9-47d5-b2bc-b49951ef5b38",
   "metadata": {},
   "source": [
    "## Save FineTuned model into Model Registry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba930d08-cdf3-4522-b584-3081b5c559c4",
   "metadata": {},
   "source": [
    "### Fine-Tuned Model Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f5a384-7a8b-455c-b8b6-ec920a419902",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Model Package Group Vars\n",
    "ft_package_group_name = name_from_base(f\"{model_id.replace('/', '-')}-finetuned-sql\")\n",
    "ft_package_group_desc = \"QLoRA for model Mikael110/llama-2-7b-guanaco-fp16\"\n",
    "ft_tags = [\n",
    "    { \n",
    "        \"Key\": \"modelType\",\n",
    "        \"Value\": \"QLoRAModel\"\n",
    "    },\n",
    "    { \n",
    "        \"Key\": \"fineTuned\",\n",
    "        \"Value\": \"True\"\n",
    "    },\n",
    "    { \n",
    "        \"Key\": \"sourceDataset\",\n",
    "        \"Value\": f\"{dataset_name}\"\n",
    "    }\n",
    "]\n",
    "\n",
    "model_package_group_input_dict = {\n",
    "    \"ModelPackageGroupName\" : ft_package_group_name,\n",
    "    \"ModelPackageGroupDescription\" : ft_package_group_desc,\n",
    "    \"Tags\": ft_tags\n",
    "    \n",
    "}\n",
    "create_model_pacakge_group_response = sm_client.create_model_package_group(\n",
    "    **model_package_group_input_dict\n",
    ")\n",
    "print(f'Created ModelPackageGroup Arn : {create_model_pacakge_group_response[\"ModelPackageGroupArn\"]}')\n",
    "\n",
    "ft_model_pkg_group_name = create_model_pacakge_group_response[\"ModelPackageGroupArn\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd1c96e-f9d2-42a0-ac79-2499b2e6bb02",
   "metadata": {},
   "source": [
    "### Register a New Model into Fine-Tuned Model Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08834142-c53a-4b1c-b173-104aed33d257",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "    transformers_version='4.28',\n",
    "    pytorch_version='2.0',  \n",
    "    py_version='py310',\n",
    "    model_data=huggingface_estimator.model_data,\n",
    "    role=role,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ade077-e175-41c8-969d-e0c538b49077",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_response = huggingface_model.register(\n",
    "    content_types=[\"application/json\"],\n",
    "    response_types=[\"application/json\"],\n",
    "    inference_instances=[\n",
    "        \"ml.p2.16xlarge\", \n",
    "        \"ml.p3.16xlarge\", \n",
    "        \"ml.g4dn.4xlarge\", \n",
    "        \"ml.g4dn.8xlarge\", \n",
    "        \"ml.g4dn.12xlarge\", \n",
    "        \"ml.g4dn.16xlarge\"\n",
    "    ],\n",
    "    transform_instances=[\n",
    "        \"ml.p2.16xlarge\", \n",
    "        \"ml.p3.16xlarge\", \n",
    "        \"ml.g4dn.4xlarge\", \n",
    "        \"ml.g4dn.8xlarge\", \n",
    "        \"ml.g4dn.12xlarge\", \n",
    "        \"ml.g4dn.16xlarge\"\n",
    "    ],\n",
    "    model_package_group_name=ft_model_pkg_group_name,\n",
    "    approval_status=\"Approved\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a70ee4-8766-4170-981d-f2b0abd33be3",
   "metadata": {},
   "source": [
    "### Add FineTuned Model to Model Collection with Parent Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce5c010-ff31-414b-8454-0628b9c6d4c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create model collection for finetuned and link it back to the base\n",
    "finetuned_collection = model_collector.create(\n",
    "    collection_name=name_from_base(model_group_for_finetune),\n",
    "    parent_collection_name=collection_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41762a9e-f4ce-493b-88d4-ce34b488a614",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# add finetuned model package group to the new finetuned collection\n",
    "_response = model_collector.add_model_groups(\n",
    "    collection_name=model_group_for_finetune,\n",
    "    model_groups=[ft_model_pkg_group_name]\n",
    ")\n",
    "\n",
    "print(f\"Model collection creation status: {_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11af77c6-0345-4269-8306-db48a8ba82fe",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Understanding Parent (Base) - Child (QLoRA) Model Registry Relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c708466-ed6a-4206-b1f6-c8c4405a3928",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from collections import OrderedDict\n",
    "from anytree import (\n",
    "    AnyNode as Node, \n",
    "    RenderTree, \n",
    "    DoubleStyle\n",
    ")\n",
    "from anytree.dotexport import RenderTreeGraph\n",
    "\n",
    "\n",
    "def recursively_build_model_tree(\n",
    "    root_model_package_group, \n",
    "    output_dict, \n",
    "    level=0\n",
    "):\n",
    "    \"\"\" Recursively extracts model collections \n",
    "    to build a relationship dictonary \"\"\"\n",
    "    output_dict[root_model_package_group] = []\n",
    "    \n",
    "    model_packages = model_collector.list_collection(root_model_package_group)\n",
    "    \n",
    "    for model_package in model_packages:\n",
    "        if model_package['Type'] == 'Collection':\n",
    "            \n",
    "            output_dict[root_model_package_group].append(\n",
    "                {\n",
    "                    \"package_name\": model_package['Name'],\n",
    "                    \"type\": model_package[\"Type\"]\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            recursively_build_model_tree(\n",
    "                model_package['Name'], \n",
    "                output_dict, \n",
    "                level+1\n",
    "            )\n",
    "        elif model_package['Type'] == 'AWS::SageMaker::ModelPackageGroup':\n",
    "            output_dict[root_model_package_group].append(\n",
    "                {\n",
    "                    \"package_name\": model_package['Name'],\n",
    "                    \"type\": model_package[\"Type\"]\n",
    "                }\n",
    "            )\n",
    "    \n",
    "    return output_dict\n",
    "\n",
    "\n",
    "def build_tree(raw_data):\n",
    "    \"\"\" Builds a tree using dictionary input \"\"\"\n",
    "    source_dict = {}\n",
    "    for k, values in raw_data.items():\n",
    "        if not any(source_dict):\n",
    "            source_dict[k] = Node(name=k, type_of=\"root\")\n",
    "        for v in values:\n",
    "            source_dict[v['package_name']] = Node(\n",
    "                name=v['package_name'],\n",
    "                type_of=v['type'].split(':')[-1],\n",
    "                parent=source_dict[k]\n",
    "            )\n",
    "    return RenderTree(\n",
    "        source_dict[collection_name], \n",
    "        style=DoubleStyle()\n",
    "    ), source_dict[collection_name]\n",
    "\n",
    "\n",
    "raw_data = recursively_build_model_tree(\n",
    "    root_model_package_group=collection_name, \n",
    "    output_dict=OrderedDict()\n",
    ")\n",
    "\n",
    "_tree, raw_node = build_tree(raw_data=raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b871c17-3ea6-4c19-a21f-39b011ab3d60",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_path = \"test.jpg\"\n",
    "RenderTreeGraph(raw_node).to_picture(image_path)\n",
    "Image.open(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67066177-1fe5-4dfe-b206-367738318873",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 2.0.0 Python 3.10 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/pytorch-2.0.0-gpu-py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
