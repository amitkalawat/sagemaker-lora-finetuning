{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac3b8064-f819-45c9-84d2-841f2e540cb4",
   "metadata": {},
   "source": [
    "Overview\n",
    "This notebook shows how to:\n",
    "\n",
    "Define a set of Pipeline parameters that can be used to parametrize a SageMaker Pipeline.\n",
    "Define a Training step that finetunes a sentence transformer embedding model.\n",
    "Define a Create Model step that creates a model from the model artifacts used in training.\n",
    "Define a Register Model step that creates a model package from the estimator and model artifacts used to finetune the model.\n",
    "Define and create a Pipeline definition in a DAG, with the defined parameters and steps.\n",
    "Start a Pipeline execution and wait for execution to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c586f6e-c488-4350-9b0d-d9d8a4d3d188",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install 'sagemaker' --upgrade -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da32a475-929b-4517-bd71-94ba35732685",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger().setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f79deb-3721-49fe-843d-3bd867e5c1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.workflow.pipeline_context import LocalPipelineSession, PipelineSession\n",
    "\n",
    "pipeline_session = PipelineSession()\n",
    "region = pipeline_session.boto_region_name\n",
    "default_bucket = pipeline_session.default_bucket()\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5c72ac-9746-4057-8367-ee9be1a9d697",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.parameters import ParameterString, ParameterFloat, ParameterInteger, ParameterBoolean\n",
    "from sagemaker.huggingface import HuggingFaceProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.workflow.steps import CacheConfig\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "training_instance_count = 1\n",
    "evaluation_instance_count = 1\n",
    "evaluation_instance_type = \"ml.m5.xlarge\"\n",
    "training_instance_type = \"ml.g5.2xlarge\"\n",
    "\n",
    "\n",
    "%store -r train_s3_path\n",
    "%store -r valid_s3_path\n",
    "%store -r prefix\n",
    "%store -r model_id\n",
    "\n",
    "model_output_s3_loc = f\"s3://{default_bucket}/data/finetuning-{model_id.replace('/', '-')}/model\"\n",
    "# model_eval_s3_loc = f\"s3://{default_bucket}/data/finetuning-{model_id.replace('/', '-')}/modeleval\"\n",
    "\n",
    "base_model_pkg_group_name = name_from_base(model_id.replace('/', '-'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736ad26b-3d1d-47c9-a4b3-29e951877ab8",
   "metadata": {},
   "source": [
    "Setup setup caching to 12 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176fd8de-c93b-4995-9cfb-e78ec264a43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_config = CacheConfig(enable_caching=True, expire_after=\"T12H\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f0a520-4809-4c02-85c7-c4024a6cf571",
   "metadata": {},
   "source": [
    "# Define Parameters to parametize SageMaker Pipeline Executions\n",
    "Define Pipeline parameters that you can use to parametrize the pipeline. Parameters enable custom pipeline executions and schedules without having to modify the Pipeline definition.\n",
    "\n",
    "The supported parameter types include:\n",
    "\n",
    "* ParameterString - represents a str Python type\n",
    "* ParameterInteger - represents an int Python type\n",
    "* ParameterFloat - represents a float Python type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b77c4f9-6b3c-4bfb-b566-3a188ede239e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "# model id\n",
    "model_id_param = ParameterString(name=\"ModelId\", default_value=model_id)\n",
    "# epochs\n",
    "epochs_param = ParameterInteger(name=\"Epochs\", default_value=1)\n",
    "# batch size\n",
    "batch_size_param = ParameterInteger(name=\"BatchSize\", default_value=10)\n",
    "# eval steps\n",
    "evaluation_steps_param = ParameterInteger(name=\"EvalSteps\", default_value=50)\n",
    "\n",
    "#data locations\n",
    "training_dataset_s3_loc_param = ParameterString(name=\"TrainingDatasetS3LocParam\", default_value=train_s3_path)\n",
    "eval_dataset_s3_loc_param = ParameterString(name=\"EvalDatasetS3LocParam\", default_value=valid_s3_path)\n",
    "model_output_s3_loc_param = ParameterString(name=\"ModelOutputS3LocParam\", default_value=model_output_s3_loc)\n",
    "\n",
    "#instance type\n",
    "training_job_instance_type_param = ParameterString(name=\"TrainingJobInstanceType\", default_value=\"ml.g5.2xlarge\")\n",
    "eval_job_instance_type_param = ParameterString(name=\"EvaluationJobInstanceType\", default_value=\"ml.g4dn.2xlarge\")\n",
    "\n",
    "base_model_group_name_param = ParameterString(name=\"BaseModelRegistryGroupName\", default_value=base_model_pkg_group_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aba9196-ae51-427d-8264-83539f2a9f09",
   "metadata": {},
   "source": [
    "Training Step\n",
    "In this section, use define a training step to finetune an embedding model on the given dataset. Configure an Estimator for the HuggingFace and the input dataset. A typical training script loads data from the input channels, configures training with hyperparameters, trains a model, and saves a model to model_dir so that it can be hosted later.\n",
    "\n",
    "The model path where the models from training are saved is also specified.\n",
    "\n",
    "Note: the instance_type parameter may be used in multiple places in the pipeline. In this case, the instance_type is passed into the estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78bb36a-ad22-4a42-bd90-ba092027bb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters = {\n",
    "    \"model_id\": model_id_param,                             # pre-trained model\n",
    "    \"epochs\": epochs_param,\n",
    "    \"batch_size\": batch_size_param,\n",
    "    \"evaluation_steps\": evaluation_steps_param\n",
    "}\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point=\"train.py\",                                 # train script\n",
    "    source_dir=\"scripts\",                                   # directory which includes all the files needed for training\n",
    "    instance_type=training_job_instance_type_param,         # instances type used for the training job\n",
    "    instance_count=1,                                       # the number of instances used for training\n",
    "    base_job_name=name_from_base(f\"{prefix}-training-step\"),          # the name of the training job\n",
    "    role=role,                                              # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    volume_size=100,                                        # the size of the EBS volume in GB\n",
    "    transformers_version=\"4.28\",                            # the transformers version used in the training job\n",
    "    pytorch_version=\"2.0\",                                  # the pytorch_version version used in the training job\n",
    "    py_version=\"py310\",                                     # the python version used in the training job\n",
    "    hyperparameters=hyperparameters,                        # the hyperparameters passed to the training job\n",
    "    environment={\"HUGGINGFACE_HUB_CACHE\": \"/tmp/.cache\"},   # set env variable to cache models in /tmp\n",
    "    sagemaker_session=pipeline_session,                     # specifies a sagemaker session object\n",
    "    output_path=model_output_s3_loc_param                   # s3 location for model artifact\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3873c027-9740-4ce4-b420-37a717764400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a data input dictonary with our uploaded s3 uris\n",
    "data = {\"train\": training_dataset_s3_loc_param, \"valid\": eval_dataset_s3_loc_param}\n",
    "\n",
    "# starting the train job with our uploaded datasets as input\n",
    "train_args = huggingface_estimator.fit(data, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7cee9d-5175-4d8b-aaae-e101cc846689",
   "metadata": {},
   "source": [
    "Define the Training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94df1795-d404-423e-8e19-e1c3509ef925",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.workflow.steps import TrainingStep\n",
    "\n",
    "step_train = TrainingStep(\n",
    "    name=\"EmbeddingTrain\",\n",
    "    step_args=train_args,\n",
    "    cache_config=cache_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878611e6-95be-46eb-9514-60e80d7b4234",
   "metadata": {},
   "source": [
    "# Define an Evlatuion Step\n",
    "A processing step is used for triggering a processing job for model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983b57d8-893c-4351-ad5b-8cce4af98c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "\n",
    "# Initialize the HuggingFaceProcessor\n",
    "hfp = HuggingFaceProcessor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=eval_job_instance_type_param,\n",
    "    transformers_version=\"4.28\",\n",
    "    pytorch_version=\"2.0\",\n",
    "    py_version=\"py310\",\n",
    "    base_job_name=name_from_base(f\"{prefix}-evaluation-step\"),\n",
    "    sagemaker_session=pipeline_session\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69819d57-c24b-4c2c-ade3-bd6264de240c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.properties import PropertyFile\n",
    "\n",
    "# Run the processing job\n",
    "step_args = hfp.run(\n",
    "    code='evaluation.py',\n",
    "    source_dir='pipeline',\n",
    "    arguments=[\"base-model-id\", \"sentence-transformers/msmarco-bert-base-dot-v5\", \n",
    "               \"--model-file\", \"model.tar.gz\",\n",
    "               \"--test-file\", \"val_dataset.json\"],\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            input_name=\"data\",\n",
    "            source=eval_dataset_s3_loc_param,\n",
    "            destination='/opt/ml/processing/input/data/'\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            input_name=\"model\",\n",
    "            source=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "            destination=\"/opt/ml/processing/model\"),\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name=\"evaluation\",\n",
    "            source=\"/opt/ml/processing/evaluation\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "evaluation_report = PropertyFile(\n",
    "    name=f\"{prefix}-evaluation-report\",\n",
    "    output_name=\"evaluation\",\n",
    "    path=\"evaluation.json\",\n",
    ")\n",
    "\n",
    "step_eval = ProcessingStep(\n",
    "    name=\"EmbeddingEvaluation\",\n",
    "    step_args=step_args,\n",
    "    property_files=[evaluation_report],\n",
    "    cache_config=cache_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39120a69-158f-480c-bf5e-2ce25cb7db38",
   "metadata": {},
   "source": [
    "# Register Model\n",
    "SageMaker Model Registry supports the following features and functionality:\n",
    "\n",
    "* Catalog models for production.\n",
    "* Manage model versions. \n",
    "* Associate metadata, such as training metrics, with a model.\n",
    "* Manage the approval status of a model.\n",
    "* Deploy models to production.\n",
    "* Automate model deployment with CI/CD.\n",
    "\n",
    "In this workshop, we are going to register the finetuned embedding model as a model package using SageMaker Model Registry. \n",
    "\n",
    "A model package is an abstraction of reusable model artifacts that packages all ingredients required for inference. \n",
    "Primarily, it consists of an inference specification that defines the inference image to use along with an optional model weights location.\n",
    "\n",
    "A model package group is a collection of model packages. A model package group can be created for a specific ML business problem, and new versions of the model packages can be added to it. Typically, customers are expected to create a ModelPackageGroup for a SageMaker pipeline so that model package versions can be added to the group for every SageMaker Pipeline run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f96f0f-5220-4a79-b356-5522c7af8310",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
    "import json\n",
    "from sagemaker.workflow.model_step import ModelStep\n",
    "\n",
    "# retrieve the llm image uri\n",
    "triton_image=f\"785573368785.dkr.ecr.{region}.amazonaws.com/sagemaker-tritonserver:22.12-py3\"\n",
    "\n",
    "# print ecr image uri\n",
    "print(f\"llm image uri: {triton_image}\")\n",
    "\n",
    "inference_instance_type = \"ml.g5.2xlarge\"\n",
    "number_of_gpu = 1\n",
    "health_check_timeout = 3600\n",
    "\n",
    "# create HuggingFaceModel with the image uri\n",
    "huggingface_model = HuggingFaceModel(\n",
    "    model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    image_uri=triton_image,\n",
    "    transformers_version=\"4.28\",\n",
    "    pytorch_version=\"2.0\",\n",
    "    py_version=\"py310\",\n",
    "    model_server_workers=1,\n",
    "    role=role,\n",
    "    name=name_from_base(model_id.replace('/', '-')),\n",
    "    sagemaker_session=pipeline_session\n",
    ")\n",
    "\n",
    "create_step_args = huggingface_model.create(instance_type=inference_instance_type)\n",
    "step_create_model = ModelStep(\n",
    "    name=\"CreateModel\",\n",
    "    step_args=create_step_args,\n",
    "    depends_on=[step_eval]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae40913a-bd45-4e00-812a-335f5e5bcfc6",
   "metadata": {},
   "source": [
    "# Model Metrics\n",
    "To capture the model training and evalution metrics from a SageMaker Training job, we use a `ModelMetrics` class. We captured the model evaluation metrics in a `evaluation.json`, stored in the specified S3 location. With that information, we create a `ModelMetrics` object to incl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289e97cb-22e9-4bba-921c-efa4f7f4f602",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model_metrics import MetricsSource, ModelMetrics\n",
    "import os \n",
    "\n",
    "model_package_group_name = f\"{model_id.replace('/', '-')}-finetuned\"\n",
    "model_metrics = ModelMetrics(\n",
    "    model_statistics=MetricsSource(\n",
    "        s3_uri=\"{}/evaluation.json\".format(\n",
    "                step_eval.arguments[\"ProcessingOutputConfig\"][\"Outputs\"][0][\"S3Output\"][\"S3Uri\"]\n",
    "            ),\n",
    "            content_type=\"application/json\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478add21-c561-4e45-bcfb-caa5ad29372b",
   "metadata": {},
   "outputs": [],
   "source": [
    "register_args = huggingface_model.register(\n",
    "    content_types=[\"application/json\"],\n",
    "    response_types=[\"application/json\"],\n",
    "    inference_instances=[\n",
    "        \"ml.p2.16xlarge\",\n",
    "        \"ml.p3.16xlarge\",\n",
    "        \"ml.g4dn.4xlarge\",\n",
    "        \"ml.g4dn.8xlarge\",\n",
    "        \"ml.g4dn.12xlarge\",\n",
    "        \"ml.g4dn.16xlarge\",\n",
    "        \"ml.g5.2xlarge\",\n",
    "        \"ml.g5.12xlarge\",\n",
    "    ],\n",
    "    model_package_group_name=model_package_group_name,\n",
    "    customer_metadata_properties={\"training-image-uri\": huggingface_estimator.training_image_uri()}, #Store the training image url\n",
    "    approval_status=\"PendingManualApproval\",\n",
    "    model_metrics=model_metrics\n",
    ")\n",
    "step_register = ModelStep(name=\"RegisterModel\",\n",
    "                          step_args=register_args,\n",
    "                          depends_on=[step_eval, step_create_model])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21994953-c5ad-4edd-bbf4-eadbfd50bf14",
   "metadata": {},
   "source": [
    "# Define a Pipeline of Parameters and Steps \n",
    "In this section, we combine all the steps into a Pipeline so it can be executed.\n",
    "A pipeline requires a name, parameters, and steps. Names must be unique within an (account, region) pair.\n",
    "\n",
    "Note:\n",
    "\n",
    "* All the parameters used in the definitions must be present.\n",
    "* Steps passed into the pipeline do not have to be listed in the order of execution. The SageMaker Pipeline service resolves the data dependency DAG as steps for the execution to complete.\n",
    "* Steps must be unique to across the pipeline step list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539120a2-de90-4c61-8d42-67db934947fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.execution_variables import ExecutionVariables\n",
    "from sagemaker.workflow.pipeline_experiment_config import PipelineExperimentConfig\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    name=f\"{prefix}-pipeline\",\n",
    "    parameters=[\n",
    "        model_id_param,\n",
    "        epochs_param,\n",
    "        batch_size_param,\n",
    "        evaluation_steps_param,\n",
    "        training_dataset_s3_loc_param,\n",
    "        eval_dataset_s3_loc_param,\n",
    "        model_output_s3_loc_param,\n",
    "        training_job_instance_type_param,\n",
    "        eval_job_instance_type_param,\n",
    "        base_model_group_name_param\n",
    "    ],\n",
    "    steps=[step_train, step_eval, step_create_model, step_register],\n",
    "    sagemaker_session=pipeline_session\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a316047-205b-42a5-b69f-8a69efc9f1f9",
   "metadata": {},
   "source": [
    "## Examining the pipeline definition\n",
    "The JSON of the pipeline definition can be examined to confirm the pipeline is well-defined and the parameters and step properties resolve correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4115d2ac-75d3-4ea6-a277-63d04aadee7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "definition = json.loads(pipeline.definition())\n",
    "definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458846bc-5c30-45ab-bbc5-bdb53a38ca05",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.upsert(role_arn=role)\n",
    "execution = pipeline.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43bbaec-ed32-4b15-b81a-0679810807b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224d8de2-6416-4c8c-9cce-9495858a1293",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
