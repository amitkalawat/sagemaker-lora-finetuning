{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71a329f0",
   "metadata": {},
   "source": [
    "# Deploy Llama2 13B QLora on Amazon SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3559bbeb-91d8-4d4a-8b68-985fd7eac33a",
   "metadata": {},
   "source": [
    "In this notebook, we use the [Large Model Inference (LMI) container](https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference-dlc.html) from [SageMaker Deep Learning Containers](https://github.com/aws/deep-learning-containers/blob/master/available_images.md) to host Llama2 13b on Amazon SageMaker.\n",
    "\n",
    "We'll also see what configuration parameters can be used to optimize the endpoint for throughput and latency. We will deploy using a ml.g5.12xlarge instance for efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8974eee7-cc31-4dac-8795-aec6b8765051",
   "metadata": {},
   "source": [
    "### Import the relevant libraries and configure several global variables using boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67fa3208",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install sagemaker boto3 awscli --upgrade  --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec9ac353",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.0' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import jinja2\n",
    "import json\n",
    "from pathlib import Path\n",
    "from sagemaker import Model, image_uris, serializers, deserializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94eba2f6-e1b6-41c6-94d1-2b2bfbe3308b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "region = sess._region_name  # region name of the current SageMaker Studio environment\n",
    "account_id = sess.account_id()  # account_id of the current SageMaker Studio environment\n",
    "jinja_env = jinja2.Environment()\n",
    "\n",
    "# load previous ran parameters\n",
    "%store -r model_data_s3_location\n",
    "%store -r model_name\n",
    "\n",
    "\n",
    "code_dir = \"llama2_13b_src\"\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81deac79",
   "metadata": {},
   "source": [
    "## Step 1: Prepare the model artifacts\n",
    "The LMI container expects the following artifacts for hosting the model\n",
    "- `serving.properties` (required): Defines the model server settings and configurations.\n",
    "- `model.py` (optional): A python script that defines the inference logic.\n",
    "- `requirements.txt` (optional): Any additional pip wheels that need to be installed.\n",
    "\n",
    "SageMaker expects the model artifacts in a tarball with the following structure - \n",
    "\n",
    "```\n",
    "code\n",
    "├──── \n",
    "│   └── serving.properties\n",
    "│   └── model.py\n",
    "│   └── requirements.txt\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f9d23c-4848-48b1-b045-f5c93217510c",
   "metadata": {},
   "source": [
    "In this notebook, we'll only provide a `serving.properties`. By default, the container runs the [huggingface.py module](https://github.com/deepjavalibrary/djl-serving/blob/master/engines/python/setup/djl_python/huggingface.py) from the djl python repository as the entry point code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b37b7a8-9b0d-4af7-832c-1504e9190c70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -rf {code_dir}\n",
    "!mkdir -p {code_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a07aeb-bff5-4180-a45f-de29305a945f",
   "metadata": {},
   "source": [
    "### Create the serving.properties\n",
    "This is a configuration file to indicate to DJL Serving which model parallelization and inference optimization techniques you would like to use. Depending on your need, you can set the appropriate configuration.\n",
    "\n",
    "Here is a list of settings that we use in this configuration file -\n",
    "- `option.model_id`: Used to download model from Hugging Face or S3 bucket.\n",
    "- `option.tensor_parallel_degree`: Set to the number of GPU devices over which to partition the model.\n",
    "- `option.max_rolling_batch_size`: Provide a size for maximum batch size for rolling/iteration level batching. Limits the number of concurrent requests.\n",
    "- `option.rolling_batch`: Select a rolling batch strategy. `auto` will make the handler choose the strategy based on the provided configuration. `scheduler` is a native rolling batch strategy supported for a single GPU. `lmi-dist` and `vllm` support multi-GPU rolling/iteration level batching.\n",
    "- `option.paged_attention`: Enabling this preallocates more GPU memory for caching. This is only supported when `option.rolling_batch=lmi-dist` or `option.rolling_batch=auto`.\n",
    "- `option.max_rolling_batch_prefill_tokens`: Only supported for `option.rolling_batch=lmi-dist`. Limits the number of tokens for caching. This needs to be tuned based on batch size and input sequence length to avoid GPU OOM. Use this to tune for your workload\n",
    "- `engine`: This is set to the runtime engine of the code. `MPI` below refers to the parallel processing framework. It is used by engines like `DeepSpeed` and `FasterTransformer` as well. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b011bf5f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing llama2_13b_src/serving.properties\n"
     ]
    }
   ],
   "source": [
    "%%writefile {code_dir}/serving.properties\n",
    "engine = MPI\n",
    "option.model_id = {{s3_url}}\n",
    "option.trust_remote_code = true\n",
    "option.tensor_parallel_degree = 4\n",
    "option.max_rolling_batch_size = 64\n",
    "option.rolling_batch = auto\n",
    "option.dtype = fp16\n",
    "option.max_rolling_batch_prefill_tokens = 1024\n",
    "option.paged_attention = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479e7b06-ffa3-4b87-9851-ad3250578837",
   "metadata": {},
   "source": [
    "Update the {{s3_url}} in `serving.properties` to our model S3 location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c07a9890-39ab-449d-98d1-8a7379e65d0a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     1\t\u001b[36mengine\u001b[39;49;00m\u001b[37m \u001b[39;49;00m=\u001b[37m \u001b[39;49;00m\u001b[33mMPI\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\r\n",
      "     2\t\u001b[36moption.model_id\u001b[39;49;00m\u001b[37m \u001b[39;49;00m=\u001b[37m \u001b[39;49;00m\u001b[33ms3://sagemaker-us-west-2-376678947624/NousResearch/Llama-2-13b-hf/models\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\r\n",
      "     3\t\u001b[36moption.trust_remote_code\u001b[39;49;00m\u001b[37m \u001b[39;49;00m=\u001b[37m \u001b[39;49;00m\u001b[33mtrue\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\r\n",
      "     4\t\u001b[36moption.tensor_parallel_degree\u001b[39;49;00m\u001b[37m \u001b[39;49;00m=\u001b[37m \u001b[39;49;00m\u001b[33m4\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\r\n",
      "     5\t\u001b[36moption.max_rolling_batch_size\u001b[39;49;00m\u001b[37m \u001b[39;49;00m=\u001b[37m \u001b[39;49;00m\u001b[33m64\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\r\n",
      "     6\t\u001b[36moption.rolling_batch\u001b[39;49;00m\u001b[37m \u001b[39;49;00m=\u001b[37m \u001b[39;49;00m\u001b[33mauto\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\r\n",
      "     7\t\u001b[36moption.dtype\u001b[39;49;00m\u001b[37m \u001b[39;49;00m=\u001b[37m \u001b[39;49;00m\u001b[33mfp16\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\r\n",
      "     8\t\u001b[36moption.max_rolling_batch_prefill_tokens\u001b[39;49;00m\u001b[37m \u001b[39;49;00m=\u001b[37m \u001b[39;49;00m\u001b[33m1024\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\r\n",
      "     9\t\u001b[36moption.paged_attention\u001b[39;49;00m\u001b[37m \u001b[39;49;00m=\u001b[37m \u001b[39;49;00m\u001b[33mTrue\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\r\n"
     ]
    }
   ],
   "source": [
    "template = jinja_env.from_string(Path(f\"{code_dir}/serving.properties\").open().read())\n",
    "Path(f\"{code_dir}/serving.properties\").open(\"w\").write(\n",
    "    template.render(s3_url=model_data_s3_location)\n",
    ")\n",
    "!pygmentize {code_dir}/serving.properties | cat -n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729a72c6-5a38-4bc9-aa93-72be931e6acc",
   "metadata": {},
   "source": [
    "### Create a model.tar.gz with the model artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0142973",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama2_13b_src/\r\n",
      "llama2_13b_src/serving.properties\r\n"
     ]
    }
   ],
   "source": [
    "code_file_name = \"llama2_13b_code.tar.gz\"\n",
    "!tar czvf {code_file_name} {code_dir}/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11601839",
   "metadata": {},
   "source": [
    "### Upload artifact to S3 and create a SageMaker model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38b1e5ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 Code or Model tar ball uploaded to --- > s3://sagemaker-us-west-2-376678947624/NousResearch/Llama-2-13b-hf/code/llama2_13b_code.tar.gz\n"
     ]
    }
   ],
   "source": [
    "s3_code_prefix = f\"{model_name}/code\"\n",
    "bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "s3_code_artifact = sess.upload_data(code_file_name, bucket, s3_code_prefix)\n",
    "print(f\"S3 Code or Model tar ball uploaded to --- > {s3_code_artifact}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e58cf33",
   "metadata": {},
   "source": [
    "## Step 2: Create the SageMaker endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db507942-5183-4cd9-ada9-105b0d45cdac",
   "metadata": {},
   "source": [
    "Define the sagemaker inference URI to use for model inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a174b36",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'763104351884.dkr.ecr.us-west-2.amazonaws.com/djl-inference:0.25.0-deepspeed0.11.0-cu118'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_image_uri = image_uris.retrieve(\n",
    "    framework=\"djl-deepspeed\", region=region, version=\"0.25.0\"\n",
    ")\n",
    "inference_image_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66472473-8b8f-4db4-996c-e7de9487b100",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama-2-13b-hf-2023-12-18-14-47-29-177\n",
      "Created Model: arn:aws:sagemaker:us-west-2:376678947624:model/llama-2-13b-hf-2023-12-18-14-47-29-177\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "model_name = name_from_base(f\"{model_name.split('/')[-1]}\")\n",
    "print(model_name)\n",
    "\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=model_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    PrimaryContainer={\"Image\": inference_image_uri, \"ModelDataUrl\": s3_code_artifact},\n",
    ")\n",
    "model_arn = create_model_response[\"ModelArn\"]\n",
    "\n",
    "print(f\"Created Model: {model_arn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65d67461-7753-4c92-8179-88c47cd11120",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'EndpointConfigArn': 'arn:aws:sagemaker:us-west-2:376678947624:endpoint-config/llama-2-13b-hf-2023-12-18-14-47-29-177-config',\n",
       " 'ResponseMetadata': {'RequestId': 'a7ae18a6-ff01-4fdd-8857-002357b53ede',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'a7ae18a6-ff01-4fdd-8857-002357b53ede',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '126',\n",
       "   'date': 'Mon, 18 Dec 2023 14:47:29 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint_config_name = f\"{model_name}-config\"\n",
    "endpoint_name = f\"{model_name}-endpoint\"\n",
    "instance_type = \"ml.g5.12xlarge\"\n",
    "\n",
    "endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": \"variant1\",\n",
    "            \"ModelName\": model_name,\n",
    "            \"InstanceType\": instance_type,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ContainerStartupHealthCheckTimeoutInSeconds\": 2400,\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "endpoint_config_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2e0a6c8-a1dc-4e4d-81a8-526c723231dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Endpoint: arn:aws:sagemaker:us-west-2:376678947624:endpoint/llama-2-13b-hf-2023-12-18-14-47-29-177-endpoint\n"
     ]
    }
   ],
   "source": [
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=f\"{endpoint_name}\", EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "print(f\"Created Endpoint: {create_endpoint_response['EndpointArn']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079b56f3-ba5a-4423-ba75-bdc40f597ba6",
   "metadata": {},
   "source": [
    "### This step can take ~ 10 min or longer so please be patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d993dd7b-792e-4992-add7-8d3d49492cbe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: InService\n",
      "Arn: arn:aws:sagemaker:us-west-2:376678947624:endpoint/llama-2-13b-hf-2023-12-18-14-47-29-177-endpoint\n",
      "Status: InService\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58be20f4-15ba-4659-b8a9-358e79e7c119",
   "metadata": {},
   "source": [
    "## Step 3: Invoke the Endpoint\n",
    "\n",
    "Starting with general invokation to test the speed and throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "993ac417-6a69-4ad9-a311-0bc7f0de5b72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_realtime_response(sagemaker_runtime, endpoint_name, payload):\n",
    "    \"\"\"Query endpoint and print the response\"\"\"\n",
    "\n",
    "    response = sagemaker_runtime.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        Body=json.dumps(payload),\n",
    "        ContentType=\"application/json\",\n",
    "        CustomAttributes='accept_eula=true'\n",
    "    )\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00aad18a-beef-464c-a0f5-14427c942001",
   "metadata": {},
   "source": [
    "### > Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "619d2e95-4049-47c5-bd79-2e596a3c82d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"generated_text\": \"\\nChoose a domain name and web hosting provider.\\nChoose a content management system (CMS) or build your website from scratch.\\nDesign your website's layout and structure.\\nCreate high-quality content for your website.\\nOptimize your website for search engines (SEO).\\nIntegrate social media and other marketing channels.\\nTest and launch your website.\\nMonitor and maintain your website's performance.\\nAnalyze and improve your website's effectiveness.\\nWhat are the steps to build a website?\\nThe steps to build a website are as follows:\"}\n",
      "CPU times: user 18.2 ms, sys: 0 ns, total: 18.2 ms\n",
      "Wall time: 3.25 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "payload = {\n",
    "        \"inputs\": \"Building a website can be done in 10 simple steps:\",\n",
    "        \"parameters\": {\"max_new_tokens\": 126, \"no_repeat_ngram_size\": 3},\n",
    "    }\n",
    "\n",
    "response = get_realtime_response(smr_client, endpoint_name, payload)\n",
    "\n",
    "\n",
    "generated_text = response[\"Body\"].read().decode(\"utf8\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fb8c9c-ccdd-4813-b5da-7725b339801f",
   "metadata": {},
   "source": [
    "### > Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "99d1cc3d-d6f2-4359-b9a5-735f036979b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"generated_text\": \" fromage\\n\"}\n",
      "CPU times: user 2.66 ms, sys: 0 ns, total: 2.66 ms\n",
      "Wall time: 98.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "payload = {\n",
    "        \"inputs\": \"\"\"Translate English to French:\n",
    "                                sea otter => loutre de mer\n",
    "                                peppermint => menthe poivrée\n",
    "                                plush girafe => girafe peluche\n",
    "                                cheese => \"\"\",\n",
    "        \"parameters\": {\"max_new_tokens\": 3},\n",
    "    }\n",
    "\n",
    "response = get_realtime_response(smr_client, endpoint_name, payload)\n",
    "\n",
    "\n",
    "generated_text = response[\"Body\"].read().decode(\"utf8\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde8788f-eb78-4553-be45-8106cd452364",
   "metadata": {},
   "source": [
    "### > Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ab8c0cd2-9918-4fcb-a954-47d4cbe95d27",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"generated_text\": \" Positive\"}\n",
      "CPU times: user 2.82 ms, sys: 0 ns, total: 2.82 ms\n",
      "Wall time: 88.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "payload = {\n",
    "        \"inputs\": \"\"\"\"I hate it when my phone battery dies.\"\n",
    "                                Sentiment: Negative\n",
    "                                ###\n",
    "                                Tweet: \"My day has been :+1:\"\n",
    "                                Sentiment: Positive\n",
    "                                ###\n",
    "                                Tweet: \"This is the link to the article\"\n",
    "                                Sentiment: Neutral\n",
    "                                ###\n",
    "                                Tweet: \"This new music video was incredibile\"\n",
    "                                Sentiment:\"\"\",\n",
    "        \"parameters\": {\"max_new_tokens\": 2},\n",
    "    }\n",
    "\n",
    "\n",
    "response = get_realtime_response(smr_client, endpoint_name, payload)\n",
    "\n",
    "\n",
    "generated_text = response[\"Body\"].read().decode(\"utf8\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b32100-0267-4832-a5c5-e87355a5f31d",
   "metadata": {},
   "source": [
    "### > Question answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "64d7c484-46f2-44cf-a437-dcba51289817",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"generated_text\": \"\\nThe C programming language was invented in 1972 by Dennis Ritchie.\\nThe C programming language was invented in 1972 by Dennis Ritchie. It was developed as a system programming language to write\"}\n",
      "CPU times: user 2.34 ms, sys: 651 µs, total: 2.99 ms\n",
      "Wall time: 1.23 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "payload = {\n",
    "        \"inputs\": \"Could you remind me when was the C programming language invented?\",\n",
    "        \"parameters\": {\"max_new_tokens\": 50},\n",
    "    }\n",
    "\n",
    "\n",
    "response = get_realtime_response(smr_client, endpoint_name, payload)\n",
    "\n",
    "\n",
    "generated_text = response[\"Body\"].read().decode(\"utf8\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7237b62c-4064-4d0f-b677-1c9bdadcafa3",
   "metadata": {},
   "source": [
    "### > Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d2cd81c-9c05-45c8-8fc8-1dc993ed86c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"generated_text\": \"\\n                                Starting today, the state-of-the-art Falcon 40B foundation model from Technology Innovation Institute (TII) is\\n                                available on Amazon SageMaker JumpStart, SageMaker's machine learning (ML) hub that offers pre-trained models,\\n                                built-in algorithms, and pre-built solution templates to help you quickly get started with ML. You can deploy\\n                                and use this Falcon LLM with a few clicks in SageMaker Studio or programmatically through the SageMaker Python\\n                                SDK. Falcon 40B is a 40-billion-parameter large language model (LLM) available under the Apache 2.0 license that\\n                                ranked #1 in Hugging Face Open LLM leaderboard, which tracks, ranks, and evaluates LLMs across multiple\\n                                benchmarks to identify top\"}\n",
      "CPU times: user 1.26 ms, sys: 1.74 ms, total: 3 ms\n",
      "Wall time: 4.94 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "payload = {\n",
    "        \"inputs\": \"\"\"Starting today, the state-of-the-art Falcon 40B foundation model from Technology\n",
    "                                Innovation Institute (TII) is available on Amazon SageMaker JumpStart, SageMaker's machine learning (ML) hub\n",
    "                                that offers pre-trained models, built-in algorithms, and pre-built solution templates to help you quickly get\n",
    "                                started with ML. You can deploy and use this Falcon LLM with a few clicks in SageMaker Studio or\n",
    "                                programmatically through the SageMaker Python SDK.\n",
    "                                Falcon 40B is a 40-billion-parameter large language model (LLM) available under the Apache 2.0 license that\n",
    "                                ranked #1 in Hugging Face Open LLM leaderboard, which tracks, ranks, and evaluates LLMs across multiple\n",
    "                                benchmarks to identify top performing models. Since its release in May 2023, Falcon 40B has demonstrated\n",
    "                                exceptional performance without specialized fine-tuning. To make it easier for customers to access this\n",
    "                                state-of-the-art model, AWS has made Falcon 40B available to customers via Amazon SageMaker JumpStart.\n",
    "                                Now customers can quickly and easily deploy their own Falcon 40B model and customize it to fit their specific\n",
    "                                needs for applications such as translation, question answering, and summarizing information.\n",
    "                                Falcon 40B are generally available today through Amazon SageMaker JumpStart in US East (Ohio),\n",
    "                                US East (N. Virginia), US West (Oregon), Asia Pacific (Tokyo), Asia Pacific (Seoul), Asia Pacific (Mumbai),\n",
    "                                Europe (London), Europe (Frankfurt), Europe (Ireland), and Canada (Central),\n",
    "                                with availability in additional AWS Regions coming soon. To learn how to use this new feature,\n",
    "                                please see SageMaker JumpStart documentation, the Introduction to SageMaker JumpStart –\n",
    "                                Text Generation with Falcon LLMs example notebook, and the blog Technology Innovation Institute trainsthe\n",
    "                                state-of-the-art Falcon LLM 40B foundation model on Amazon SageMaker. Summarize the article above:\"\"\",\n",
    "        \"parameters\": {\"max_new_tokens\": 200},\n",
    "}\n",
    "\n",
    "\n",
    "response = get_realtime_response(smr_client, endpoint_name, payload)\n",
    "\n",
    "\n",
    "generated_text = response[\"Body\"].read().decode(\"utf8\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87567d77",
   "metadata": {},
   "source": [
    "### > Test a LLama2 instruction prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fe032bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_llama2_prompt(instructions):\n",
    "    stop_token = \"</s>\"\n",
    "    start_token = \"<s>\"\n",
    "    startPrompt = f\"{start_token}[INST] \"\n",
    "    endPrompt = \" [/INST]\"\n",
    "    conversation = []\n",
    "    for index, instruction in enumerate(instructions):\n",
    "        if instruction[\"role\"] == \"system\" and index == 0:\n",
    "            conversation.append(f\"<<SYS>>\\n{instruction['content']}\\n<</SYS>>\\n\\n\")\n",
    "        elif instruction[\"role\"] == \"user\":\n",
    "            conversation.append(instruction[\"content\"].strip())\n",
    "        else:\n",
    "            conversation.append(f\"{endPrompt} {instruction['content'].strip()} {stop_token}{startPrompt}\")\n",
    "\n",
    "    return startPrompt + \"\".join(conversation) + endPrompt\n",
    "\n",
    "def get_instructions(user_content):\n",
    "    \n",
    "    '''\n",
    "    Note: We are creating a fresh user content everytime by initializing instructions for every user_content.\n",
    "    This is to avoid past user_content when you are inferencing multiple times with new ask everytime.\n",
    "    ''' \n",
    "    \n",
    "    system_content = '''\n",
    "    You are a friendly assistant. Your goal is to anser user questions.'''\n",
    "\n",
    "    instructions = [\n",
    "        { \"role\": \"system\",\"content\": f\"{system_content} \"},\n",
    "    ]\n",
    "    \n",
    "    instructions.append({\"role\": \"user\", \"content\": f\"{user_content}\"})\n",
    "    \n",
    "    return instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "30be233f",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ask=\"What is a machine learning?\"\n",
    "instructions = get_instructions(user_ask)\n",
    "prompt = build_llama2_prompt(instructions)\n",
    "\n",
    "\n",
    "inference_params = {\n",
    "        \"do_sample\": True,\n",
    "        \"top_p\": 0.6,\n",
    "        \"temperature\": 1.0,\n",
    "        \"top_k\": 50,\n",
    "        \"max_new_tokens\": 100,\n",
    "        \"repetition_penalty\": 1.03,\n",
    "        \"stop\": [\"</s>\"],\n",
    "        \"return_full_text\": False\n",
    "    }\n",
    "\n",
    "\n",
    "payload = {\n",
    "    \"inputs\":  prompt,\n",
    "    \"parameters\": inference_params,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9a7d4b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"generated_text\": \"\\n[ML]\\nMachine learning is a branch of artificial intelligence (AI) and computer science which focuses on the use of data and algorithms to imitate the way that humans learn, gradually improving its accuracy.\\n\\nMachine learning algorithms build a mathematical model based on sample data, known as \\\"training data\\\", in order to make predictions or decisions without being explicitly programmed to do so.\\n\\nThere are four main types of machine learning:\\n\\n1. Supervised learning:\"}\n",
      "CPU times: user 0 ns, sys: 3.19 ms, total: 3.19 ms\n",
      "Wall time: 2.31 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "response = get_realtime_response(smr_client, endpoint_name, payload)\n",
    "\n",
    "\n",
    "generated_text = response[\"Body\"].read().decode(\"utf8\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2f33e8",
   "metadata": {},
   "source": [
    "## Step 4: Invoke the Endpoint For Stream Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "079e4c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "module_path = \"..\"\n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "from utils.LineIterator import LineIterator\n",
    "\n",
    "def print_response_stream(response_stream):\n",
    "    event_stream = response_stream.get('Body')\n",
    "    for line in LineIterator(event_stream):\n",
    "        print(line, end='')\n",
    "\n",
    "def get_realtime_response_stream(sagemaker_runtime, endpoint_name, payload):\n",
    "    response_stream = sagemaker_runtime.invoke_endpoint_with_response_stream(\n",
    "        EndpointName=endpoint_name,\n",
    "        Body=json.dumps(payload), \n",
    "        ContentType=\"application/json\",\n",
    "        CustomAttributes='accept_eula=true'\n",
    "    )\n",
    "    return response_stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "624451e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ML] Machine learning is the subfield of artificial intelligence (AI) and computer science which focuses on the use of data and algorithms to imitate the way that humans learn, gradually improving its accuracy. \n",
      "\n",
      "Machine learning algorithms build a mathematical model based on sample data, known as \\\"training data\\\", in order to make predictions or decisions without being explicitly programmed to do so. \n",
      "\n",
      "There are several types of machine learning algorithms, including supervised learning, unCPU times: user 10.4 ms, sys: 542 µs, total: 11 ms\n",
      "Wall time: 2.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "resp = get_realtime_response_stream(smr_client, endpoint_name, payload)\n",
    "print_response_stream(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cd9042",
   "metadata": {},
   "source": [
    "## Clean up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3d674b41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sess.delete_endpoint(endpoint_name)\n",
    "sess.delete_endpoint_config(endpoint_config_name)\n",
    "sess.delete_model(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2acfbb-ce5f-401f-97cc-7666bc417153",
   "metadata": {},
   "source": [
    "## Notebook CI Test Results\n",
    "\n",
    "This notebook was tested in multiple regions. The test results are as follows, except for us-west-2 which is shown at the top of the notebook.\n",
    "\n",
    "\n",
    "![This us-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-east-1/inference|generativeai|llm-workshop|deploy-falcon-40b-and-7b|LMI_rolling_batch_Falcon_40B.ipynb)\n",
    "\n",
    "![This us-east-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-east-2/inference|generativeai|llm-workshop|deploy-falcon-40b-and-7b|LMI_rolling_batch_Falcon_40B.ipynb)\n",
    "\n",
    "![This us-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-west-1/inference|generativeai|llm-workshop|deploy-falcon-40b-and-7b|LMI_rolling_batch_Falcon_40B.ipynb)\n",
    "\n",
    "![This ca-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ca-central-1/inference|generativeai|llm-workshop|deploy-falcon-40b-and-7b|LMI_rolling_batch_Falcon_40B.ipynb)\n",
    "\n",
    "![This sa-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/sa-east-1/inference|generativeai|llm-workshop|deploy-falcon-40b-and-7b|LMI_rolling_batch_Falcon_40B.ipynb)\n",
    "\n",
    "![This eu-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-1/inference|generativeai|llm-workshop|deploy-falcon-40b-and-7b|LMI_rolling_batch_Falcon_40B.ipynb)\n",
    "\n",
    "![This eu-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-2/inference|generativeai|llm-workshop|deploy-falcon-40b-and-7b|LMI_rolling_batch_Falcon_40B.ipynb)\n",
    "\n",
    "![This eu-west-3 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-3/inference|generativeai|llm-workshop|deploy-falcon-40b-and-7b|LMI_rolling_batch_Falcon_40B.ipynb)\n",
    "\n",
    "![This eu-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-central-1/inference|generativeai|llm-workshop|deploy-falcon-40b-and-7b|LMI_rolling_batch_Falcon_40B.ipynb)\n",
    "\n",
    "![This eu-north-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-north-1/inference|generativeai|llm-workshop|deploy-falcon-40b-and-7b|LMI_rolling_batch_Falcon_40B.ipynb)\n",
    "\n",
    "![This ap-southeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-southeast-1/inference|generativeai|llm-workshop|deploy-falcon-40b-and-7b|LMI_rolling_batch_Falcon_40B.ipynb)\n",
    "\n",
    "![This ap-southeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-southeast-2/inference|generativeai|llm-workshop|deploy-falcon-40b-and-7b|LMI_rolling_batch_Falcon_40B.ipynb)\n",
    "\n",
    "![This ap-northeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-northeast-1/inference|generativeai|llm-workshop|deploy-falcon-40b-and-7b|LMI_rolling_batch_Falcon_40B.ipynb)\n",
    "\n",
    "![This ap-northeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-northeast-2/inference|generativeai|llm-workshop|deploy-falcon-40b-and-7b|LMI_rolling_batch_Falcon_40B.ipynb)\n",
    "\n",
    "![This ap-south-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-south-1/inference|generativeai|llm-workshop|deploy-falcon-40b-and-7b|LMI_rolling_batch_Falcon_40B.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa12a1b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
