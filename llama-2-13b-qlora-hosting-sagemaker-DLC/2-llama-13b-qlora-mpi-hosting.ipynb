{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71a329f0",
   "metadata": {},
   "source": [
    "# Deploy Llama2 13B QLora on Amazon SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3559bbeb-91d8-4d4a-8b68-985fd7eac33a",
   "metadata": {},
   "source": [
    "In this notebook, we use the [Large Model Inference (LMI) container](https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference-dlc.html) from [SageMaker Deep Learning Containers](https://github.com/aws/deep-learning-containers/blob/master/available_images.md) to host Llama2 13b on Amazon SageMaker.\n",
    "\n",
    "We'll also see what configuration parameters can be used to optimize the endpoint for throughput and latency. We will deploy using a ml.g5.12xlarge instance for efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8974eee7-cc31-4dac-8795-aec6b8765051",
   "metadata": {},
   "source": [
    "### Import the relevant libraries and configure several global variables using boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fa3208",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install sagemaker boto3 awscli --upgrade  --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9ac353",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import jinja2\n",
    "import json\n",
    "from pathlib import Path\n",
    "from sagemaker import Model, image_uris, serializers, deserializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94eba2f6-e1b6-41c6-94d1-2b2bfbe3308b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "region = sess._region_name  # region name of the current SageMaker Studio environment\n",
    "account_id = sess.account_id()  # account_id of the current SageMaker Studio environment\n",
    "jinja_env = jinja2.Environment()\n",
    "\n",
    "# load previous ran parameters\n",
    "%store -r model_data_s3_location\n",
    "%store -r model_name\n",
    "\n",
    "\n",
    "code_dir = \"llama2_13b_src\"\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81deac79",
   "metadata": {},
   "source": [
    "## Step 1: Prepare the model artifacts\n",
    "The LMI container expects the following artifacts for hosting the model\n",
    "- `serving.properties` (required): Defines the model server settings and configurations.\n",
    "- `model.py` (optional): A python script that defines the inference logic.\n",
    "- `requirements.txt` (optional): Any additional pip wheels that need to be installed.\n",
    "\n",
    "SageMaker expects the model artifacts in a tarball with the following structure - \n",
    "\n",
    "```\n",
    "code\n",
    "├──── \n",
    "│   └── serving.properties\n",
    "│   └── model.py\n",
    "│   └── requirements.txt\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f9d23c-4848-48b1-b045-f5c93217510c",
   "metadata": {},
   "source": [
    "In this notebook, we'll only provide a `serving.properties`. By default, the container runs the [huggingface.py module](https://github.com/deepjavalibrary/djl-serving/blob/master/engines/python/setup/djl_python/huggingface.py) from the djl python repository as the entry point code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b37b7a8-9b0d-4af7-832c-1504e9190c70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -rf {code_dir}\n",
    "!mkdir -p {code_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a07aeb-bff5-4180-a45f-de29305a945f",
   "metadata": {},
   "source": [
    "### Create the serving.properties\n",
    "This is a configuration file to indicate to DJL Serving which model parallelization and inference optimization techniques you would like to use. Depending on your need, you can set the appropriate configuration.\n",
    "\n",
    "Here is a list of settings that we use in this configuration file -\n",
    "- `option.model_id`: Used to download model from Hugging Face or S3 bucket.\n",
    "- `option.tensor_parallel_degree`: Set to the number of GPU devices over which to partition the model.\n",
    "- `option.max_rolling_batch_size`: Provide a size for maximum batch size for rolling/iteration level batching. Limits the number of concurrent requests.\n",
    "- `option.rolling_batch`: Select a rolling batch strategy. `auto` will make the handler choose the strategy based on the provided configuration. `scheduler` is a native rolling batch strategy supported for a single GPU. `lmi-dist` and `vllm` support multi-GPU rolling/iteration level batching.\n",
    "- `option.paged_attention`: Enabling this preallocates more GPU memory for caching. This is only supported when `option.rolling_batch=lmi-dist` or `option.rolling_batch=auto`.\n",
    "- `option.max_rolling_batch_prefill_tokens`: Only supported for `option.rolling_batch=lmi-dist`. Limits the number of tokens for caching. This needs to be tuned based on batch size and input sequence length to avoid GPU OOM. Use this to tune for your workload\n",
    "- `engine`: This is set to the runtime engine of the code. `MPI` below refers to the parallel processing framework. It is used by engines like `DeepSpeed` and `FasterTransformer` as well. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b011bf5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile {code_dir}/serving.properties\n",
    "engine = MPI\n",
    "option.model_id = {{s3_url}}\n",
    "option.trust_remote_code = true\n",
    "option.tensor_parallel_degree = 4\n",
    "option.max_rolling_batch_size = 64\n",
    "option.rolling_batch = auto\n",
    "option.dtype = fp16\n",
    "option.max_rolling_batch_prefill_tokens = 1024\n",
    "option.paged_attention = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479e7b06-ffa3-4b87-9851-ad3250578837",
   "metadata": {},
   "source": [
    "Update the {{s3_url}} in `serving.properties` to our model S3 location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07a9890-39ab-449d-98d1-8a7379e65d0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template = jinja_env.from_string(Path(f\"{code_dir}/serving.properties\").open().read())\n",
    "Path(f\"{code_dir}/serving.properties\").open(\"w\").write(\n",
    "    template.render(s3_url=model_data_s3_location)\n",
    ")\n",
    "!pygmentize {code_dir}/serving.properties | cat -n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729a72c6-5a38-4bc9-aa93-72be931e6acc",
   "metadata": {},
   "source": [
    "### Create a model.tar.gz with the model artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0142973",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "code_file_name = \"llama2_13b_code.tar.gz\"\n",
    "!tar czvf {code_file_name} {code_dir}/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11601839",
   "metadata": {},
   "source": [
    "### Upload artifact to S3 and create a SageMaker model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b1e5ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_code_prefix = f\"{model_name}/code\"\n",
    "bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "s3_code_artifact = sess.upload_data(code_file_name, bucket, s3_code_prefix)\n",
    "print(f\"S3 Code or Model tar ball uploaded to --- > {s3_code_artifact}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e58cf33",
   "metadata": {},
   "source": [
    "## Step 2: Create the SageMaker endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db507942-5183-4cd9-ada9-105b0d45cdac",
   "metadata": {},
   "source": [
    "Define the sagemaker inference URI to use for model inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a174b36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inference_image_uri = image_uris.retrieve(\n",
    "    framework=\"djl-deepspeed\", region=region, version=\"0.25.0\"\n",
    ")\n",
    "inference_image_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66472473-8b8f-4db4-996c-e7de9487b100",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "model_name = name_from_base(f\"{model_name.split('/')[-1]}\")\n",
    "print(model_name)\n",
    "\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=model_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    PrimaryContainer={\"Image\": inference_image_uri, \"ModelDataUrl\": s3_code_artifact},\n",
    ")\n",
    "model_arn = create_model_response[\"ModelArn\"]\n",
    "\n",
    "print(f\"Created Model: {model_arn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d67461-7753-4c92-8179-88c47cd11120",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint_config_name = f\"{model_name}-config\"\n",
    "endpoint_name = f\"{model_name}-endpoint\"\n",
    "instance_type = \"ml.g5.12xlarge\"\n",
    "\n",
    "endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": \"variant1\",\n",
    "            \"ModelName\": model_name,\n",
    "            \"InstanceType\": instance_type,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ContainerStartupHealthCheckTimeoutInSeconds\": 2400,\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "endpoint_config_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e0a6c8-a1dc-4e4d-81a8-526c723231dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=f\"{endpoint_name}\", EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "print(f\"Created Endpoint: {create_endpoint_response['EndpointArn']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079b56f3-ba5a-4423-ba75-bdc40f597ba6",
   "metadata": {},
   "source": [
    "### This step can take ~ 10 min or longer so please be patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d993dd7b-792e-4992-add7-8d3d49492cbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58be20f4-15ba-4659-b8a9-358e79e7c119",
   "metadata": {},
   "source": [
    "## Step 3: Invoke the Endpoint\n",
    "\n",
    "Starting with general invokation to test the speed and throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993ac417-6a69-4ad9-a311-0bc7f0de5b72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_realtime_response(sagemaker_runtime, endpoint_name, payload):\n",
    "    \"\"\"Query endpoint and print the response\"\"\"\n",
    "\n",
    "    response = sagemaker_runtime.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        Body=json.dumps(payload),\n",
    "        ContentType=\"application/json\",\n",
    "        CustomAttributes='accept_eula=true'\n",
    "    )\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00aad18a-beef-464c-a0f5-14427c942001",
   "metadata": {},
   "source": [
    "### > Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619d2e95-4049-47c5-bd79-2e596a3c82d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "payload = {\n",
    "        \"inputs\": \"Building a website can be done in 10 simple steps:\",\n",
    "        \"parameters\": {\"max_new_tokens\": 126, \"no_repeat_ngram_size\": 3},\n",
    "    }\n",
    "\n",
    "response = get_realtime_response(smr_client, endpoint_name, payload)\n",
    "\n",
    "\n",
    "generated_text = response[\"Body\"].read().decode(\"utf8\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fb8c9c-ccdd-4813-b5da-7725b339801f",
   "metadata": {},
   "source": [
    "### > Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d1cc3d-d6f2-4359-b9a5-735f036979b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "payload = {\n",
    "        \"inputs\": \"\"\"Translate English to French:\n",
    "                                sea otter => loutre de mer\n",
    "                                peppermint => menthe poivrée\n",
    "                                plush girafe => girafe peluche\n",
    "                                cheese => \"\"\",\n",
    "        \"parameters\": {\"max_new_tokens\": 3},\n",
    "    }\n",
    "\n",
    "response = get_realtime_response(smr_client, endpoint_name, payload)\n",
    "\n",
    "\n",
    "generated_text = response[\"Body\"].read().decode(\"utf8\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde8788f-eb78-4553-be45-8106cd452364",
   "metadata": {},
   "source": [
    "### > Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8c0cd2-9918-4fcb-a954-47d4cbe95d27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "payload = {\n",
    "        \"inputs\": \"\"\"\"I hate it when my phone battery dies.\"\n",
    "                                Sentiment: Negative\n",
    "                                ###\n",
    "                                Tweet: \"My day has been :+1:\"\n",
    "                                Sentiment: Positive\n",
    "                                ###\n",
    "                                Tweet: \"This is the link to the article\"\n",
    "                                Sentiment: Neutral\n",
    "                                ###\n",
    "                                Tweet: \"This new music video was incredibile\"\n",
    "                                Sentiment:\"\"\",\n",
    "        \"parameters\": {\"max_new_tokens\": 2},\n",
    "    }\n",
    "\n",
    "\n",
    "response = get_realtime_response(smr_client, endpoint_name, payload)\n",
    "\n",
    "\n",
    "generated_text = response[\"Body\"].read().decode(\"utf8\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b32100-0267-4832-a5c5-e87355a5f31d",
   "metadata": {},
   "source": [
    "### > Question answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d7c484-46f2-44cf-a437-dcba51289817",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "payload = {\n",
    "        \"inputs\": \"Could you remind me when was the C programming language invented?\",\n",
    "        \"parameters\": {\"max_new_tokens\": 50},\n",
    "    }\n",
    "\n",
    "\n",
    "response = get_realtime_response(smr_client, endpoint_name, payload)\n",
    "\n",
    "\n",
    "generated_text = response[\"Body\"].read().decode(\"utf8\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7237b62c-4064-4d0f-b677-1c9bdadcafa3",
   "metadata": {},
   "source": [
    "### > Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2cd81c-9c05-45c8-8fc8-1dc993ed86c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "payload = {\n",
    "        \"inputs\": \"\"\"Starting today, the state-of-the-art Falcon 40B foundation model from Technology\n",
    "                                Innovation Institute (TII) is available on Amazon SageMaker JumpStart, SageMaker's machine learning (ML) hub\n",
    "                                that offers pre-trained models, built-in algorithms, and pre-built solution templates to help you quickly get\n",
    "                                started with ML. You can deploy and use this Falcon LLM with a few clicks in SageMaker Studio or\n",
    "                                programmatically through the SageMaker Python SDK.\n",
    "                                Falcon 40B is a 40-billion-parameter large language model (LLM) available under the Apache 2.0 license that\n",
    "                                ranked #1 in Hugging Face Open LLM leaderboard, which tracks, ranks, and evaluates LLMs across multiple\n",
    "                                benchmarks to identify top performing models. Since its release in May 2023, Falcon 40B has demonstrated\n",
    "                                exceptional performance without specialized fine-tuning. To make it easier for customers to access this\n",
    "                                state-of-the-art model, AWS has made Falcon 40B available to customers via Amazon SageMaker JumpStart.\n",
    "                                Now customers can quickly and easily deploy their own Falcon 40B model and customize it to fit their specific\n",
    "                                needs for applications such as translation, question answering, and summarizing information.\n",
    "                                Falcon 40B are generally available today through Amazon SageMaker JumpStart in US East (Ohio),\n",
    "                                US East (N. Virginia), US West (Oregon), Asia Pacific (Tokyo), Asia Pacific (Seoul), Asia Pacific (Mumbai),\n",
    "                                Europe (London), Europe (Frankfurt), Europe (Ireland), and Canada (Central),\n",
    "                                with availability in additional AWS Regions coming soon. To learn how to use this new feature,\n",
    "                                please see SageMaker JumpStart documentation, the Introduction to SageMaker JumpStart –\n",
    "                                Text Generation with Falcon LLMs example notebook, and the blog Technology Innovation Institute trainsthe\n",
    "                                state-of-the-art Falcon LLM 40B foundation model on Amazon SageMaker. Summarize the article above:\"\"\",\n",
    "        \"parameters\": {\"max_new_tokens\": 200},\n",
    "}\n",
    "\n",
    "\n",
    "response = get_realtime_response(smr_client, endpoint_name, payload)\n",
    "\n",
    "\n",
    "generated_text = response[\"Body\"].read().decode(\"utf8\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87567d77",
   "metadata": {},
   "source": [
    "### > Test a LLama2 instruction prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe032bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_llama2_prompt(instructions):\n",
    "    stop_token = \"</s>\"\n",
    "    start_token = \"<s>\"\n",
    "    startPrompt = f\"{start_token}[INST] \"\n",
    "    endPrompt = \" [/INST]\"\n",
    "    conversation = []\n",
    "    for index, instruction in enumerate(instructions):\n",
    "        if instruction[\"role\"] == \"system\" and index == 0:\n",
    "            conversation.append(f\"<<SYS>>\\n{instruction['content']}\\n<</SYS>>\\n\\n\")\n",
    "        elif instruction[\"role\"] == \"user\":\n",
    "            conversation.append(instruction[\"content\"].strip())\n",
    "        else:\n",
    "            conversation.append(f\"{endPrompt} {instruction['content'].strip()} {stop_token}{startPrompt}\")\n",
    "\n",
    "    return startPrompt + \"\".join(conversation) + endPrompt\n",
    "\n",
    "def get_instructions(user_content):\n",
    "    \n",
    "    '''\n",
    "    Note: We are creating a fresh user content everytime by initializing instructions for every user_content.\n",
    "    This is to avoid past user_content when you are inferencing multiple times with new ask everytime.\n",
    "    ''' \n",
    "    \n",
    "    system_content = '''\n",
    "    You are a friendly assistant. Your goal is to anser user questions.'''\n",
    "\n",
    "    instructions = [\n",
    "        { \"role\": \"system\",\"content\": f\"{system_content} \"},\n",
    "    ]\n",
    "    \n",
    "    instructions.append({\"role\": \"user\", \"content\": f\"{user_content}\"})\n",
    "    \n",
    "    return instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30be233f",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ask=\"What is a machine learning?\"\n",
    "instructions = get_instructions(user_ask)\n",
    "prompt = build_llama2_prompt(instructions)\n",
    "\n",
    "\n",
    "inference_params = {\n",
    "        \"do_sample\": True,\n",
    "        \"top_p\": 0.6,\n",
    "        \"temperature\": 1.0,\n",
    "        \"top_k\": 50,\n",
    "        \"max_new_tokens\": 100,\n",
    "        \"repetition_penalty\": 1.03,\n",
    "        \"stop\": [\"</s>\"],\n",
    "        \"return_full_text\": False\n",
    "    }\n",
    "\n",
    "\n",
    "payload = {\n",
    "    \"inputs\":  prompt,\n",
    "    \"parameters\": inference_params,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7d4b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "response = get_realtime_response(smr_client, endpoint_name, payload)\n",
    "\n",
    "\n",
    "generated_text = response[\"Body\"].read().decode(\"utf8\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2f33e8",
   "metadata": {},
   "source": [
    "## Step 4: Invoke the Endpoint For Stream Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079e4c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "module_path = \"..\"\n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "from utils.LineIterator import LineIterator\n",
    "\n",
    "def print_response_stream(response_stream):\n",
    "    event_stream = response_stream.get('Body')\n",
    "    for line in LineIterator(event_stream):\n",
    "        print(line, end='')\n",
    "\n",
    "def get_realtime_response_stream(sagemaker_runtime, endpoint_name, payload):\n",
    "    response_stream = sagemaker_runtime.invoke_endpoint_with_response_stream(\n",
    "        EndpointName=endpoint_name,\n",
    "        Body=json.dumps(payload), \n",
    "        ContentType=\"application/json\",\n",
    "        CustomAttributes='accept_eula=true'\n",
    "    )\n",
    "    return response_stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624451e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "resp = get_realtime_response_stream(smr_client, endpoint_name, payload)\n",
    "print_response_stream(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cd9042",
   "metadata": {},
   "source": [
    "## Clean up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d674b41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sess.delete_endpoint(endpoint_name)\n",
    "sess.delete_endpoint_config(endpoint_config_name)\n",
    "sess.delete_model(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2acfbb-ce5f-401f-97cc-7666bc417153",
   "metadata": {},
   "source": [
    "## Notebook CI Test Results\n",
    "\n",
    "This notebook was tested in multiple regions. The test results are as follows, except for us-west-2 which is shown at the top of the notebook.\n",
    "\n",
    "\n",
    "![This us-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-east-1/inference|generativeai|llm-workshop|deploy-falcon-40b-and-7b|LMI_rolling_batch_Falcon_40B.ipynb)\n",
    "\n",
    "![This us-east-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-east-2/inference|generativeai|llm-workshop|deploy-falcon-40b-and-7b|LMI_rolling_batch_Falcon_40B.ipynb)\n",
    "\n",
    "![This us-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-west-1/inference|generativeai|llm-workshop|deploy-falcon-40b-and-7b|LMI_rolling_batch_Falcon_40B.ipynb)\n",
    "\n",
    "![This ca-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ca-central-1/inference|generativeai|llm-workshop|deploy-falcon-40b-and-7b|LMI_rolling_batch_Falcon_40B.ipynb)\n",
    "\n",
    "![This sa-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/sa-east-1/inference|generativeai|llm-workshop|deploy-falcon-40b-and-7b|LMI_rolling_batch_Falcon_40B.ipynb)\n",
    "\n",
    "![This eu-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-1/inference|generativeai|llm-workshop|deploy-falcon-40b-and-7b|LMI_rolling_batch_Falcon_40B.ipynb)\n",
    "\n",
    "![This eu-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-2/inference|generativeai|llm-workshop|deploy-falcon-40b-and-7b|LMI_rolling_batch_Falcon_40B.ipynb)\n",
    "\n",
    "![This eu-west-3 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-3/inference|generativeai|llm-workshop|deploy-falcon-40b-and-7b|LMI_rolling_batch_Falcon_40B.ipynb)\n",
    "\n",
    "![This eu-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-central-1/inference|generativeai|llm-workshop|deploy-falcon-40b-and-7b|LMI_rolling_batch_Falcon_40B.ipynb)\n",
    "\n",
    "![This eu-north-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-north-1/inference|generativeai|llm-workshop|deploy-falcon-40b-and-7b|LMI_rolling_batch_Falcon_40B.ipynb)\n",
    "\n",
    "![This ap-southeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-southeast-1/inference|generativeai|llm-workshop|deploy-falcon-40b-and-7b|LMI_rolling_batch_Falcon_40B.ipynb)\n",
    "\n",
    "![This ap-southeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-southeast-2/inference|generativeai|llm-workshop|deploy-falcon-40b-and-7b|LMI_rolling_batch_Falcon_40B.ipynb)\n",
    "\n",
    "![This ap-northeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-northeast-1/inference|generativeai|llm-workshop|deploy-falcon-40b-and-7b|LMI_rolling_batch_Falcon_40B.ipynb)\n",
    "\n",
    "![This ap-northeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-northeast-2/inference|generativeai|llm-workshop|deploy-falcon-40b-and-7b|LMI_rolling_batch_Falcon_40B.ipynb)\n",
    "\n",
    "![This ap-south-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-south-1/inference|generativeai|llm-workshop|deploy-falcon-40b-and-7b|LMI_rolling_batch_Falcon_40B.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa12a1b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
