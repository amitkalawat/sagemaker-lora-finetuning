{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71a329f0",
   "metadata": {},
   "source": [
    "# Mistral 7B deployment guide\n",
    "In this tutorial, you will use LMI container from DLC to SageMaker and run inference with it.\n",
    "\n",
    "Please make sure the following permission granted before running the notebook:\n",
    "\n",
    "- S3 bucket push access\n",
    "- SageMaker access\n",
    "\n",
    "## Step 1: Let's bump up SageMaker and import stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "67fa3208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install sagemaker --upgrade  --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec9ac353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import Model, image_uris, serializers, deserializers\n",
    "\n",
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "region = sess._region_name  # region name of the current SageMaker Studio environment\n",
    "account_id = sess.account_id()  # account_id of the current SageMaker Studio environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81deac79",
   "metadata": {},
   "source": [
    "## Step 2: Start preparing model artifacts\n",
    "In LMI contianer, we expect some artifacts to help setting up the model\n",
    "- serving.properties (required): Defines the model server settings\n",
    "- model.py (optional): A python file to define the core inference logic\n",
    "- requirements.txt (optional): Any additional pip wheel need to install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b011bf5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing serving.properties\n"
     ]
    }
   ],
   "source": [
    "%%writefile serving.properties\n",
    "engine=Python\n",
    "option.model_id=mistralai/Mistral-7B-Instruct-v0.2\n",
    "option.dtype=fp16\n",
    "option.max_model_len=8096\n",
    "option.task=text-generation\n",
    "option.rolling_batch=vllm\n",
    "option.tensor_parallel_degree=1\n",
    "option.device_map=auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0142973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mymodel/\n",
      "mymodel/serving.properties\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "mkdir mymodel\n",
    "mv serving.properties mymodel/\n",
    "tar czvf mymodel.tar.gz mymodel/\n",
    "rm -rf mymodel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e58cf33",
   "metadata": {},
   "source": [
    "## Step 3: Start building SageMaker endpoint\n",
    "In this step, we will build SageMaker endpoint from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d955679",
   "metadata": {},
   "source": [
    "### Getting the container image URI\n",
    "\n",
    "[Large Model Inference available DLC](https://github.com/aws/deep-learning-containers/blob/master/available_images.md#large-model-inference-containers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a174b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_uri = image_uris.retrieve(\n",
    "        framework=\"djl-deepspeed\",\n",
    "        region=sess.boto_session.region_name,\n",
    "        version=\"0.26.0\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11601839",
   "metadata": {},
   "source": [
    "### Upload artifact on S3 and create SageMaker model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38b1e5ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 Code or Model tar ball uploaded to --- > s3://sagemaker-us-west-2-376678947624/large-model-lmi/code/mymodel.tar.gz\n"
     ]
    }
   ],
   "source": [
    "s3_code_prefix = \"large-model-lmi/code\"\n",
    "bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "code_artifact = sess.upload_data(\"mymodel.tar.gz\", bucket, s3_code_prefix)\n",
    "print(f\"S3 Code or Model tar ball uploaded to --- > {code_artifact}\")\n",
    "\n",
    "model = Model(image_uri=image_uri, model_data=code_artifact, role=role)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004f39f6",
   "metadata": {},
   "source": [
    "### 4.2 Create SageMaker endpoint\n",
    "\n",
    "You need to specify the instance to use and endpoint names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e0e61cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------!"
     ]
    }
   ],
   "source": [
    "instance_type = \"ml.g5.2xlarge\"\n",
    "endpoint_name = sagemaker.utils.name_from_base(\"lmi-model\")\n",
    "\n",
    "model.deploy(initial_instance_count=1,\n",
    "             instance_type=instance_type,\n",
    "             endpoint_name=endpoint_name\n",
    "            )\n",
    "\n",
    "# our requests and responses will be in json format so we specify the serializer and the deserializer\n",
    "predictor = sagemaker.Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    sagemaker_session=sess,\n",
    "    serializer=serializers.JSONSerializer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb63ee65",
   "metadata": {},
   "source": [
    "## Step 5: Test and benchmark the inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79786708",
   "metadata": {},
   "source": [
    "Firstly let's try to run with a wrong inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2bcef095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.9 ms, sys: 366 Âµs, total: 22.2 ms\n",
      "Wall time: 23.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "response = predictor.predict(\n",
    "    {\"inputs\": \"write a blog about foundation model ops\", \"parameters\": {\"max_tokens\":2000}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca86a383",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nTitle: Revolutionizing AI Operations with Foundation Model Ops\\n\\nIntroduction:\\nArtificial Intelligence (AI) has become a game-changer in many industries, enabling businesses to streamline their processes, gain new insights, and enhance customer experiences. However, as AI models become more complex, managing them effectively becomes increasingly challenging. Foundation Model Ops is an emerging approach to managing AI operations that aims to simplify and streamline AI model deployment, scaling, and maintenance. In this blog post, we will explore what Foundation Model Ops is, its benefits, and how it revolutionizes AI operations.\\n\\nDefinition of Foundation Model Ops:\\nFoundation Model Ops refers to the set of practices, tools, and infrastructure that enable organizations to deploy, manage, and operate large-scale AI models efficiently and effectively. These models are called foundation models because they serve as the foundation upon which more specialized models can be built. Foundation Models, such as BERT, RoBERTa, and T5, are pre-trained models that have been trained on vast amounts of data and can understand and generate human-like text.\\n\\nBenefits of Foundation Model Ops:\\n1. Simplifies AI Model Deployment: Foundation Model Ops provides a standardized way to deploy, manage, and operate AI models, making it easier for businesses to adopt and scale AI models across their organizations.\\n2. Improves Efficiency and Cost Savings: By enabling the sharing of pre-trained foundation models, organizations can save time and money by avoiding the need to train their models from scratch. Instead, they can fine-tune the pre-trained models on their specific use case.\\n3. Enhances Flexibility: Foundation Model Ops allows organizations to easily experiment with different models, architectures, and training techniques, enabling them to quickly adapt to changing business needs and requirements.\\n4. Boosts Productivity: Foundation Model Ops automates repetitive tasks, allowing data scientists and AI developers to focus on value-added activities, such as building custom applications and models.\\n5. Enhances Security: Foundation Model Ops provides a secure and controlled environment for deploying and operating AI models, ensuring that sensitive data remains protected and compliant with regulatory requirements.\\n\\nRevolutionizing AI Operations:\\nFoundation Model Ops is revolutionizing AI operations by simplifying and streamlining the deployment, scaling, and maintenance of AI models. It enables organizations to leverage pre-trained foundation models, automate repetitive tasks, and standardize their AI operations. Furthermore, Foundation Model Ops provides a secure and controlled environment for deploying and operating AI models, ensuring that sensitive data remains protected and compliant with regulatory requirements. As AI becomes more prevalent in business, Foundation Model Ops is becoming an essential component of any AI strategy.\\n\\nConclusion:\\nFoundation Model Ops is an emerging approach to managing AI operations that enables organizations to deploy, manage, and operate large-scale AI models efficiently and effectively. It simplifies and streamlines AI model deployment, improves efficiency and cost savings, enhances productivity, and boosts security. As AI continues to transform businesses and industries, Foundation Model Ops will become an essential component of any AI strategy. We are only at the beginning of the Foundation Model Ops journey, and we are excited to see what the future holds. Stay tuned for more updates on Foundation Model Ops and its impact on AI operations.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "output = json.loads(response)['generated_text']\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8bb473ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84.7457627118644"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2000/23.6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cd9042",
   "metadata": {},
   "source": [
    "## Clean up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d674b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.delete_endpoint(endpoint_name)\n",
    "sess.delete_endpoint_config(endpoint_name)\n",
    "model.delete_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
